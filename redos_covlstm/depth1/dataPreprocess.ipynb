{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-14T09:08:51.919064Z",
     "start_time": "2024-08-14T09:08:50.736155Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T09:08:57.394464Z",
     "start_time": "2024-08-14T09:08:57.376468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clip_nc_region(nc_file,date):\n",
    "    # 打开 NetCDF 文件\n",
    "    data = xr.open_dataset(nc_file)\n",
    "    # 确定经纬度范围\n",
    "    lon_range = np.linspace(112.0, 117.1, data['u'].shape[2])\n",
    "    lat_range = np.linspace(12.9, 15.7, data['u'].shape[1])\n",
    "    u = xr.DataArray(data['u'].values, dims=['lv', 'latu', 'lonu'], coords={'lonu': lon_range, 'latu': lat_range})\n",
    "    \n",
    "    # 创建 DataArray 对象并赋予坐标\n",
    "    lon_range = np.linspace(112.0, 117.1, data['v'].shape[2])\n",
    "    lat_range = np.linspace(12.9, 15.7, data['v'].shape[1])\n",
    "    v = xr.DataArray(data['v'].values, dims=['lv', 'latv', 'lonv'], coords={'lonv': lon_range, 'latv': lat_range})\n",
    "    \n",
    "    # 确定目标经纬度范围\n",
    "    target_lon = data['lon'].values\n",
    "    target_lat = data['lat'].values\n",
    "    \n",
    "    # 进行插值\n",
    "    regrid_u = u.interp(lonu=target_lon, latu=target_lat, method='linear')\n",
    "    regrid_v = v.interp(lonv=target_lon, latv=target_lat, method='linear')\n",
    "    \n",
    "    # 截取 s, t, zeta 使用的 lon 和 lat\n",
    "    # 指定要截取的经纬度范围\n",
    "    lon_min, lon_max = 112, 117.1  # 经度范围\n",
    "    lat_min, lat_max = 12.9, 15.7  # 纬度范围\n",
    "    subset_t = data['t'].sel(lon=slice(lon_min, lon_max), lat=slice(lat_min, lat_max))\n",
    "    subset_s = data['s'].sel(lon=slice(lon_min, lon_max), lat=slice(lat_min, lat_max))\n",
    "    subset_zeta = data['zeta'].sel(lon=slice(lon_min, lon_max), lat=slice(lat_min, lat_max))\n",
    "    subset_u = regrid_u.sel(lonu=slice(lon_min, lon_max), latu=slice(lat_min, lat_max))\n",
    "    subset_v = regrid_v.sel(lonv=slice(lon_min, lon_max), latv=slice(lat_min, lat_max))\n",
    "    # 创建新的数据集\n",
    "    subset = xr.Dataset({\n",
    "        't': subset_t,\n",
    "        's': subset_s,\n",
    "        'zeta': subset_zeta,\n",
    "        'u': subset_u,\n",
    "        'v': subset_v\n",
    "    })\n",
    "    #保存为新的 NetCDF 文件\n",
    "    subset.to_netcdf('E:/DataSet/redos/Subset_1.0_1995/subset_'+date+'.nc')"
   ],
   "id": "a2abe2ec17820b82",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def date_incrementer(start_year, end_year):\n",
    "    # 从指定年份的1月1日开始\n",
    "    current_date = datetime(start_year, 1, 1)\n",
    "\n",
    "    # 指定结束日期\n",
    "    end_date = datetime(end_year + 1, 1, 1)\n",
    "\n",
    "    # 循环自增日期\n",
    "    while current_date < end_date:\n",
    "        # 转为8位字符串格式 YYYYMMDD\n",
    "        date_str = current_date.strftime('%Y%m%d')\n",
    "        nc_file = 'E:/DataSet/redos/REDOS_1.0_1995/REDOS_1.0_' + date_str + '.nc'\n",
    "        # data = xr.open_dataset(nc_file)\n",
    "        clip_nc_region(nc_file, date_str)\n",
    "        print(nc_file)\n",
    "        # 日期自增1天\n",
    "        current_date += timedelta(days=1)"
   ],
   "id": "e4c88e4bce4ef806",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T09:36:03.790643Z",
     "start_time": "2024-08-14T09:36:03.741628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#对比原数据与截取后的数据\n",
    "original_data = xr.open_dataset('E:/DataSet/redos/REDOS_1.0_1995/REDOS_1.0_19951231.nc')\n",
    "# 读取截取后的数据\n",
    "subset_data = xr.open_dataset('E:/DataSet/redos/Subset_1.0_1995/subset_19951231.nc')\n",
    "lon_min, lon_max = 112, 117.1\n",
    "lat_min, lat_max = 12.9, 15.7\n",
    "original_subset_t = original_data['t'].sel(lon=slice(lon_min, lon_max), lat=slice(lat_min, lat_max))\n",
    "original_subset_s = original_data['s'].sel(lon=slice(lon_min, lon_max), lat=slice(lat_min,lat_max ))\n",
    "# original_subset_u = original_data['u'].sel(lonu=slice(lon_min, lon_max), latu=slice(lat_min, lat_max))\n",
    "# original_subset_v = original_data['v'].sel(lonv=slice(lon_min, lon_max), latv=slice(lat_min, lat_max))\n",
    "t_equal = original_subset_t.equals(subset_data['t'])\n",
    "s_equal = original_subset_s.equals(subset_data['s'])\n",
    "# u_equal = original_subset_u.equals(subset_data['u'])\n",
    "# v_equal = original_subset_v.equals(subset_data['v'])\n",
    "# print(original_subset_u[:5, :5]) \n",
    "subset_u = subset_data['u']\n",
    "print(subset_u[:5, :5]) \n",
    "print(original_data['u'][:5, :5])\n",
    "# print(f\"t data match: {t_equal}\")\n",
    "# print(f\"s data match: {s_equal}\")\n",
    "# print(f\"u data match: {u_equal}\")\n",
    "# print(f\"v data match: {v_equal}\")\n"
   ],
   "id": "eb404b8f23380d19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'u' (lv: 5, latu: 5, lonu: 52)>\n",
      "[1300 values with dtype=float64]\n",
      "Coordinates:\n",
      "  * lonu     (lonu) float64 112.0 112.1 112.2 112.3 ... 116.8 116.9 117.0 117.1\n",
      "  * latu     (latu) float64 12.99 13.09 13.18 13.28 13.38\n",
      "Dimensions without coordinates: lv\n",
      "<xarray.DataArray 'u' (lv: 5, latu: 5, lonu: 350)>\n",
      "[8750 values with dtype=float32]\n",
      "Dimensions without coordinates: lv, latu, lonu\n",
      "Attributes:\n",
      "    long_name:  u component of current\n",
      "    units:      m/s\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:17:17.656397Z",
     "start_time": "2024-08-14T14:17:14.041853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "# 打开保存的 NetCDF 文件\n",
    "# subset_data = xr.open_dataset('subset_file_3.nc')\n",
    "# t=subset_data['t']\n",
    "# s=subset_data['s']\n",
    "# zeta=subset_data['zeta']\n",
    "# u=subset_data['u']\n",
    "# v=subset_data['v']\n",
    "original_data = xr.open_dataset('E:/DataSet/redos/REDOS_1.0_1995/REDOS_1.0_19951231.nc')\n",
    "file_obj = nc.Dataset('E:/DataSet/redos/Subset_1.0_1995/subset_19951231.nc')\n",
    "t = file_obj.variables['t']\n",
    "t_data=original_data['t']\n",
    "t_arr=t[:]\n",
    "# 定义经纬度范围\n",
    "lon_min, lon_max = 112, 117.1\n",
    "lat_min, lat_max = 12.9, 15.7\n",
    "\n",
    "# 选择该区域内的 t 变量数据\n",
    "subset_t = original_data['t'].sel(lon=slice(lon_min, lon_max), lat=slice(lat_min, lat_max))\n",
    "\n",
    "# 输出该区域内的 t 数据\n",
    "print(subset_t)\n",
    "print(subset_t.values)  # 输出具体的数值\n",
    "# t_arr[0]\n",
    "def scaler(data):\n",
    "    #normalise [0,1]\n",
    "    data_max = np.nanmax(data)\n",
    "    data_min = np.nanmin(data)\n",
    "    data_scale = data_max - data_min\n",
    "    data_std = (data - data_min) / data_scale\n",
    "    # data_std = data_std * (2)  -1\n",
    "    data_std [np.isnan(data_std)] = 0\n",
    "    print(data_max,data_min,data_std)\n",
    "    return data_max,data_min,data_scale\n",
    "scaler(subset_t[0])"
   ],
   "id": "1559416d9a0156a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 't' (lv: 24, lat: 28, lon: 52)>\n",
      "[34944 values with dtype=float32]\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 112.0 112.1 112.2 112.3 ... 116.8 116.9 117.0 117.1\n",
      "  * lat      (lat) float64 12.99 13.09 13.18 13.28 ... 15.32 15.41 15.51 15.61\n",
      "Dimensions without coordinates: lv\n",
      "Attributes:\n",
      "    long_name:  temperature\n",
      "    units:      degree_Celsius\n",
      "[[[ 4.5385175  4.544624   4.544624  ...  4.71996    4.7138543  4.70862  ]\n",
      "  [ 4.5603256  4.570793   4.5786443 ...  4.7121096  4.706003   4.705131 ]\n",
      "  [ 4.6144094  4.6466856  4.667621  ...  4.704259   4.694663   4.6981525]\n",
      "  ...\n",
      "  [ 4.6170263  4.6353455  4.61092   ...  4.4443064  4.4765825  4.526305 ]\n",
      "  [ 4.5952187  4.6144094  4.6213884 ...  4.452158   4.4931564  4.5367727]\n",
      "  [ 4.5463686  4.596091   4.6423235 ...  4.4678593  4.507986   4.5489855]]\n",
      "\n",
      " [[ 4.7443857  4.756598   4.7609596 ...  5.119484   5.1648445  5.2459707]\n",
      "  [ 4.8106823  4.829873   4.84383   ...  5.0906973  5.1203566  5.1857805]\n",
      "  [ 4.9101267  4.9450197  4.9615936 ...  5.0741234  5.087208   5.129952 ]\n",
      "  ...\n",
      "  [ 5.141292   5.1273346  5.11425   ...  4.8691273  4.88134    4.906637 ]\n",
      "  [ 5.1857805  5.1718235  5.1587386 ...  4.842958   4.8621492  4.894425 ]\n",
      "  [ 5.238992   5.240737   5.2320137 ...  4.8246393  4.846447   4.8734894]]\n",
      "\n",
      " [[ 5.5922823  5.6001334  5.590538  ...  6.004891   6.0929956  6.2125034]\n",
      "  [ 5.604495   5.622814   5.6332817 ...  5.928999   6.00838    6.113059 ]\n",
      "  [ 5.5591345  5.604495   5.6428776 ...  5.8731704  5.9394665  6.0371666]\n",
      "  ...\n",
      "  [ 5.916786   5.8810215  5.867936  ...  5.4980717  5.513774   5.539943 ]\n",
      "  [ 6.001402   5.9717426  5.942084  ...  5.4483495  5.4623065  5.490221 ]\n",
      "  [ 6.104336   6.0816555  6.0772934 ...  5.428286   5.443988   5.487604 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[25.090425  24.920322  24.908981  ... 26.661476  26.753942  26.834196 ]\n",
      "  [25.045065  24.96045   24.921194  ... 26.630945  26.734753  26.821112 ]\n",
      "  [25.041574  24.967428  24.929045  ... 26.589947  26.709455  26.803665 ]\n",
      "  ...\n",
      "  [23.871792  23.853472  23.761005  ... 25.591137  25.597244  25.527458 ]\n",
      "  [23.806366  23.815962  23.729603  ... 25.467268  25.52397   25.488203 ]\n",
      "  [23.735708  23.672901  23.589159  ... 25.285826  25.375675  25.390503 ]]\n",
      "\n",
      " [[25.085192  24.91596   24.904621  ... 26.651009  26.747837  26.827219 ]\n",
      "  [25.039831  24.956087  24.916832  ... 26.61786   26.728645  26.814133 ]\n",
      "  [25.03547   24.96132   24.923811  ... 26.576862  26.703348  26.796686 ]\n",
      "  ...\n",
      "  [23.868301  23.849112  23.757517  ... 25.58416   25.589394  25.519608 ]\n",
      "  [23.802006  23.811602  23.72524   ... 25.460289  25.516119  25.481226 ]\n",
      "  [23.731348  23.66854   23.585669  ... 25.278847  25.367823  25.382652 ]]\n",
      "\n",
      " [[25.079084  24.910727  24.898514  ... 26.622223  26.74173   26.822857 ]\n",
      "  [25.032852  24.949982  24.911598  ... 26.578606  26.721666  26.808899 ]\n",
      "  [25.027618  24.954342  24.916832  ... 26.543713  26.695498  26.791452 ]\n",
      "  ...\n",
      "  [23.856089  23.838642  23.748793  ... 25.578053  25.58416   25.514374 ]\n",
      "  [23.790665  23.80026   23.715645  ... 25.455055  25.511757  25.475992 ]\n",
      "  [23.718262  23.656328  23.57433   ... 25.274485  25.366951  25.38178  ]]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "2-dimensional boolean indexing is not supported. ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 37\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28mprint\u001B[39m(data_max,data_min,data_std)\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data_std,data_min,data_scale\n\u001B[1;32m---> 37\u001B[0m \u001B[43mscaler\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubset_t\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 34\u001B[0m, in \u001B[0;36mscaler\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m     32\u001B[0m data_std \u001B[38;5;241m=\u001B[39m (data \u001B[38;5;241m-\u001B[39m data_min) \u001B[38;5;241m/\u001B[39m data_scale\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# data_std = data_std * (2)  -1\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m data_std [np\u001B[38;5;241m.\u001B[39misnan(data_std)] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28mprint\u001B[39m(data_max,data_min,data_std)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data_std,data_min,data_scale\n",
      "File \u001B[1;32mD:\\CondaEnvs\\torchtmp\\lib\\site-packages\\xarray\\core\\dataarray.py:827\u001B[0m, in \u001B[0;36mDataArray.__setitem__\u001B[1;34m(self, key, value)\u001B[0m\n\u001B[0;32m    822\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoords[key] \u001B[38;5;241m=\u001B[39m value\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    824\u001B[0m     \u001B[38;5;66;03m# Coordinates in key, value and self[key] should be consistent.\u001B[39;00m\n\u001B[0;32m    825\u001B[0m     \u001B[38;5;66;03m# TODO Coordinate consistency in key is checked here, but it\u001B[39;00m\n\u001B[0;32m    826\u001B[0m     \u001B[38;5;66;03m# causes unnecessary indexing. It should be optimized.\u001B[39;00m\n\u001B[1;32m--> 827\u001B[0m     obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, DataArray):\n\u001B[0;32m    829\u001B[0m         assert_coordinate_consistent(value, obj\u001B[38;5;241m.\u001B[39mcoords\u001B[38;5;241m.\u001B[39mvariables)\n",
      "File \u001B[1;32mD:\\CondaEnvs\\torchtmp\\lib\\site-packages\\xarray\\core\\dataarray.py:818\u001B[0m, in \u001B[0;36mDataArray.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    815\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_coord(key)\n\u001B[0;32m    816\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    817\u001B[0m     \u001B[38;5;66;03m# xarray-style array indexing\u001B[39;00m\n\u001B[1;32m--> 818\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_item_key_to_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\CondaEnvs\\torchtmp\\lib\\site-packages\\xarray\\core\\dataarray.py:1389\u001B[0m, in \u001B[0;36mDataArray.isel\u001B[1;34m(self, indexers, drop, missing_dims, **indexers_kwargs)\u001B[0m\n\u001B[0;32m   1386\u001B[0m indexers \u001B[38;5;241m=\u001B[39m either_dict_or_kwargs(indexers, indexers_kwargs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124misel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(is_fancy_indexer(idx) \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indexers\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m-> 1389\u001B[0m     ds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_to_temp_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_isel_fancy\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1390\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindexers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdrop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing_dims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing_dims\u001B[49m\n\u001B[0;32m   1391\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1392\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_from_temp_dataset(ds)\n\u001B[0;32m   1394\u001B[0m \u001B[38;5;66;03m# Much faster algorithm for when all indexers are ints, slices, one-dimensional\u001B[39;00m\n\u001B[0;32m   1395\u001B[0m \u001B[38;5;66;03m# lists, or zero or one-dimensional np.ndarray's\u001B[39;00m\n",
      "File \u001B[1;32mD:\\CondaEnvs\\torchtmp\\lib\\site-packages\\xarray\\core\\dataset.py:2464\u001B[0m, in \u001B[0;36mDataset._isel_fancy\u001B[1;34m(self, indexers, drop, missing_dims)\u001B[0m\n\u001B[0;32m   2460\u001B[0m var_indexers \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   2461\u001B[0m     k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m valid_indexers\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m var\u001B[38;5;241m.\u001B[39mdims\n\u001B[0;32m   2462\u001B[0m }\n\u001B[0;32m   2463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m var_indexers:\n\u001B[1;32m-> 2464\u001B[0m     new_var \u001B[38;5;241m=\u001B[39m \u001B[43mvar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvar_indexers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2465\u001B[0m     \u001B[38;5;66;03m# drop scalar coordinates\u001B[39;00m\n\u001B[0;32m   2466\u001B[0m     \u001B[38;5;66;03m# https://github.com/pydata/xarray/issues/6554\u001B[39;00m\n\u001B[0;32m   2467\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoords \u001B[38;5;129;01mand\u001B[39;00m drop \u001B[38;5;129;01mand\u001B[39;00m new_var\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mD:\\CondaEnvs\\torchtmp\\lib\\site-packages\\xarray\\core\\variable.py:1322\u001B[0m, in \u001B[0;36mVariable.isel\u001B[1;34m(self, indexers, missing_dims, **indexers_kwargs)\u001B[0m\n\u001B[0;32m   1319\u001B[0m indexers \u001B[38;5;241m=\u001B[39m drop_dims_from_indexers(indexers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdims, missing_dims)\n\u001B[0;32m   1321\u001B[0m key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(indexers\u001B[38;5;241m.\u001B[39mget(dim, \u001B[38;5;28mslice\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m)) \u001B[38;5;28;01mfor\u001B[39;00m dim \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdims)\n\u001B[1;32m-> 1322\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32mD:\\CondaEnvs\\torchtmp\\lib\\site-packages\\xarray\\core\\variable.py:869\u001B[0m, in \u001B[0;36mVariable.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    856\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m: T_Variable, key) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T_Variable:\n\u001B[0;32m    857\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return a new Variable object whose contents are consistent with\u001B[39;00m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;124;03m    getting the provided key from the underlying data.\u001B[39;00m\n\u001B[0;32m    859\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    867\u001B[0m \u001B[38;5;124;03m    array `x.values` directly.\u001B[39;00m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 869\u001B[0m     dims, indexer, new_order \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_broadcast_indexes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    870\u001B[0m     data \u001B[38;5;241m=\u001B[39m as_indexable(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data)[indexer]\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m new_order:\n",
      "File \u001B[1;32mD:\\CondaEnvs\\torchtmp\\lib\\site-packages\\xarray\\core\\variable.py:712\u001B[0m, in \u001B[0;36mVariable._broadcast_indexes\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    709\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(k, BASIC_INDEXING_TYPES) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m key):\n\u001B[0;32m    710\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_broadcast_indexes_basic(key)\n\u001B[1;32m--> 712\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_indexers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    713\u001B[0m \u001B[38;5;66;03m# Detect it can be mapped as an outer indexer\u001B[39;00m\n\u001B[0;32m    714\u001B[0m \u001B[38;5;66;03m# If all key is unlabeled, or\u001B[39;00m\n\u001B[0;32m    715\u001B[0m \u001B[38;5;66;03m# key can be mapped as an OuterIndexer.\u001B[39;00m\n\u001B[0;32m    716\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(k, Variable) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m key):\n",
      "File \u001B[1;32mD:\\CondaEnvs\\torchtmp\\lib\\site-packages\\xarray\\core\\variable.py:758\u001B[0m, in \u001B[0;36mVariable._validate_indexers\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    753\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\n\u001B[0;32m    754\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBoolean array size \u001B[39m\u001B[38;5;132;01m{:d}\u001B[39;00m\u001B[38;5;124m is used to index array \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    755\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith shape \u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mlen\u001B[39m(k), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape))\n\u001B[0;32m    756\u001B[0m     )\n\u001B[0;32m    757\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m k\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 758\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\n\u001B[0;32m    759\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m-dimensional boolean indexing is \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    760\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot supported. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k\u001B[38;5;241m.\u001B[39mndim)\n\u001B[0;32m    761\u001B[0m     )\n\u001B[0;32m    762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(k, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdims\u001B[39m\u001B[38;5;124m\"\u001B[39m, (dim,)) \u001B[38;5;241m!=\u001B[39m (dim,):\n\u001B[0;32m    763\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\n\u001B[0;32m    764\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBoolean indexer should be unlabeled or on the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    765\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msame dimension to the indexed array. Indexer is \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    768\u001B[0m         )\n\u001B[0;32m    769\u001B[0m     )\n",
      "\u001B[1;31mIndexError\u001B[0m: 2-dimensional boolean indexing is not supported. "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T13:22:10.926972Z",
     "start_time": "2024-08-13T13:22:10.909976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tempfunc(nc_file,type,depth):\n",
    "    daily_data = []\n",
    "    #zeta为海表没有深度，所以永远为0\n",
    "    if type=='zeta':\n",
    "        temp_lv0 = nc_file.variables[type][ :, :]\n",
    "    else:\n",
    "        temp_lv0 = nc_file.variables[type][depth, :, :]\n",
    "    #模拟20天的数据\n",
    "    for i in range(30):\n",
    "        daily_data.append(temp_lv0)\n",
    "    day_lon_lat = np.array(daily_data)\n",
    "    return day_lon_lat"
   ],
   "id": "24abeb23a9dea5b4",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T13:22:12.811448Z",
     "start_time": "2024-08-13T13:22:12.788455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset(data, time_step):\n",
    "    dataX = []\n",
    "    for i in range(data.shape[0] - time_step + 1):\n",
    "        dataX.append(data[i:i + time_step])\n",
    "    return np.array(dataX)\n",
    "def read_raw_data(vtype, depth, time_step,nc_file):\n",
    "    #训练用的数据是第0层，也就是海表，原来那个是按照深度进行划分的，这个nc文件是按天数进行划分的，这里只有一天，所以shape[0]=1\n",
    "    train_argo = tempfunc(nc_file,vtype,0)\n",
    "    label_argo = tempfunc(nc_file,vtype,depth)\n",
    "    width = train_argo.shape[2] #对应经度\n",
    "    lenth = train_argo.shape[1] #对应纬度\n",
    "    X = create_dataset(train_argo, time_step)\n",
    "    X = X.reshape(X.shape[0],time_step,lenth,width,1)\n",
    "    Y = label_argo[time_step-1 : label_argo.shape[0]] \n",
    "    Y =Y.reshape(Y.shape[0],lenth,width,1)\n",
    "    #X 转置维度，变为 (样本数, 时间步长, 通道数, 纬度, 经度)。\n",
    "    #Y 转置维度，变为 (样本数, 时间步长， 经度, 纬度)。\n",
    "    X = X.transpose(0,1,4,2,3)\n",
    "    Y = Y.transpose(0,3,1,2)\n",
    "    return X, Y"
   ],
   "id": "53226cd5b68809a9",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T13:23:27.043105Z",
     "start_time": "2024-08-13T13:23:26.998104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#这几个数据格式一样，但是内容不一样，读的分别是不同的列\n",
    "import netCDF4 as nc\n",
    "file_path = './subset_file_3.nc'\n",
    "file_obj = nc.Dataset(file_path)\n",
    "train_sssa,_=read_raw_data('s',0,3,file_obj)\n",
    "train_ssha,_ = read_raw_data('zeta',0,3,file_obj) #海面高度异常（Sea Level Anomaly）,他写的是sla，但是这里是zeta\n",
    "train_sswu,_ = read_raw_data('u',0,3,file_obj)#U vwnd分量的风速（即沿经度方向的风速）,这里是u\n",
    "train_sswv,_ = read_raw_data('v',0,3,file_obj)#V vwnd分量的风速（即沿纬度方向的风速），这里是v\n",
    "train_argo, label_argo = read_raw_data('t', 1, 3,file_obj)#temp 代表温度数据,预测深度为1时的海温"
   ],
   "id": "c36fa63b5fb9d102",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T13:23:28.758845Z",
     "start_time": "2024-08-13T13:23:28.743853Z"
    }
   },
   "cell_type": "code",
   "source": "train_sssa.shape,train_ssha.shape,train_sswu.shape,train_sswv.shape,label_argo.shape,train_argo.shape",
   "id": "e99324e88ec5d96c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28, 3, 1, 28, 52),\n",
       " (28, 3, 1, 28, 52),\n",
       " (28, 3, 1, 28, 52),\n",
       " (28, 3, 1, 28, 52),\n",
       " (28, 1, 28, 52),\n",
       " (28, 3, 1, 28, 52))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T16:54:42.995314Z",
     "start_time": "2024-08-15T16:54:42.875675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# 打开 NetCDF 数据集\n",
    "ds = xr.open_dataset('subset_file_3.nc')\n",
    "\"\"\"\n",
    "给zeta补深度\n",
    "\"\"\"\n",
    "zeta = ds['zeta']\n",
    "\n",
    "# 创建新的深度坐标，24层深度\n",
    "new_depth = np.arange(24)\n",
    "\n",
    "# 获取 zeta 的形状（假设它是2D：lat, lon）\n",
    "lat_dim, lon_dim = zeta.shape\n",
    "\n",
    "# 创建新的 zeta 数据数组，并初始化为 0\n",
    "new_zeta_data = np.zeros((24, lat_dim, lon_dim))  # 新的 zeta 数据，24层深度\n",
    "\n",
    "# 将原来的 zeta 数据放入新的数组中的第一层\n",
    "new_zeta_data[0, :, :] = zeta.values\n",
    "\n",
    "# 创建新的 zeta DataArray，并添加深度维度\n",
    "new_zeta = xr.DataArray(\n",
    "    data=new_zeta_data,\n",
    "    dims=['lv', 'lat', 'lon'],  # 新增 'lv' 维度，并保留原有纬度和经度\n",
    "    coords={'lv': new_depth, 'lat': zeta.coords['lat'], 'lon': zeta.coords['lon']}\n",
    ")\n",
    "\n",
    "# 将新的 zeta 数据加入原数据集\n",
    "ds['zeta'] = new_zeta\n",
    "\n",
    "# 检查结果\n",
    "print(ds['zeta'][22])\n",
    "\n",
    "# 定义三个点的经纬度\n",
    "points = {\n",
    "    'a': {'lon': 113.5, 'lat': 14.2},\n",
    "    'b': {'lon': 114.0, 'lat': 15.0},\n",
    "    'c': {'lon': 115.2, 'lat': 14.8}\n",
    "}\n",
    "\n",
    "# 提取24层深度剖面数据\n",
    "profiles = {}\n",
    "\n",
    "for point_name, coords in points.items():\n",
    "    profiles[point_name] = ds.sel(lon=coords['lon'], lat=coords['lat'], method='nearest')\n",
    "\n",
    "# 打印或保存提取的数据\n",
    "# for point_name, profile in profiles.items():\n",
    "#     print(f\"Profile data for point {point_name}:\")\n",
    "#     print(profile)\n",
    "#todo 把所有文件的点都提取出来\n"
   ],
   "id": "463b9360407ee0a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'zeta' (lat: 28, lon: 52)>\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 112.0 112.1 112.2 112.3 ... 116.8 116.9 117.0 117.1\n",
      "  * lat      (lat) float64 12.99 13.09 13.18 13.28 ... 15.32 15.41 15.51 15.61\n",
      "    lv       int32 22\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d639298626942018"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

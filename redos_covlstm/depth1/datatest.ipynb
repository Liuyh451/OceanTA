{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-14T01:56:48.338869Z",
     "start_time": "2024-08-14T01:56:48.130698Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:58.717457Z",
     "start_time": "2024-08-13T14:19:58.681454Z"
    }
   },
   "cell_type": "code",
   "source": "import xarray as xr",
   "id": "37b3833bda647501",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T01:57:49.613816Z",
     "start_time": "2024-08-14T01:57:49.603043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def increment_number(number_str):\n",
    "    # 自增数字部分\n",
    "    new_number = str(int(number_str) + 1).zfill(2)  # 保持两位数格式\n",
    "    return new_number\n",
    "num1='1'\n",
    "def extract_nc_layer_data(path,type,depth):\n",
    "    number_str = \"00\"\n",
    "    daily_data = []\n",
    "    #使用31天的数据\n",
    "    for i in range(31):\n",
    "        number_str = increment_number(number_str)\n",
    "        num=number_str\n",
    "        nc_file=path+'/subset_file_'+num+'.nc'\n",
    "        file_obj = nc.Dataset(nc_file)\n",
    "        #zeta为海表没有深度，所以永远为0\n",
    "        if type=='zeta':\n",
    "            temp_lv0 = file_obj.variables[type][ :, :]\n",
    "        else:\n",
    "            temp_lv0 = file_obj.variables[type][depth, :, :]\n",
    "        daily_data.append(temp_lv0)\n",
    "    day_lon_lat = np.array(daily_data)\n",
    "    return day_lon_lat\n"
   ],
   "id": "f396e8acb93c75fa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T01:57:53.878871Z",
     "start_time": "2024-08-14T01:57:53.871033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset(data, time_step):\n",
    "    dataX = []\n",
    "    for i in range(data.shape[0] - time_step + 1):\n",
    "        dataX.append(data[i:i + time_step])\n",
    "    return np.array(dataX)"
   ],
   "id": "b81fa10d5f578690",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T01:57:56.302403Z",
     "start_time": "2024-08-14T01:57:56.292612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_raw_data(vtype, depth, time_step,nc_file):\n",
    "    #训练用的数据是第0层，也就是海表，原来那个是按照深度进行划分的，这个nc文件是按天数进行划分的，这里只有一天，所以shape[0]=1\n",
    "    train_argo = extract_nc_layer_data(nc_file,vtype,0)\n",
    "    label_argo = extract_nc_layer_data(nc_file,vtype,depth)\n",
    "    width = train_argo.shape[2] #对应经度\n",
    "    lenth = train_argo.shape[1] #对应纬度\n",
    "    X = create_dataset(train_argo, time_step)\n",
    "    X = X.reshape(X.shape[0],time_step,lenth,width,1)\n",
    "    Y = label_argo[time_step-1 : label_argo.shape[0]] \n",
    "    Y =Y.reshape(Y.shape[0],lenth,width,1)\n",
    "    #X 转置维度，变为 (样本数, 时间步长, 通道数, 纬度, 经度)。\n",
    "    #Y 转置维度，变为 (样本数, 时间步长， 经度, 纬度)。\n",
    "    X = X.transpose(0,1,4,2,3)\n",
    "    Y = Y.transpose(0,3,1,2)\n",
    "    return X, Y"
   ],
   "id": "f2879410f524d19f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T01:58:03.536569Z",
     "start_time": "2024-08-14T01:58:01.028799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#这几个数据格式一样，但是内容不一样，读的分别是不同的列\n",
    "import netCDF4 as nc\n",
    "file_path = 'E:/DataSet/redos/REDOS_1.0_1994/1'\n",
    "train_sssa,_=read_raw_data('s',0,3,file_path)\n",
    "train_ssha,_ = read_raw_data('zeta',0,3,file_path) #海面高度异常（Sea Level Anomaly）,他写的是sla，但是这里是zeta\n",
    "train_sswu,_ = read_raw_data('u',0,3,file_path)#U vwnd分量的风速（即沿经度方向的风速）,这里是u\n",
    "train_sswv,_ = read_raw_data('v',0,3,file_path)#V vwnd分量的风速（即沿纬度方向的风速），这里是v\n",
    "train_argo, label_argo = read_raw_data('t', 1, 3,file_path)#temp 代表温度数据,预测深度为1时的海温"
   ],
   "id": "46d748e23247a1ec",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T01:58:08.855917Z",
     "start_time": "2024-08-14T01:58:08.843184Z"
    }
   },
   "cell_type": "code",
   "source": "train_sswv.shape,train_sswu.shape,train_sssa.shape,train_argo.shape,train_ssha.shape,label_argo.shape",
   "id": "ab42e2adf54dfb06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29, 3, 1, 28, 52),\n",
       " (29, 3, 1, 28, 52),\n",
       " (29, 3, 1, 28, 52),\n",
       " (29, 3, 1, 28, 52),\n",
       " (29, 3, 1, 28, 52),\n",
       " (29, 1, 28, 52))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.079489Z",
     "start_time": "2024-08-13T14:19:59.061490Z"
    }
   },
   "cell_type": "code",
   "source": "label_argo.shape,train_argo.shape\n",
   "id": "40fd7f973d210458",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28, 1, 28, 52), (28, 3, 1, 28, 52))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.140786Z",
     "start_time": "2024-08-13T14:19:59.118784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scaler(data):\n",
    "    #normalise [0,1]\n",
    "    data_max = np.nanmax(data)\n",
    "    data_min = np.nanmin(data)\n",
    "    data_scale = data_max - data_min\n",
    "    data_std = (data - data_min) / data_scale\n",
    "    # data_std = data_std * (2)  -1\n",
    "    data_std [np.isnan(data_std)] = 0\n",
    "    return data_std,data_min,data_scale\n",
    "\n",
    "#反归一化\n",
    "def unscaler(data, data_min, data_scale):\n",
    "    data_inv = (data * data_scale) + data_min\n",
    "    return data_inv"
   ],
   "id": "982481e7d6a7a5e4",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.171789Z",
     "start_time": "2024-08-13T14:19:59.147790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#对数据进行归一化\n",
    "sta_train,_,_ = scaler(train_argo[:-12,:])\n",
    "ssa_train,_,_  = scaler(train_sssa[:-12,:])\n",
    "ssha_train,_,_ = scaler(train_ssha[:-12,:])\n",
    "sswu_train,_,_ = scaler(train_sswu[:-12,:])\n",
    "sswv_train,_,_ = scaler(train_sswv[:-12,:])\n",
    "true_train,_,_ = scaler(label_argo[:-12,:])"
   ],
   "id": "e0d14250205e86e3",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.218791Z",
     "start_time": "2024-08-13T14:19:59.194789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#用倒数12个数据作为验证集\n",
    "sta_test,_,_ = scaler(train_argo[-12:])\n",
    "ssa_test,_,_  = scaler(train_sssa[-12:])\n",
    "ssha_test,_,_ = scaler(train_ssha[-12:])\n",
    "sswu_test,_,_ = scaler(train_sswu[-12:])\n",
    "sswv_test,_,_ = scaler(train_sswv[-12:])"
   ],
   "id": "179355ca045252ba",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.266795Z",
     "start_time": "2024-08-13T14:19:59.255799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_reduced(data,tag):\n",
    "    #全减\n",
    "    if tag==0:\n",
    "        data_new=data[:, :, :, :-1, :-1]\n",
    "    #第4维减1\n",
    "    elif tag==1:\n",
    "        data_new=data[:, :, :, :-1, :]\n",
    "    else:\n",
    "        data_new=data[:, :, :, :, :-1]\n",
    "    return data_new"
   ],
   "id": "44e734c1c8f79d52",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.313799Z",
     "start_time": "2024-08-13T14:19:59.289799Z"
    }
   },
   "cell_type": "code",
   "source": "sta_test.shape,ssa_test.shape,ssha_test.shape,sswu_test.shape,sswv_test.shape,label_argo.shape",
   "id": "11bdc4fdb816244b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 3, 1, 28, 52),\n",
       " (12, 3, 1, 28, 52),\n",
       " (12, 3, 1, 28, 52),\n",
       " (12, 3, 1, 28, 52),\n",
       " (12, 3, 1, 28, 52),\n",
       " (28, 1, 28, 52))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.329799Z",
     "start_time": "2024-08-13T14:19:59.316799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#将多个不同类型的训练数据和测试数据沿着指定轴进行拼接，axis=2即增加特征的数量（即通道或变量的数量）\n",
    "sta_train = np.concatenate((sta_train,ssa_train,ssha_train,sswu_train,sswv_train),axis = 2 )\n",
    "sta_test = np.concatenate((sta_test,ssa_test,ssha_test,sswu_test,sswv_test),axis = 2)"
   ],
   "id": "637aaabe12ea4b19",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.361803Z",
     "start_time": "2024-08-13T14:19:59.347803Z"
    }
   },
   "cell_type": "code",
   "source": "true_train.shape",
   "id": "ed2ef039110e3d21",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1, 28, 52)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.393806Z",
     "start_time": "2024-08-13T14:19:59.385808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# true_train=true_train[ :, :, :-1, :-1]\n",
    "# true_train.shape"
   ],
   "id": "1857739d9569e78d",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.457809Z",
     "start_time": "2024-08-13T14:19:59.439811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_test,test_min,test_scale = scaler(label_argo[-12:])\n",
    "#true_test是归一化后的 label_argo 数据，对应于最后 12 个时间步的标签数据\n",
    "#test_min是 label_argo[-12:] 数据中的最小值，在归一化过程中用作偏移量。\n",
    "#test_scale是 label_argo[-12:] 数据的范围，即最大值与最小值的差值。在归一化过程中用于缩放数据"
   ],
   "id": "6570683b81737540",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.489812Z",
     "start_time": "2024-08-13T14:19:59.478813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#将拼接后的数据作为训练集\n",
    "X_train = sta_train\n",
    "#训练集的标签\n",
    "true_train = true_train\n",
    "\n",
    "#训练集上用于评估\n",
    "X_eval = sta_test\n",
    "#\n",
    "true_eval = true_test\n",
    "X_test = sta_test\n",
    "true_test = true_test\n",
    "true_test.shape,X_eval.shape"
   ],
   "id": "b77d32c0d5755c87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 1, 28, 52), (12, 3, 5, 28, 52))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.537818Z",
     "start_time": "2024-08-13T14:19:59.516815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset"
   ],
   "id": "6a7cfe733af2e182",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.569829Z",
     "start_time": "2024-08-13T14:19:59.554829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Configs:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "configs = Configs()\n",
    "\n",
    "# trainer related\n",
    "configs.vtype = 't'\n",
    "# configs.depth = 11\n",
    "# configs.time_step = 1\n",
    "configs.n_cpu = 0\n",
    "# configs.device = torch.device('cpu')\n",
    "configs.device = torch.device('cuda:0')\n",
    "configs.batch_size_test = 4\n",
    "configs.batch_size = 2\n",
    "#configs.lr = 0.001\n",
    "configs.weight_decay = 0\n",
    "configs.display_interval = 5\n",
    "configs.num_epochs = 10\n",
    "#这是早停的耐心参数。即使模型在900个epoch内没有改善性能，训练仍会继续。如果在900个epoch内性能没有改善，训练将停止\n",
    "configs.early_stopping = True\n",
    "configs.patience = 10\n",
    "#禁用梯度裁剪（Gradient Clipping）。梯度裁剪用于防止梯度爆炸问题，但在这里未启用\n",
    "configs.gradient_clipping = False\n",
    "#设置梯度裁剪的阈值为1。如果梯度裁剪启用，梯度的最大值将被限制为1。不过在这种配置下，由于梯度裁剪被禁用，这个参数实际上不会生效\n",
    "configs.clipping_threshold = 1.\n",
    "\n",
    "# lr warmup\n",
    "#这是学习率预热的步数设置。在训练的前3000步内，学习率将逐渐从一个较小的值线性增加到预设的学习率。这种技术通常用于训练的初始阶段，以帮助模型更稳定地开始训练，减少初期的震荡。\n",
    "configs.warmup = 3000\n",
    "\n",
    "# data related\n",
    "#这是输入数据的维度设置。这通常取决于你使用的数据的特征数或通道数\n",
    "configs.input_dim = 1 # 4 #这里应该是5吧 但是写的1我总感觉是5\n",
    "'''\n",
    "人家这个1是对的这个模型就是要保证输入通道和输出通道得一样\n",
    "默认为1\n",
    "'''\n",
    "configs.output_dim = 1\n",
    "#表示模型的输入序列长度为5，即模型在预测时会使用前5个时间步的数据作为输入\n",
    "configs.input_length = 5\n",
    "#表示模型的输出长度为1，即模型预测一个时间步的值。通常用于单步预测\n",
    "configs.output_length = 1\n",
    "#表示输入序列中的数据点之间的时间间隔为1。即数据是逐步连续的，没有跳跃\n",
    "configs.input_gap = 1\n",
    "#表示预测的时间偏移量为24。这可能意味着模型的目标是预测未来24个时间步后的数据点\n",
    "configs.pred_shift = 24\n",
    "#这个列表包含了一系列的深度值，这可能与模型的层次结构或者不同深度的输入特征相关联\n",
    "configs.depth = [5,6,11,16,20,25,30,34,36,38,40,42,44,46,48,50,51,52,53,54,55,57]\n",
    "#这个列表可能对应于不同深度的索引或层次级别。每个索引可能用于定位或选择特定深度的特征或数据\n",
    "configs.depthindex = [30,50, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900]\n",
    "\n",
    "# model\n",
    "#表示模型的维度即每个输入数据在模型中的表示为256维\n",
    "configs.d_model = 256\n",
    "#表示模型处理数据时的patch（小块）的大小为5×5。这通常用于图像或序列数据的分块处理\n",
    "configs.patch_size = (5,5)\n",
    "#表示嵌入的空间尺寸。这里12*16可能是表示最终嵌入的特征图的尺寸（例如视觉模型中的特征图大小）\n",
    "configs.emb_spatial_size = 12*16\n",
    "#表示多头注意力机制中的头数为4。多头注意力允许模型从不同的角度“看”数据，从而捕捉不同的关系\n",
    "configs.nheads = 4\n",
    "#表示前馈神经网络的维度用于增加模型的表达能力\n",
    "configs.dim_feedforward =512\n",
    "#表示在模型中使用的dropout率为0.3。Dropout是一种正则化技术，用于减少过拟合。\n",
    "configs.dropout = 0.3\n",
    "#表示编码器的层数为4。这意味着模型有4个堆叠的编码器层\n",
    "configs.num_encoder_layers = 4\n",
    "configs.num_decoder_layers = 4\n",
    "#这可能是学习率的衰减率（scheduler decay rate），用来控制模型训练过程中学习率的递减速度，以便在训练的后期进行更细致的优化\n",
    "configs.ssr_decay_rate = 3.e-6\n",
    "\n",
    "\n",
    "# plot 表示绘图的分辨率为600 DPI\n",
    "configs.plot_dpi = 600\n"
   ],
   "id": "237be25d7eb6364e",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.694491Z",
     "start_time": "2024-08-13T14:19:59.591832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class covlstmformer(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super().__init__()\n",
    "        self.configs = configs\n",
    "        self.d_model = 25\n",
    "        self.device = configs.device\n",
    "        #5,输入通道数（或输入特征图的通道数）。通常对应于输入数据的深度。\n",
    "        #8：输出通道数（或输出特征图的通道数）。表示卷积操作后输出的特征图的深度。\n",
    "        #3：卷积核的大小，通常表示一个 3x3 的卷积核（filter）。\n",
    "        #5：步幅（stride）\n",
    "        self.cov1 = Cov(5, 8,3, 5)\n",
    "        self.cov2 = Cov(5, 8,3, 5)\n",
    "        #两个编码器\n",
    "        self.encode1 = EncoderLayer(self.d_model, 1, configs.dim_feedforward, configs.dropout)\n",
    "        self.encode2 = EncoderLayer(self.d_model, 1, configs.dim_feedforward, configs.dropout)\n",
    "        self.cov_last  = Cov_last(5, 8,3, 1)\n",
    "    def forward(self,x):\n",
    "        resdual1 = self.cov1(x)\n",
    "        #将特征图按 (5, 5) 大小的块展开或重新排列\n",
    "        resdual1 = unfold_StackOverChannel(resdual1, (5, 5))\n",
    "        x = resdual1\n",
    "        #跳跃连接操作\n",
    "        x = resdual1 + self.encode1(x)\n",
    "         # Debug: Print shape after first addition\n",
    "        print(\"Shape after resdual1 + self.encode1(x):\", x.shape)\n",
    "        #函数将特征图 x 折叠成 (60, 80) 尺寸，可能对应于输入尺寸的恢复\n",
    "        x = fold_tensor(x, (28, 52), (5, 5))\n",
    "        \n",
    "        resdual2 = x + self.cov2(x) # xiu gai 的地方在这\n",
    "        resdual2 = unfold_StackOverChannel(resdual2, (5, 5))\n",
    "        x = resdual2\n",
    "        x = resdual2 + self.encode2(x)\n",
    "        # Debug: Print shape after second addition\n",
    "        print(\"Shape after resdual2 + self.encode2(x):\", x.shape)\n",
    "        x = fold_tensor(x, (28, 52), (5, 5))\n",
    "        x = self.cov_last(x)\n",
    "        return x\n",
    "#编码器层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nheads, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        #实现了时间维度上的多头注意力机制\n",
    "        self.time_attn = MultiHeadedAttention(d_model, nheads, TimeAttention, dropout)\n",
    "        #实现了空间维度上的多头注意力机制\n",
    "        self.space_attn = MultiHeadedAttention(d_model, nheads, SpaceAttention, dropout)\n",
    "        '''\n",
    "        #一个更复杂的全连接网络（被注释掉），可以用于替代 feed_forward 部分\n",
    "        self.net = nn.Sequential(\n",
    "                  nn.Linear(256, 25),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(25, 256),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(256, 512),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(512,256)\n",
    "                  )\n",
    "        '''\n",
    "        #前馈神经网络，用于进一步处理通过注意力机制后的输出。该网络包括两个线性层和ReLU激活函数，用于非线性映射和特征提取\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "            )\n",
    "    '''\n",
    "    #一个分离空间和时间注意力的实现（被注释掉），可能用于更精细地控制注意力的应用顺序\n",
    "    def divided_space_time_attn(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        Apply space and time attention sequentially\n",
    "        Args:\n",
    "            query (N, S, T, D)\n",
    "            key (N, S, T, D)\n",
    "            value (N, S, T, D)\n",
    "        Returns:\n",
    "            (N, S, T, D)\n",
    "        \"\"\"\n",
    "        m = self.time_attn(query, key, value, mask)\n",
    "        return self.space_attn(m, m, m, mask)\n",
    "    '''\n",
    "    def forward(self, x, mask=None):\n",
    "        # x = self.sublayer[0](x, lambda x: self.divided_space_time_attn(x, x, x, mask))\n",
    "        # x = x + self.net(x)\n",
    "        # return self.sublayer[1](x, self.feed_forward)\n",
    "        #融合时间和空间注意力机制\n",
    "        x = x + self.time_attn(x, x, x, mask)\n",
    "        x = x+ self.space_attn(x, x, x,mask)\n",
    "        #应用前馈神经网络处理，并将结果与经过空间注意力后的输出相加，生成最终的编码器输出。\n",
    "        x = x + self.feed_forward(x)\n",
    "        return x\n",
    "#卷积长短时记忆网络（ConvLSTM）的单元 ConvLSTMCell\n",
    "class ConvLSTMCell(nn.Module):\n",
    " \n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    " \n",
    "        super(ConvLSTMCell, self).__init__()\n",
    " \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.kernel_size = kernel_size\n",
    "        #根据卷积核的大小，自动计算填充大小，以确保输入和输出张量的空间尺寸（高度和宽度）一致。\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2 # 保证在传递过程中 （h,w）不变\n",
    "        #是否在卷积操作中添加偏置项\n",
    "        self.bias = bias\n",
    " #定义了一个二维卷积层，该卷积层接收输入张量和当前隐藏状态张量的拼接，并输出 4 倍的隐藏状态张量大小，用于计算 LSTM 的四个门（输入门 i、遗忘门 f、输出门 o 和候选状态 g）。\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim, # i门，f门，o门，g门放在一起计算，然后在split开\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    " \n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state # 每个timestamp包含两个状态张量：h和c\n",
    " \n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis # 把输入张量与h状态张量沿通道维度串联\n",
    " \n",
    "        combined_conv = self.conv(combined) # i门，f门，o门，g门放在一起计算，然后在split开\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    " \n",
    "        c_next = f * c_cur + i * g  # c状态张量更新\n",
    "        h_next = o * torch.tanh(c_next) # h状态张量更新\n",
    " \n",
    "        return h_next, c_next # 输出当前timestamp的两个状态张量\n",
    " \n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        \"\"\"\n",
    "        初始状态张量初始化.第一个timestamp的状态张量0初始化\n",
    "        :param batch_size:\n",
    "        :param image_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        height, width = image_size\n",
    "        init_h = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "        init_c = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "        return (init_h,init_c)\n",
    " \n",
    " \n",
    "class ConvLSTM(nn.Module):\n",
    " \n",
    "    \"\"\"\n",
    "    Parameters:参数介绍\n",
    "        input_dim: Number of channels in input# 输入张量的通道数\n",
    "        hidden_dim: Number of hidden channels # h,c两个状态张量的通道数，可以是一个列表\n",
    "        kernel_size: Size of kernel in convolutions # 卷积核的尺寸，默认所有层的卷积核尺寸都是一样的,也可以设定不通lstm层的卷积核尺寸不同\n",
    "        num_layers: Number of LSTM layers stacked on each other # 卷积层的层数，需要与len(hidden_dim)相等\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers # 是否返回所有lstm层的h状态\n",
    "        Note: Will do same padding. # 相同的卷积核尺寸，相同的padding尺寸\n",
    "    Input:输入介绍\n",
    "        A tensor of size [B, T, C, H, W] or [T, B, C, H, W]# 需要是5维的\n",
    "    Output:输出介绍\n",
    "        返回的是两个列表：layer_output_list，last_state_list\n",
    "        列表0：layer_output_list--单层列表，每个元素表示一层LSTM层的输出h状态,每个元素的size=[B,T,hidden_dim,H,W]\n",
    "        列表1：last_state_list--双层列表，每个元素是一个二元列表[h,c],表示每一层的最后一个timestamp的输出状态[h,c],h.size=c.size = [B,hidden_dim,H,W]\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:使用示例\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    " \n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    " \n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers) # 转为列表\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers) # 转为列表\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers: # 判断一致性\n",
    "            raise ValueError('Inconsistent list length.')\n",
    " \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    " \n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers): # 多层LSTM设置\n",
    "            # 当前LSTM层的输入维度\n",
    "            # if i==0:\n",
    "            #     cur_input_dim = self.input_dim\n",
    "            # else:\n",
    "            #     cur_input_dim = self.hidden_dim[i - 1]\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1] # 与上等价\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    " \n",
    "        self.cell_list = nn.ModuleList(cell_list) # 把定义的多个LSTM层串联成网络模型\n",
    " \n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: 5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    " \n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            b, _, _, h, w = input_tensor.size()  # 自动获取 b,h,w信息\n",
    "            hidden_state = self._init_hidden(batch_size=b,image_size=(h, w))\n",
    " \n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    " \n",
    "        seq_len = input_tensor.size(1) # 根据输入张量获取lstm的长度\n",
    "        cur_layer_input = input_tensor\n",
    " \n",
    "        for layer_idx in range(self.num_layers): # 逐层计算\n",
    " \n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len): # 逐个stamp计算\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],cur_state=[h, c])\n",
    "                output_inner.append(h) # 第 layer_idx 层的第t个stamp的输出状态\n",
    " \n",
    "            layer_output = torch.stack(output_inner, dim=1) # 第 layer_idx 层的第所有stamp的输出状态串联\n",
    "            cur_layer_input = layer_output # 准备第layer_idx+1层的输入张量\n",
    " \n",
    "            layer_output_list.append(layer_output) # 当前层的所有timestamp的h状态的串联\n",
    "            last_state_list.append([h, c]) # 当前层的最后一个stamp的输出状态的[h,c]\n",
    " \n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    " \n",
    "        return layer_output_list, last_state_list\n",
    " \n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        \"\"\"\n",
    "        所有lstm层的第一个timestamp的输入状态0初始化\n",
    "        :param batch_size:\n",
    "        :param image_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    " \n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        \"\"\"\n",
    "        检测输入的kernel_size是否符合要求，要求kernel_size的格式是list或tuple\n",
    "        :param kernel_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    " \n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        \"\"\"\n",
    "        扩展到多层lstm情况\n",
    "        :param param:\n",
    "        :param num_layers:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n",
    "\n",
    "class Cov(nn.Module):\n",
    "     def __init__(self, intput_dim, hidden_dim, bn_dim, output_dim):# bn_dim是时间步\n",
    "          super().__init__()\n",
    "          self.cov1 = ConvLSTM(input_dim = intput_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn1 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov2 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn2 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov3 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = output_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "     def forward(self, x):\n",
    "          x,_ = self.cov1(x) # 因为上面的Covlstm返回两个值所以先用_接住第二个用不到的值\n",
    "          x = self.bn1(x[0])\n",
    "          x,_ = self.cov2(x)\n",
    "          x = self.bn2(x[0])\n",
    "          x,_ = self.cov3(x)\n",
    "          x = x[0]\n",
    "          return x\n",
    "     \n",
    "class Cov_last(nn.Module):\n",
    "     def __init__(self, intput_dim, hidden_dim, bn_dim, output_dim):# bn_dim是时间步\n",
    "          super().__init__()\n",
    "          self.cov1 = ConvLSTM(input_dim = intput_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn1 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov2 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn2 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov3 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = output_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "     def forward(self, x):\n",
    "          x,_ = self.cov1(x) # 因为上面的Covlstm返回两个值所以先用_接住第二个用不到的值\n",
    "          x = self.bn1(x[0])\n",
    "          x,_ = self.cov2(x)\n",
    "          x = self.bn2(x[0])\n",
    "          x,_ = self.cov3(x)\n",
    "          x = x[0]\n",
    "          return x[:, -1] \n",
    "     \n",
    "def unfold_StackOverChannel(img, kernel_size):\n",
    "    \"\"\"\n",
    "    divide the original image to patches, then stack the grids in each patch along the channels\n",
    "    Args:\n",
    "        img (N, *, C, H, W): the last two dimensions must be the spatial dimension\n",
    "        kernel_size: tuple of length 2\n",
    "    Returns:\n",
    "        output (N, *, C*H_k*N_k, H_output, W_output)\n",
    "    \"\"\"\n",
    "    T = img.size(1)\n",
    "    n_dim = len(img.size())\n",
    "    assert n_dim == 4 or n_dim == 5\n",
    "\n",
    "    pt = img.unfold(-2, size=kernel_size[0], step=kernel_size[0])\n",
    "    pt = pt.unfold(-2, size=kernel_size[1], step=kernel_size[1]).flatten(-2)  # (N, *, C, n0, n1, k0*k1)\n",
    "    if n_dim == 4:  # (N, C, H, W)\n",
    "        pt = pt.permute(0, 1, 4, 2, 3).flatten(1, 2)\n",
    "    elif n_dim == 5:  # (N, T, C, H, W)\n",
    "        pt = pt.permute(0, 1, 2, 5, 3, 4).flatten(2, 3)\n",
    "    assert pt.size(-3) == img.size(-3) * kernel_size[0] * kernel_size[1]\n",
    "    pt = pt.reshape(pt.size(0), T, 25, -1).permute(0, 3, 1, 2)\n",
    "    return pt     \n",
    "def fold_tensor(tensor, output_size, kernel_size):\n",
    "    \"\"\"\n",
    "    reconstruct the image from its non-overlapping patches\n",
    "    Args:\n",
    "        input tensor of size (N, *, C*k_h*k_w, n_h, n_w)\n",
    "        output_size of size(H, W), the size of the original image to be reconstructed\n",
    "        kernel_size: (k_h, k_w)\n",
    "        stride is usually equal to kernel_size for non-overlapping sliding window\n",
    "    Returns:\n",
    "        (N, *, C, H=n_h*k_h, W=n_w*k_w)\n",
    "    \"\"\"\n",
    "    tensor = tensor.reshape(-1,50,3,25)\n",
    "    T = tensor.size(2)\n",
    "    tensor = tensor.permute(0, 2, 3, 1)  # (N, T, C_, S)\n",
    "    tensor = tensor.reshape(tensor.size(0), T, 25,\n",
    "                                5, 10)\n",
    "    tensor = tensor.float()\n",
    "    n_dim = len(tensor.size())\n",
    "    assert n_dim == 4 or n_dim == 5\n",
    "    f = tensor.flatten(0, 1) if n_dim == 5 else tensor\n",
    "    folded = F.fold(f.flatten(-2), output_size=output_size, kernel_size=kernel_size, stride=kernel_size)\n",
    "    if n_dim == 5:\n",
    "        folded = folded.reshape(tensor.size(0), tensor.size(1), *folded.size()[1:])\n",
    "    return folded.reshape(-1,T,5,28,52)\n",
    "\n",
    "\n",
    "def TimeAttention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    attention over the time axis\n",
    "    Args:\n",
    "        query, key, value: linearly-transformed query, key, value (N, h, S, T, D)\n",
    "        mask: of size (T (query), T (key)) specifying locations (which key) the query can and cannot attend to\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)  # (N, h, S, T, T)\n",
    "    if mask is not None:\n",
    "        assert mask.dtype == torch.bool\n",
    "        assert len(mask.size()) == 2\n",
    "        scores = scores.masked_fill(mask[None, None, None], float(\"-inf\"))\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value)  # (N, h, S, T, D)\n",
    "\n",
    "\n",
    "def SpaceAttention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    attention over the two space axes\n",
    "    Args:\n",
    "        query, key, value: linearly-transformed query, key, value (N, h, S, T, D)\n",
    "        mask: None (space attention does not need mask), this argument is intentionally set for consistency\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    query = query.transpose(2, 3)  # (N, h, T, S, D)\n",
    "    key = key.transpose(2, 3)  # (N, h, T, S, D)\n",
    "    value = value.transpose(2, 3)  # (N, h, T, S, D)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)  # (N, h, T, S, S)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value).transpose(2, 3)  # (N, h, S, T_q, D)\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, nheads, attn, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model % nheads == 0\n",
    "        self.d_k = d_model // nheads\n",
    "        self.nheads = nheads\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.attn = attn\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Transform the query, key, value into different heads, then apply the attention in parallel\n",
    "        Args:\n",
    "            query, key, value: size (N, S, T, D)\n",
    "        Returns:\n",
    "            (N, S, T, D)\n",
    "        \"\"\"\n",
    "        nbatches = query.size(0)\n",
    "        nspace = query.size(1)\n",
    "        ntime = query.size(2)\n",
    "        # (N, h, S, T, d_k)\n",
    "        query, key, value = \\\n",
    "            [l(x).view(x.size(0), x.size(1), x.size(2), self.nheads, self.d_k).permute(0, 3, 1, 2, 4)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # (N, h, S, T, d_k)\n",
    "        x = self.attn(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # (N, S, T, D)\n",
    "        x = x.permute(0, 2, 3, 1, 4).contiguous() \\\n",
    "             .view(nbatches, nspace, ntime, self.nheads * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dropout,attn):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(50, 50) for _ in range(3)])\n",
    "        self.attn = attn\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query, key, value = \\\n",
    "        [l(x)\n",
    "        for l, x in zip(self.linears, (query, key, value))]\n",
    "        x = self.attn(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        return x\n"
   ],
   "id": "92ecc8ccc612c761",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.742498Z",
     "start_time": "2024-08-13T14:19:59.697495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NoamOpt:\n",
    "    \"\"\"\n",
    "    learning rate warmup and decay\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "               (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, configs):\n",
    "        self.configs = configs\n",
    "        self.device = configs.device\n",
    "        torch.manual_seed(5)\n",
    "        self.network  = covlstmformer(configs).to(configs.device)\n",
    "        adam = torch.optim.Adam(self.network.parameters(), lr=0, weight_decay=configs.weight_decay)\n",
    "        factor = math.sqrt(configs.d_model * configs.warmup) * 0.0014\n",
    "        self.opt = NoamOpt(configs.d_model, factor, warmup=configs.warmup, optimizer=adam)\n",
    "\n",
    "\n",
    "    def loss_sst(self, y_pred, y_true):\n",
    "        # y_pred/y_true (N, 26, 24, 48)\n",
    "        rmse = torch.mean((y_pred - y_true) ** 2, dim=[2, 3])\n",
    "        rmse = torch.sum(rmse.sqrt().mean(dim=0))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "\n",
    "    def train_once(self, input_sst, sst_true, ssr_ratio):\n",
    "        sst_pred = self.network(input_sst.float().to(self.device))\n",
    "        self.opt.optimizer.zero_grad()\n",
    "        loss_sst = self.loss_sst(sst_pred, sst_true.float().to(self.device))\n",
    "        # loss_nino = self.loss_nino(nino_pred, nino_true.float().to(self.device))\n",
    "        loss_sst.backward()\n",
    "        if configs.gradient_clipping:\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), configs.clipping_threshold)\n",
    "        self.opt.step()\n",
    "        return loss_sst.item()\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        # nino_pred = []\n",
    "        sst_pred = []\n",
    "        with torch.no_grad():\n",
    "            for input_sst, sst_true, in dataloader_test:\n",
    "                sst = self.network(input_sst.float().to(self.device))\n",
    "                # nino_pred.append(nino)\n",
    "                sst_pred.append(sst)\n",
    "\n",
    "        return torch.cat(sst_pred, dim=0)\n",
    "\n",
    "    def infer(self, dataset, dataloader):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            sst_pred = self.test(dataloader)\n",
    "            # nino_true = torch.from_numpy(dataset.target_nino).float().to(self.device)\n",
    "            sst_true = torch.from_numpy(dataset.target_sst).float().to(self.device)\n",
    "            # sc = self.score(nino_pred, nino_true)\n",
    "#             print(sst_pred.shape)\n",
    "#             print(sst_true.shape)\n",
    "            loss_sst = self.loss_sst(sst_pred, sst_true).item()\n",
    "            # loss_nino = self.loss_nino(nino_pred, nino_true).item()\n",
    "        return loss_sst\n",
    "\n",
    "    def infer_test(self, dataset, dataloader):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            sst_pred = self.test(dataloader)\n",
    "            # nino_true = torch.from_numpy(dataset.target_nino).float().to(self.device)\n",
    "            sst_true = torch.from_numpy(dataset.target_sst).float().to(self.device)\n",
    "            # sc = self.score(nino_pred, nino_true)\n",
    "            loss_sst = self.loss_sst(sst_pred, sst_true).item()\n",
    "            # loss_nino = self.loss_nino(nino_pred, nino_true).item()\n",
    "        return loss_sst, sst_pred, sst_true\n",
    "\n",
    "    def train(self, dataset_train, dataset_eval, chk_path):\n",
    "        torch.manual_seed(0)\n",
    "        # print('loading train dataloader')\n",
    "        dataloader_train = DataLoader(dataset_train, batch_size=self.configs.batch_size, shuffle=True)\n",
    "        # print('loading eval dataloader')\n",
    "        dataloader_eval = DataLoader(dataset_eval, batch_size=self.configs.batch_size_test, shuffle=False)\n",
    "       \n",
    "        count = 0\n",
    "        best = math.inf\n",
    "        ssr_ratio = 1\n",
    "        for i in range(self.configs.num_epochs):\n",
    "            print('\\nepoch: {0}'.format(i + 1))\n",
    "            # train\n",
    "            self.network.train()\n",
    "            for j, (input_sst, sst_true) in enumerate(dataloader_train):\n",
    "                print(j,input_sst.shape,sst_true.shape)\n",
    "                if ssr_ratio > 0:\n",
    "                    ssr_ratio = max(ssr_ratio - self.configs.ssr_decay_rate, 0)\n",
    "                    print(\"ssr_ratio\",ssr_ratio)\n",
    "                loss_sst = self.train_once(input_sst, sst_true, ssr_ratio)  # y_pred for one batch\\\n",
    "                print(\"1111111111111\",loss_sst)\n",
    "\n",
    "                if j % self.configs.display_interval == 0:\n",
    "\n",
    "                    print('batch training loss: {:.5f}, ssr: {:.5f}, lr: {:.5f}'.format(loss_sst, ssr_ratio, self.opt.rate()))\n",
    "\n",
    "                # increase the number of evaluations in order not to miss the optimal point\n",
    "                # which is feasible because of the less training time of timesformer\n",
    "                if (i + 1 >= 9) and (j + 1) % 300 == 0:\n",
    "                    loss_sst_eval = self.infer(dataset=dataset_eval, dataloader=dataloader_eval)\n",
    "                    print('epoch eval loss: sc: {:.4f}'.format(loss_sst_eval))\n",
    "                    if loss_sst_eval < best:\n",
    "                        self.save_model(chk_path)\n",
    "                        best = loss_sst_eval\n",
    "                        count = 0\n",
    "\n",
    "            # evaluation\n",
    "            loss_sst_eval = self.infer(dataset=dataset_eval, dataloader=dataloader_eval)\n",
    "            print('epoch eval loss:\\nsst: {:.2f}'.format(loss_sst_eval))\n",
    "            if loss_sst_eval >= best:\n",
    "                count += 1\n",
    "                print('eval score is not improved for {} epoch'.format(count))\n",
    "            else:\n",
    "                count = 0\n",
    "                print('eval score is improved from {:.5f} to {:.5f}, saving model'.format(best, loss_sst_eval))\n",
    "                self.save_model(chk_path)\n",
    "                best = loss_sst_eval\n",
    "\n",
    "            if count == self.configs.patience:\n",
    "                print('early stopping reached, best score is {:5f}'.format(best))\n",
    "                break\n",
    "\n",
    "    def save_configs(self, config_path):\n",
    "        with open(config_path, 'wb') as path:\n",
    "            pickle.dump(self.configs, path)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save({'net': self.network.state_dict(),\n",
    "                    'optimizer': self.opt.optimizer.state_dict()}, path)\n",
    "        "
   ],
   "id": "e2fff2dedf97ed62",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.758498Z",
     "start_time": "2024-08-13T14:19:59.745497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class cmip_dataset(Dataset):\n",
    "    def __init__(self, datax,datay):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_sst = datax\n",
    "        self.target_sst = datay\n",
    "\n",
    "\n",
    "    def GetDataShape(self):\n",
    "        return {'sst input': self.input_sst.shape,\n",
    "                'sst target': self.target_sst.shape}\n",
    "\n",
    "    def __len__(self,):\n",
    "        return self.input_sst.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_sst[idx], self.target_sst[idx]"
   ],
   "id": "f70960cea11e1c34",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.773499Z",
     "start_time": "2024-08-13T14:19:59.761499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_train = cmip_dataset(X_train,true_train)\n",
    "print(dataset_train.GetDataShape())"
   ],
   "id": "1d9e727cf354fade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst input': (16, 3, 5, 28, 52), 'sst target': (16, 1, 28, 52)}\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.789498Z",
     "start_time": "2024-08-13T14:19:59.778501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_eval = cmip_dataset(X_eval,true_eval)\n",
    "print(dataset_eval.GetDataShape())"
   ],
   "id": "3726ce816ba35504",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst input': (12, 3, 5, 28, 52), 'sst target': (12, 1, 28, 52)}\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:19:59.836506Z",
     "start_time": "2024-08-13T14:19:59.792502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(configs)\n",
    "trainer.save_configs('config_train.pkl')\n"
   ],
   "id": "d557c5956dbda86f",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:20:07.151608Z",
     "start_time": "2024-08-13T14:19:59.839506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "trainer.train(dataset_train, dataset_eval, 'checkpoint.chk')"
   ],
   "id": "49f2b703cde2b432",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1\n",
      "0 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.999997\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9412862062454224\n",
      "batch training loss: 0.94129, ssr: 1.00000, lr: 0.00000\n",
      "1 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.999994\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9430834650993347\n",
      "2 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999910000000001\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9428662061691284\n",
      "3 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999880000000001\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.942020058631897\n",
      "4 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999850000000001\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9413566589355469\n",
      "5 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999820000000001\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9415388107299805\n",
      "batch training loss: 0.94154, ssr: 0.99998, lr: 0.00000\n",
      "6 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999790000000002\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9413255453109741\n",
      "7 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999760000000002\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9400910139083862\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "epoch eval loss:\n",
      "sst: 1.02\n",
      "eval score is improved from inf to 1.02114, saving model\n",
      "\n",
      "epoch: 2\n",
      "0 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999730000000002\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9389810562133789\n",
      "batch training loss: 0.93898, ssr: 0.99997, lr: 0.00000\n",
      "1 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999700000000002\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9378491044044495\n",
      "2 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999670000000003\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9368577003479004\n",
      "3 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999640000000003\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.935357928276062\n",
      "4 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999610000000003\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9348154067993164\n",
      "5 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999580000000003\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9347919225692749\n",
      "batch training loss: 0.93479, ssr: 0.99996, lr: 0.00001\n",
      "6 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999550000000004\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9309568405151367\n",
      "7 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999520000000004\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9312669634819031\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "epoch eval loss:\n",
      "sst: 1.02\n",
      "eval score is improved from 1.02114 to 1.01954, saving model\n",
      "\n",
      "epoch: 3\n",
      "0 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999490000000004\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9294267892837524\n",
      "batch training loss: 0.92943, ssr: 0.99995, lr: 0.00001\n",
      "1 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999460000000004\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9280672073364258\n",
      "2 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999430000000005\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9268235564231873\n",
      "3 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999400000000005\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.924463152885437\n",
      "4 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999370000000005\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9230304956436157\n",
      "5 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999340000000005\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9202917814254761\n",
      "batch training loss: 0.92029, ssr: 0.99993, lr: 0.00001\n",
      "6 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999310000000006\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9180530905723572\n",
      "7 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999280000000006\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9153056144714355\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "epoch eval loss:\n",
      "sst: 1.01\n",
      "eval score is improved from 1.01954 to 1.01331, saving model\n",
      "\n",
      "epoch: 4\n",
      "0 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999250000000006\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.913805365562439\n",
      "batch training loss: 0.91381, ssr: 0.99993, lr: 0.00001\n",
      "1 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999220000000006\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9104698300361633\n",
      "2 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999190000000007\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9082641005516052\n",
      "3 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999160000000007\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9058562517166138\n",
      "4 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999130000000007\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9029226303100586\n",
      "5 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999100000000007\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.9009190201759338\n",
      "batch training loss: 0.90092, ssr: 0.99991, lr: 0.00001\n",
      "6 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999070000000008\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8971728086471558\n",
      "7 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999040000000008\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8936326503753662\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "epoch eval loss:\n",
      "sst: 1.00\n",
      "eval score is improved from 1.01331 to 0.99752, saving model\n",
      "\n",
      "epoch: 5\n",
      "0 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9999010000000008\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8905234336853027\n",
      "batch training loss: 0.89052, ssr: 0.99990, lr: 0.00002\n",
      "1 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998980000000008\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8870888948440552\n",
      "2 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998950000000009\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.88283771276474\n",
      "3 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998920000000009\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8799858689308167\n",
      "4 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998890000000009\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8757350444793701\n",
      "5 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998860000000009\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8726441860198975\n",
      "batch training loss: 0.87264, ssr: 0.99989, lr: 0.00002\n",
      "6 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.999883000000001\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8682159185409546\n",
      "7 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.999880000000001\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8622846007347107\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "epoch eval loss:\n",
      "sst: 0.97\n",
      "eval score is improved from 0.99752 to 0.96610, saving model\n",
      "\n",
      "epoch: 6\n",
      "0 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.999877000000001\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8593196272850037\n",
      "batch training loss: 0.85932, ssr: 0.99988, lr: 0.00002\n",
      "1 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.999874000000001\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.855248212814331\n",
      "2 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998710000000011\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8497843742370605\n",
      "3 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998680000000011\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.846514880657196\n",
      "4 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998650000000011\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8410025835037231\n",
      "5 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998620000000011\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8351978659629822\n",
      "batch training loss: 0.83520, ssr: 0.99986, lr: 0.00002\n",
      "6 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998590000000012\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8306775689125061\n",
      "7 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998560000000012\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8259172439575195\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "epoch eval loss:\n",
      "sst: 0.91\n",
      "eval score is improved from 0.96610 to 0.91398, saving model\n",
      "\n",
      "epoch: 7\n",
      "0 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998530000000012\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8196630477905273\n",
      "batch training loss: 0.81966, ssr: 0.99985, lr: 0.00002\n",
      "1 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998500000000012\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8153132796287537\n",
      "2 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998470000000013\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.8089905381202698\n",
      "3 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998440000000013\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.803546667098999\n",
      "4 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998410000000013\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7977699041366577\n",
      "5 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998380000000013\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7917031645774841\n",
      "batch training loss: 0.79170, ssr: 0.99984, lr: 0.00003\n",
      "6 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998350000000014\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7861971855163574\n",
      "7 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998320000000014\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7808041572570801\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "epoch eval loss:\n",
      "sst: 0.84\n",
      "eval score is improved from 0.91398 to 0.84090, saving model\n",
      "\n",
      "epoch: 8\n",
      "0 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998290000000014\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7746161222457886\n",
      "batch training loss: 0.77462, ssr: 0.99983, lr: 0.00003\n",
      "1 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998260000000014\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7683789134025574\n",
      "2 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998230000000015\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7628741264343262\n",
      "3 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998200000000015\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7569462060928345\n",
      "4 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998170000000015\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7513338327407837\n",
      "5 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998140000000015\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7455716729164124\n",
      "batch training loss: 0.74557, ssr: 0.99981, lr: 0.00003\n",
      "6 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998110000000016\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7400493621826172\n",
      "7 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998080000000016\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7344921827316284\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "epoch eval loss:\n",
      "sst: 0.76\n",
      "eval score is improved from 0.84090 to 0.76245, saving model\n",
      "\n",
      "epoch: 9\n",
      "0 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998050000000016\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7287895679473877\n",
      "batch training loss: 0.72879, ssr: 0.99981, lr: 0.00003\n",
      "1 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9998020000000016\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7228900194168091\n",
      "2 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997990000000017\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7169389128684998\n",
      "3 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997960000000017\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7108956575393677\n",
      "4 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997930000000017\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.7053451538085938\n",
      "5 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997900000000017\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.6993904709815979\n",
      "batch training loss: 0.69939, ssr: 0.99979, lr: 0.00003\n",
      "6 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997870000000018\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.6932202577590942\n",
      "7 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997840000000018\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.6879136562347412\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "epoch eval loss:\n",
      "sst: 0.69\n",
      "eval score is improved from 0.76245 to 0.69385, saving model\n",
      "\n",
      "epoch: 10\n",
      "0 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997810000000018\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.6818134784698486\n",
      "batch training loss: 0.68181, ssr: 0.99978, lr: 0.00003\n",
      "1 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997780000000018\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.6752921342849731\n",
      "2 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997750000000019\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.6700648069381714\n",
      "3 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997720000000019\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.6640403270721436\n",
      "4 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997690000000019\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.6582509875297546\n",
      "5 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.9997660000000019\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.6522186994552612\n",
      "batch training loss: 0.65222, ssr: 0.99977, lr: 0.00004\n",
      "6 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.999763000000002\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.6466991901397705\n",
      "7 torch.Size([2, 3, 5, 28, 52]) torch.Size([2, 1, 28, 52])\n",
      "ssr_ratio 0.999760000000002\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([2, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([2, 250, 3, 25])\n",
      "1111111111111 0.6408398747444153\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual1 + self.encode1(x): torch.Size([4, 250, 3, 25])\n",
      "Shape after resdual2 + self.encode2(x): torch.Size([4, 250, 3, 25])\n",
      "epoch eval loss:\n",
      "sst: 0.63\n",
      "eval score is improved from 0.69385 to 0.63419, saving model\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:20:07.167609Z",
     "start_time": "2024-08-13T14:20:07.154608Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "3c5f920d25cdcb0",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T14:20:07.189612Z",
     "start_time": "2024-08-13T14:20:07.171609Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "18d87c1ec9f278e7",
   "outputs": [],
   "execution_count": 134
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

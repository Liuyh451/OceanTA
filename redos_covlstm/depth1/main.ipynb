{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T08:30:33.890031Z",
     "start_time": "2024-08-11T08:30:28.522513Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T08:30:33.916030Z",
     "start_time": "2024-08-11T08:30:33.894029Z"
    }
   },
   "source": [
    "def create_dataset(data, time_step):\n",
    "    dataX = []\n",
    "    for i in range(data.shape[0] - time_step + 1):\n",
    "        dataX.append(data[i:i + time_step])\n",
    "    return np.array(dataX)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T08:30:37.214200Z",
     "start_time": "2024-08-11T08:30:37.194201Z"
    }
   },
   "source": [
    "def read_raw_data(vtype, depth, time_step):\n",
    "    train_argo = np.load('/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/'+vtype+'_0_ano.npy')#读取数据，指定维度标签\n",
    "    label_argo = np.load('/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/'+vtype+'_'+str(depth)+'_ano.npy')#206,60,80\n",
    "    width = train_argo.shape[2] #对应经度\n",
    "    lenth = train_argo.shape[1] #对应纬度\n",
    "    X = create_dataset(train_argo, time_step)\n",
    "    X = X.reshape(X.shape[0],time_step,lenth,width,1)\n",
    "    Y = label_argo[time_step-1 : label_argo.shape[0]] \n",
    "    Y =Y.reshape(Y.shape[0],lenth,width,1)\n",
    "    #X 转置维度，变为 (样本数, 时间步长, 通道数, 纬度, 经度)。\n",
    "    #Y 转置维度，变为 (样本数, 时间步长， 经度, 纬度)。\n",
    "    X = X.transpose(0,1,4,2,3)\n",
    "    Y = Y.transpose(0,3,1,2)\n",
    "    return X, Y"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T08:30:39.597495Z",
     "start_time": "2024-08-11T08:30:39.585495Z"
    }
   },
   "source": [
    "def scaler(data):\n",
    "    #normalise [0,1]\n",
    "    data_max = np.nanmax(data)\n",
    "    data_min = np.nanmin(data)\n",
    "    data_scale = data_max - data_min\n",
    "    data_std = (data - data_min) / data_scale\n",
    "    # data_std = data_std * (2)  -1\n",
    "    data_std [np.isnan(data_std)] = 0\n",
    "    return data_std,data_min,data_scale\n",
    "\n",
    "#反归一化\n",
    "def unscaler(data, data_min, data_scale):\n",
    "    data_inv = (data * data_scale) + data_min\n",
    "    return data_inv"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T08:30:45.148724Z",
     "start_time": "2024-08-11T08:30:43.702950Z"
    }
   },
   "source": [
    "#读取盐度数据\n",
    "train_sssa, _ = read_raw_data('salt', 0, 3)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/salt_0_ano.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_sssa, _ \u001B[38;5;241m=\u001B[39m \u001B[43mread_raw_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msalt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m, in \u001B[0;36mread_raw_data\u001B[1;34m(vtype, depth, time_step)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_raw_data\u001B[39m(vtype, depth, time_step):\n\u001B[1;32m----> 2\u001B[0m     train_argo \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43mvtype\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_0_ano.npy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;66;03m#读取数据，指定维度标签\u001B[39;00m\n\u001B[0;32m      3\u001B[0m     label_argo \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39mvtype\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(depth)\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_ano.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;66;03m#206,60,80\u001B[39;00m\n\u001B[0;32m      4\u001B[0m     width \u001B[38;5;241m=\u001B[39m train_argo\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;66;03m#对应经度\u001B[39;00m\n",
      "File \u001B[1;32mD:\\CondaEnvs\\pycharm\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[0;32m    403\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 405\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos_fspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    406\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/salt_0_ano.npy'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这几个数据格式一样，但是内容不一样，读的分别是不同的列\n",
    "train_ssha,_ = read_raw_data('sla',0,3) #海面高度异常（Sea Level Anomaly）\n",
    "train_sswu,_ = read_raw_data('uwnd',0,3)#U 分量的风速（即沿经度方向的风速）\n",
    "train_sswv,_ = read_raw_data('vwnd',0,3)#V 分量的风速（即沿纬度方向的风速）\n",
    "train_argo, label_argo = read_raw_data('temp', 1, 3)#temp 代表温度数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 1, 60, 80)\n"
     ]
    }
   ],
   "source": [
    "print(label_argo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对数据进行归一化\n",
    "sta_train,_,_ = scaler(train_argo[:-12,:])\n",
    "ssa_train,_,_  = scaler(train_sssa[:-12,:])\n",
    "ssha_train,_,_ = scaler(train_ssha[:-12,:])\n",
    "sswu_train,_,_ = scaler(train_sswu[:-12,:])\n",
    "sswv_train,_,_ = scaler(train_sswv[:-12,:])\n",
    "true_train,_,_ = scaler(label_argo[:-12,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 3, 1, 60, 80)\n",
      "(166, 3, 1, 60, 80)\n",
      "(166, 3, 1, 60, 80)\n",
      "(166, 3, 1, 60, 80)\n"
     ]
    }
   ],
   "source": [
    "print(sta_train.shape)\n",
    "print(ssa_train.shape)\n",
    "print(ssha_train.shape)\n",
    "print(sswu_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用倒数12个数据作为验证集\n",
    "sta_test,_,_ = scaler(train_argo[-12:])\n",
    "ssa_test,_,_  = scaler(train_sssa[-12:])\n",
    "ssha_test,_,_ = scaler(train_ssha[-12:])\n",
    "sswu_test,_,_ = scaler(train_sswu[-12:])\n",
    "sswv_test,_,_ = scaler(train_sswv[-12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将多个不同类型的训练数据和测试数据沿着指定轴进行拼接，axis=2即增加特征的数量（即通道或变量的数量）\n",
    "sta_train = np.concatenate((sta_train,ssa_train,ssha_train,sswu_train,sswv_train),axis = 2 )\n",
    "sta_test = np.concatenate((sta_test,ssa_test,ssha_test,sswu_test,sswv_test),axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test,test_min,test_scale = scaler(label_argo[-12:])\n",
    "#true_test是归一化后的 label_argo 数据，对应于最后 12 个时间步的标签数据\n",
    "#test_min是 label_argo[-12:] 数据中的最小值，在归一化过程中用作偏移量。\n",
    "#test_scale是 label_argo[-12:] 数据的范围，即最大值与最小值的差值。在归一化过程中用于缩放数据"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T03:05:09.878443Z",
     "start_time": "2024-08-14T03:05:09.784436Z"
    }
   },
   "source": [
    "#将拼接后的数据作为训练集\n",
    "X_train = sta_train\n",
    "#训练集的标签\n",
    "true_train = true_train\n",
    "\n",
    "#训练集上用于评估\n",
    "X_eval = sta_test\n",
    "#\n",
    "true_eval = true_test\n",
    "X_test = sta_test\n",
    "true_test = true_test\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sta_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#将拼接后的数据作为训练集\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m X_train \u001B[38;5;241m=\u001B[39m \u001B[43msta_train\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#训练集的标签\u001B[39;00m\n\u001B[0;32m      4\u001B[0m true_train \u001B[38;5;241m=\u001B[39m true_train\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sta_train' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "configs = Configs()\n",
    "\n",
    "# trainer related\n",
    "configs.vtype = 'temp'\n",
    "# configs.depth = 11\n",
    "# configs.time_step = 1\n",
    "configs.n_cpu = 0\n",
    "configs.device = torch.device('cuda:0')\n",
    "configs.batch_size_test = 100\n",
    "configs.batch_size = 24\n",
    "#configs.lr = 0.001\n",
    "configs.weight_decay = 0\n",
    "configs.display_interval = 200\n",
    "configs.num_epochs = 900\n",
    "#这是早停的耐心参数。即使模型在900个epoch内没有改善性能，训练仍会继续。如果在900个epoch内性能没有改善，训练将停止\n",
    "configs.early_stopping = True\n",
    "configs.patience = 900\n",
    "#禁用梯度裁剪（Gradient Clipping）。梯度裁剪用于防止梯度爆炸问题，但在这里未启用\n",
    "configs.gradient_clipping = False\n",
    "#设置梯度裁剪的阈值为1。如果梯度裁剪启用，梯度的最大值将被限制为1。不过在这种配置下，由于梯度裁剪被禁用，这个参数实际上不会生效\n",
    "configs.clipping_threshold = 1.\n",
    "\n",
    "# lr warmup\n",
    "#这是学习率预热的步数设置。在训练的前3000步内，学习率将逐渐从一个较小的值线性增加到预设的学习率。这种技术通常用于训练的初始阶段，以帮助模型更稳定地开始训练，减少初期的震荡。\n",
    "configs.warmup = 3000\n",
    "\n",
    "# data related\n",
    "#这是输入数据的维度设置。这通常取决于你使用的数据的特征数或通道数\n",
    "configs.input_dim = 1 # 4 #这里应该是5吧 但是写的1我总感觉是5\n",
    "'''\n",
    "人家这个1是对的这个模型就是要保证输入通道和输出通道得一样\n",
    "默认为1\n",
    "'''\n",
    "configs.output_dim = 1\n",
    "#表示模型的输入序列长度为5，即模型在预测时会使用前5个时间步的数据作为输入\n",
    "configs.input_length = 5\n",
    "#表示模型的输出长度为1，即模型预测一个时间步的值。通常用于单步预测\n",
    "configs.output_length = 1\n",
    "#表示输入序列中的数据点之间的时间间隔为1。即数据是逐步连续的，没有跳跃\n",
    "configs.input_gap = 1\n",
    "#表示预测的时间偏移量为24。这可能意味着模型的目标是预测未来24个时间步后的数据点\n",
    "configs.pred_shift = 24\n",
    "#这个列表包含了一系列的深度值，这可能与模型的层次结构或者不同深度的输入特征相关联\n",
    "configs.depth = [5,6,11,16,20,25,30,34,36,38,40,42,44,46,48,50,51,52,53,54,55,57]\n",
    "#这个列表可能对应于不同深度的索引或层次级别。每个索引可能用于定位或选择特定深度的特征或数据\n",
    "configs.depthindex = [30,50, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900]\n",
    "\n",
    "# model\n",
    "#表示模型的维度即每个输入数据在模型中的表示为256维\n",
    "configs.d_model = 256\n",
    "#表示模型处理数据时的patch（小块）的大小为5×5。这通常用于图像或序列数据的分块处理\n",
    "configs.patch_size = (5,5)\n",
    "#表示嵌入的空间尺寸。这里12*16可能是表示最终嵌入的特征图的尺寸（例如视觉模型中的特征图大小）\n",
    "configs.emb_spatial_size = 12*16\n",
    "#表示多头注意力机制中的头数为4。多头注意力允许模型从不同的角度“看”数据，从而捕捉不同的关系\n",
    "configs.nheads = 4\n",
    "#表示前馈神经网络的维度用于增加模型的表达能力\n",
    "configs.dim_feedforward = 512\n",
    "#表示在模型中使用的dropout率为0.3。Dropout是一种正则化技术，用于减少过拟合。\n",
    "configs.dropout = 0.3\n",
    "#表示编码器的层数为4。这意味着模型有4个堆叠的编码器层\n",
    "configs.num_encoder_layers = 4\n",
    "configs.num_decoder_layers = 4\n",
    "#这可能是学习率的衰减率（scheduler decay rate），用来控制模型训练过程中学习率的递减速度，以便在训练的后期进行更细致的优化\n",
    "configs.ssr_decay_rate = 3.e-6\n",
    "\n",
    "\n",
    "# plot 表示绘图的分辨率为600 DPI\n",
    "configs.plot_dpi = 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class covlstmformer(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super().__init__()\n",
    "        self.configs = configs\n",
    "        self.d_model = 25\n",
    "        self.device = configs.device\n",
    "        #输入通道数（或输入特征图的通道数）。通常对应于输入数据的深度。\n",
    "        #8：输出通道数（或输出特征图的通道数）。表示卷积操作后输出的特征图的深度。\n",
    "        #3：卷积核的大小，通常表示一个 3x3 的卷积核（filter）。\n",
    "        #5：步幅（stride）\n",
    "        self.cov1 = Cov(5, 8,3, 5)\n",
    "        self.cov2 = Cov(5, 8,3, 5)\n",
    "        #两个编码器\n",
    "        self.encode1 = EncoderLayer(self.d_model, 1, configs.dim_feedforward, configs.dropout)\n",
    "        self.encode2 = EncoderLayer(self.d_model, 1, configs.dim_feedforward, configs.dropout)\n",
    "        self.cov_last  = Cov_last(5, 8,3, 1)\n",
    "    def forward(self,x):\n",
    "        resdual1 = self.cov1(x)\n",
    "        #将特征图按 (5, 5) 大小的块展开或重新排列\n",
    "        resdual1 = unfold_StackOverChannel(resdual1, (5, 5))\n",
    "        x = resdual1\n",
    "        #跳跃连接操作\n",
    "        x = resdual1 + self.encode1(x)\n",
    "        #函数将特征图 x 折叠成 (60, 80) 尺寸，可能对应于输入尺寸的恢复\n",
    "        x = fold_tensor(x, (60, 80), (5, 5))\n",
    "        \n",
    "        resdual2 = x + self.cov2(x) # xiu gai 的地方在这\n",
    "        resdual2 = unfold_StackOverChannel(resdual2, (5, 5))\n",
    "        x = resdual2\n",
    "        x = resdual2 + self.encode2(x)\n",
    "        x = fold_tensor(x, (60, 80), (5, 5))\n",
    "        x = self.cov_last(x)\n",
    "        return x\n",
    "#编码器层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nheads, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        #实现了时间维度上的多头注意力机制\n",
    "        self.time_attn = MultiHeadedAttention(d_model, nheads, TimeAttention, dropout)\n",
    "        #实现了空间维度上的多头注意力机制\n",
    "        self.space_attn = MultiHeadedAttention(d_model, nheads, SpaceAttention, dropout)\n",
    "        '''\n",
    "        #一个更复杂的全连接网络（被注释掉），可以用于替代 feed_forward 部分\n",
    "        self.net = nn.Sequential(\n",
    "                  nn.Linear(256, 25),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(25, 256),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(256, 512),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(512,256)\n",
    "                  )\n",
    "        '''\n",
    "        #前馈神经网络，用于进一步处理通过注意力机制后的输出。该网络包括两个线性层和ReLU激活函数，用于非线性映射和特征提取\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "            )\n",
    "    '''\n",
    "    #一个分离空间和时间注意力的实现（被注释掉），可能用于更精细地控制注意力的应用顺序\n",
    "    def divided_space_time_attn(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        Apply space and time attention sequentially\n",
    "        Args:\n",
    "            query (N, S, T, D)\n",
    "            key (N, S, T, D)\n",
    "            value (N, S, T, D)\n",
    "        Returns:\n",
    "            (N, S, T, D)\n",
    "        \"\"\"\n",
    "        m = self.time_attn(query, key, value, mask)\n",
    "        return self.space_attn(m, m, m, mask)\n",
    "    '''\n",
    "    def forward(self, x, mask=None):\n",
    "        # x = self.sublayer[0](x, lambda x: self.divided_space_time_attn(x, x, x, mask))\n",
    "        # x = x + self.net(x)\n",
    "        # return self.sublayer[1](x, self.feed_forward)\n",
    "        #融合时间和空间注意力机制\n",
    "        x = x + self.time_attn(x, x, x, mask)\n",
    "        x = x+ self.space_attn(x, x, x,mask)\n",
    "        #应用前馈神经网络处理，并将结果与经过空间注意力后的输出相加，生成最终的编码器输出。\n",
    "        x = x + self.feed_forward(x)\n",
    "        return x\n",
    "#卷积长短时记忆网络（ConvLSTM）的单元 ConvLSTMCell\n",
    "class ConvLSTMCell(nn.Module):\n",
    " \n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    " \n",
    "        super(ConvLSTMCell, self).__init__()\n",
    " \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.kernel_size = kernel_size\n",
    "        #根据卷积核的大小，自动计算填充大小，以确保输入和输出张量的空间尺寸（高度和宽度）一致。\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2 # 保证在传递过程中 （h,w）不变\n",
    "        #是否在卷积操作中添加偏置项\n",
    "        self.bias = bias\n",
    " #定义了一个二维卷积层，该卷积层接收输入张量和当前隐藏状态张量的拼接，并输出 4 倍的隐藏状态张量大小，用于计算 LSTM 的四个门（输入门 i、遗忘门 f、输出门 o 和候选状态 g）。\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim, # i门，f门，o门，g门放在一起计算，然后在split开\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    " \n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state # 每个timestamp包含两个状态张量：h和c\n",
    " \n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis # 把输入张量与h状态张量沿通道维度串联\n",
    " \n",
    "        combined_conv = self.conv(combined) # i门，f门，o门，g门放在一起计算，然后在split开\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    " \n",
    "        c_next = f * c_cur + i * g  # c状态张量更新\n",
    "        h_next = o * torch.tanh(c_next) # h状态张量更新\n",
    " \n",
    "        return h_next, c_next # 输出当前timestamp的两个状态张量\n",
    " \n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        \"\"\"\n",
    "        初始状态张量初始化.第一个timestamp的状态张量0初始化\n",
    "        :param batch_size:\n",
    "        :param image_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        height, width = image_size\n",
    "        init_h = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "        init_c = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "        return (init_h,init_c)\n",
    " \n",
    " \n",
    "class ConvLSTM(nn.Module):\n",
    " \n",
    "    \"\"\"\n",
    "    Parameters:参数介绍\n",
    "        input_dim: Number of channels in input# 输入张量的通道数\n",
    "        hidden_dim: Number of hidden channels # h,c两个状态张量的通道数，可以是一个列表\n",
    "        kernel_size: Size of kernel in convolutions # 卷积核的尺寸，默认所有层的卷积核尺寸都是一样的,也可以设定不通lstm层的卷积核尺寸不同\n",
    "        num_layers: Number of LSTM layers stacked on each other # 卷积层的层数，需要与len(hidden_dim)相等\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers # 是否返回所有lstm层的h状态\n",
    "        Note: Will do same padding. # 相同的卷积核尺寸，相同的padding尺寸\n",
    "    Input:输入介绍\n",
    "        A tensor of size [B, T, C, H, W] or [T, B, C, H, W]# 需要是5维的\n",
    "    Output:输出介绍\n",
    "        返回的是两个列表：layer_output_list，last_state_list\n",
    "        列表0：layer_output_list--单层列表，每个元素表示一层LSTM层的输出h状态,每个元素的size=[B,T,hidden_dim,H,W]\n",
    "        列表1：last_state_list--双层列表，每个元素是一个二元列表[h,c],表示每一层的最后一个timestamp的输出状态[h,c],h.size=c.size = [B,hidden_dim,H,W]\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:使用示例\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    " \n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    " \n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers) # 转为列表\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers) # 转为列表\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers: # 判断一致性\n",
    "            raise ValueError('Inconsistent list length.')\n",
    " \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    " \n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers): # 多层LSTM设置\n",
    "            # 当前LSTM层的输入维度\n",
    "            # if i==0:\n",
    "            #     cur_input_dim = self.input_dim\n",
    "            # else:\n",
    "            #     cur_input_dim = self.hidden_dim[i - 1]\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1] # 与上等价\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    " \n",
    "        self.cell_list = nn.ModuleList(cell_list) # 把定义的多个LSTM层串联成网络模型\n",
    " \n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: 5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    " \n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            b, _, _, h, w = input_tensor.size()  # 自动获取 b,h,w信息\n",
    "            hidden_state = self._init_hidden(batch_size=b,image_size=(h, w))\n",
    " \n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    " \n",
    "        seq_len = input_tensor.size(1) # 根据输入张量获取lstm的长度\n",
    "        cur_layer_input = input_tensor\n",
    " \n",
    "        for layer_idx in range(self.num_layers): # 逐层计算\n",
    " \n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len): # 逐个stamp计算\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],cur_state=[h, c])\n",
    "                output_inner.append(h) # 第 layer_idx 层的第t个stamp的输出状态\n",
    " \n",
    "            layer_output = torch.stack(output_inner, dim=1) # 第 layer_idx 层的第所有stamp的输出状态串联\n",
    "            cur_layer_input = layer_output # 准备第layer_idx+1层的输入张量\n",
    " \n",
    "            layer_output_list.append(layer_output) # 当前层的所有timestamp的h状态的串联\n",
    "            last_state_list.append([h, c]) # 当前层的最后一个stamp的输出状态的[h,c]\n",
    " \n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    " \n",
    "        return layer_output_list, last_state_list\n",
    " \n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        \"\"\"\n",
    "        所有lstm层的第一个timestamp的输入状态0初始化\n",
    "        :param batch_size:\n",
    "        :param image_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    " \n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        \"\"\"\n",
    "        检测输入的kernel_size是否符合要求，要求kernel_size的格式是list或tuple\n",
    "        :param kernel_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    " \n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        \"\"\"\n",
    "        扩展到多层lstm情况\n",
    "        :param param:\n",
    "        :param num_layers:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n",
    "\n",
    "class Cov(nn.Module):\n",
    "     def __init__(self, intput_dim, hidden_dim, bn_dim, output_dim):# bn_dim是时间步\n",
    "          super().__init__()\n",
    "          self.cov1 = ConvLSTM(input_dim = intput_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn1 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov2 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn2 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov3 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = output_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "     def forward(self, x):\n",
    "          x,_ = self.cov1(x) # 因为上面的Covlstm返回两个值所以先用_接住第二个用不到的值\n",
    "          x = self.bn1(x[0])\n",
    "          x,_ = self.cov2(x)\n",
    "          x = self.bn2(x[0])\n",
    "          x,_ = self.cov3(x)\n",
    "          x = x[0]\n",
    "          return x\n",
    "     \n",
    "class Cov_last(nn.Module):\n",
    "     def __init__(self, intput_dim, hidden_dim, bn_dim, output_dim):# bn_dim是时间步\n",
    "          super().__init__()\n",
    "          self.cov1 = ConvLSTM(input_dim = intput_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn1 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov2 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn2 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov3 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = output_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "     def forward(self, x):\n",
    "          x,_ = self.cov1(x) # 因为上面的Covlstm返回两个值所以先用_接住第二个用不到的值\n",
    "          x = self.bn1(x[0])\n",
    "          x,_ = self.cov2(x)\n",
    "          x = self.bn2(x[0])\n",
    "          x,_ = self.cov3(x)\n",
    "          x = x[0]\n",
    "          return x[:, -1] \n",
    "     \n",
    "def unfold_StackOverChannel(img, kernel_size):\n",
    "    \"\"\"\n",
    "    divide the original image to patches, then stack the grids in each patch along the channels\n",
    "    Args:\n",
    "        img (N, *, C, H, W): the last two dimensions must be the spatial dimension\n",
    "        kernel_size: tuple of length 2\n",
    "    Returns:\n",
    "        output (N, *, C*H_k*N_k, H_output, W_output)\n",
    "    \"\"\"\n",
    "    T = img.size(1)\n",
    "    n_dim = len(img.size())\n",
    "    assert n_dim == 4 or n_dim == 5\n",
    "\n",
    "    pt = img.unfold(-2, size=kernel_size[0], step=kernel_size[0])\n",
    "    pt = pt.unfold(-2, size=kernel_size[1], step=kernel_size[1]).flatten(-2)  # (N, *, C, n0, n1, k0*k1)\n",
    "    if n_dim == 4:  # (N, C, H, W)\n",
    "        pt = pt.permute(0, 1, 4, 2, 3).flatten(1, 2)\n",
    "    elif n_dim == 5:  # (N, T, C, H, W)\n",
    "        pt = pt.permute(0, 1, 2, 5, 3, 4).flatten(2, 3)\n",
    "    assert pt.size(-3) == img.size(-3) * kernel_size[0] * kernel_size[1]\n",
    "    pt = pt.reshape(pt.size(0), T, 25, -1).permute(0, 3, 1, 2)\n",
    "    return pt     \n",
    "def fold_tensor(tensor, output_size, kernel_size):\n",
    "    \"\"\"\n",
    "    reconstruct the image from its non-overlapping patches\n",
    "    Args:\n",
    "        input tensor of size (N, *, C*k_h*k_w, n_h, n_w)\n",
    "        output_size of size(H, W), the size of the original image to be reconstructed\n",
    "        kernel_size: (k_h, k_w)\n",
    "        stride is usually equal to kernel_size for non-overlapping sliding window\n",
    "    Returns:\n",
    "        (N, *, C, H=n_h*k_h, W=n_w*k_w)\n",
    "    \"\"\"\n",
    "    tensor = tensor.reshape(-1,192,3,25)\n",
    "    T = tensor.size(2)\n",
    "    tensor = tensor.permute(0, 2, 3, 1)  # (N, T, C_, S)\n",
    "    tensor = tensor.reshape(tensor.size(0), T, 25,\n",
    "                                12, 16)\n",
    "    tensor = tensor.float()\n",
    "    n_dim = len(tensor.size())\n",
    "    assert n_dim == 4 or n_dim == 5\n",
    "    f = tensor.flatten(0, 1) if n_dim == 5 else tensor\n",
    "    folded = F.fold(f.flatten(-2), output_size=output_size, kernel_size=kernel_size, stride=kernel_size)\n",
    "    if n_dim == 5:\n",
    "        folded = folded.reshape(tensor.size(0), tensor.size(1), *folded.size()[1:])\n",
    "    return folded.reshape(-1,T,5,28,52)\n",
    "\n",
    "\n",
    "def TimeAttention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    attention over the time axis\n",
    "    Args:\n",
    "        query, key, value: linearly-transformed query, key, value (N, h, S, T, D)\n",
    "        mask: of size (T (query), T (key)) specifying locations (which key) the query can and cannot attend to\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)  # (N, h, S, T, T)\n",
    "    if mask is not None:\n",
    "        assert mask.dtype == torch.bool\n",
    "        assert len(mask.size()) == 2\n",
    "        scores = scores.masked_fill(mask[None, None, None], float(\"-inf\"))\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value)  # (N, h, S, T, D)\n",
    "\n",
    "\n",
    "def SpaceAttention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    attention over the two space axes\n",
    "    Args:\n",
    "        query, key, value: linearly-transformed query, key, value (N, h, S, T, D)\n",
    "        mask: None (space attention does not need mask), this argument is intentionally set for consistency\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    query = query.transpose(2, 3)  # (N, h, T, S, D)\n",
    "    key = key.transpose(2, 3)  # (N, h, T, S, D)\n",
    "    value = value.transpose(2, 3)  # (N, h, T, S, D)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)  # (N, h, T, S, S)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value).transpose(2, 3)  # (N, h, S, T_q, D)\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, nheads, attn, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model % nheads == 0\n",
    "        self.d_k = d_model // nheads\n",
    "        self.nheads = nheads\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.attn = attn\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Transform the query, key, value into different heads, then apply the attention in parallel\n",
    "        Args:\n",
    "            query, key, value: size (N, S, T, D)\n",
    "        Returns:\n",
    "            (N, S, T, D)\n",
    "        \"\"\"\n",
    "        nbatches = query.size(0)\n",
    "        nspace = query.size(1)\n",
    "        ntime = query.size(2)\n",
    "        # (N, h, S, T, d_k)\n",
    "        query, key, value = \\\n",
    "            [l(x).view(x.size(0), x.size(1), x.size(2), self.nheads, self.d_k).permute(0, 3, 1, 2, 4)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # (N, h, S, T, d_k)\n",
    "        x = self.attn(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # (N, S, T, D)\n",
    "        x = x.permute(0, 2, 3, 1, 4).contiguous() \\\n",
    "             .view(nbatches, nspace, ntime, self.nheads * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dropout,attn):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(192, 192) for _ in range(3)])\n",
    "        self.attn = attn\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query, key, value = \\\n",
    "        [l(x)\n",
    "        for l, x in zip(self.linears, (query, key, value))]\n",
    "        x = self.attn(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"\"\"\n",
    "    learning rate warmup and decay\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "               (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, configs):\n",
    "        self.configs = configs\n",
    "        self.device = configs.device\n",
    "        torch.manual_seed(5)\n",
    "        self.network  = covlstmformer(configs).to(configs.device)\n",
    "        adam = torch.optim.Adam(self.network.parameters(), lr=0, weight_decay=configs.weight_decay)\n",
    "        factor = math.sqrt(configs.d_model * configs.warmup) * 0.0014\n",
    "        self.opt = NoamOpt(configs.d_model, factor, warmup=configs.warmup, optimizer=adam)\n",
    "\n",
    "\n",
    "    def loss_sst(self, y_pred, y_true):\n",
    "        # y_pred/y_true (N, 26, 24, 48)\n",
    "        rmse = torch.mean((y_pred - y_true) ** 2, dim=[2, 3])\n",
    "        rmse = torch.sum(rmse.sqrt().mean(dim=0))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "\n",
    "    def train_once(self, input_sst, sst_true, ssr_ratio):\n",
    "        sst_pred = self.network(input_sst.float().to(self.device))\n",
    "        self.opt.optimizer.zero_grad()\n",
    "        loss_sst = self.loss_sst(sst_pred, sst_true.float().to(self.device))\n",
    "        # loss_nino = self.loss_nino(nino_pred, nino_true.float().to(self.device))\n",
    "        loss_sst.backward()\n",
    "        if configs.gradient_clipping:\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), configs.clipping_threshold)\n",
    "        self.opt.step()\n",
    "        return loss_sst.item()\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        # nino_pred = []\n",
    "        sst_pred = []\n",
    "        with torch.no_grad():\n",
    "            for input_sst, sst_true, in dataloader_test:\n",
    "                sst = self.network(input_sst.float().to(self.device))\n",
    "                # nino_pred.append(nino)\n",
    "                sst_pred.append(sst)\n",
    "\n",
    "        return torch.cat(sst_pred, dim=0)\n",
    "\n",
    "    def infer(self, dataset, dataloader):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            sst_pred = self.test(dataloader)\n",
    "            # nino_true = torch.from_numpy(dataset.target_nino).float().to(self.device)\n",
    "            sst_true = torch.from_numpy(dataset.target_sst).float().to(self.device)\n",
    "            # sc = self.score(nino_pred, nino_true)\n",
    "#             print(sst_pred.shape)\n",
    "#             print(sst_true.shape)\n",
    "            loss_sst = self.loss_sst(sst_pred, sst_true).item()\n",
    "            # loss_nino = self.loss_nino(nino_pred, nino_true).item()\n",
    "        return loss_sst\n",
    "\n",
    "    def infer_test(self, dataset, dataloader):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            sst_pred = self.test(dataloader)\n",
    "            # nino_true = torch.from_numpy(dataset.target_nino).float().to(self.device)\n",
    "            sst_true = torch.from_numpy(dataset.target_sst).float().to(self.device)\n",
    "            # sc = self.score(nino_pred, nino_true)\n",
    "            loss_sst = self.loss_sst(sst_pred, sst_true).item()\n",
    "            # loss_nino = self.loss_nino(nino_pred, nino_true).item()\n",
    "        return loss_sst, sst_pred, sst_true\n",
    "\n",
    "    def train(self, dataset_train, dataset_eval, chk_path):\n",
    "        torch.manual_seed(0)\n",
    "        print('loading train dataloader')\n",
    "        dataloader_train = DataLoader(dataset_train, batch_size=self.configs.batch_size, shuffle=True)\n",
    "        print('loading eval dataloader')\n",
    "        dataloader_eval = DataLoader(dataset_eval, batch_size=self.configs.batch_size_test, shuffle=False)\n",
    "\n",
    "        count = 0\n",
    "        best = math.inf\n",
    "        ssr_ratio = 1\n",
    "        for i in range(self.configs.num_epochs):\n",
    "            print('\\nepoch: {0}'.format(i + 1))\n",
    "            # train\n",
    "            self.network.train()\n",
    "            for j, (input_sst, sst_true) in enumerate(dataloader_train):\n",
    "                if ssr_ratio > 0:\n",
    "                    ssr_ratio = max(ssr_ratio - self.configs.ssr_decay_rate, 0)\n",
    "                loss_sst = self.train_once(input_sst, sst_true, ssr_ratio)  # y_pred for one batch\n",
    "\n",
    "                if j % self.configs.display_interval == 0:\n",
    "\n",
    "                    print('batch training loss: {:.5f}, ssr: {:.5f}, lr: {:.5f}'.format(loss_sst, ssr_ratio, self.opt.rate()))\n",
    "\n",
    "                # increase the number of evaluations in order not to miss the optimal point\n",
    "                # which is feasible because of the less training time of timesformer\n",
    "                if (i + 1 >= 9) and (j + 1) % 300 == 0:\n",
    "                    loss_sst_eval = self.infer(dataset=dataset_eval, dataloader=dataloader_eval)\n",
    "                    print('epoch eval loss: sc: {:.4f}'.format(loss_sst_eval))\n",
    "                    if loss_sst_eval < best:\n",
    "                        self.save_model(chk_path)\n",
    "                        best = loss_sst_eval\n",
    "                        count = 0\n",
    "\n",
    "            # evaluation\n",
    "            loss_sst_eval = self.infer(dataset=dataset_eval, dataloader=dataloader_eval)\n",
    "            print('epoch eval loss:\\nsst: {:.2f}'.format(loss_sst_eval))\n",
    "            if loss_sst_eval >= best:\n",
    "                count += 1\n",
    "                print('eval score is not improved for {} epoch'.format(count))\n",
    "            else:\n",
    "                count = 0\n",
    "                print('eval score is improved from {:.5f} to {:.5f}, saving model'.format(best, loss_sst_eval))\n",
    "                self.save_model(chk_path)\n",
    "                best = loss_sst_eval\n",
    "\n",
    "            if count == self.configs.patience:\n",
    "                print('early stopping reached, best score is {:5f}'.format(best))\n",
    "                break\n",
    "\n",
    "    def save_configs(self, config_path):\n",
    "        with open(config_path, 'wb') as path:\n",
    "            pickle.dump(self.configs, path)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save({'net': self.network.state_dict(),\n",
    "                    'optimizer': self.opt.optimizer.state_dict()}, path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cmip_dataset(Dataset):\n",
    "    def __init__(self, datax,datay):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_sst = datax\n",
    "        self.target_sst = datay\n",
    "\n",
    "\n",
    "    def GetDataShape(self):\n",
    "        return {'sst input': self.input_sst.shape,\n",
    "                'sst target': self.target_sst.shape}\n",
    "\n",
    "    def __len__(self,):\n",
    "        return self.input_sst.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_sst[idx], self.target_sst[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst input': (166, 3, 5, 60, 80), 'sst target': (166, 1, 60, 80)}\n"
     ]
    }
   ],
   "source": [
    "dataset_train = cmip_dataset(X_train,true_train)\n",
    "print(dataset_train.GetDataShape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst input': (12, 3, 5, 60, 80), 'sst target': (12, 1, 60, 80)}\n"
     ]
    }
   ],
   "source": [
    "dataset_eval = cmip_dataset(X_eval,true_eval)\n",
    "print(dataset_eval.GetDataShape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(configs)\n",
    "trainer.save_configs('config_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train dataloader\n",
      "loading eval dataloader\n",
      "\n",
      "epoch: 1\n",
      "batch training loss: 0.49610, ssr: 1.00000, lr: 0.00000\n",
      "epoch eval loss:\n",
      "sst: 0.52\n",
      "eval score is improved from inf to 0.51574, saving model\n",
      "\n",
      "epoch: 2\n",
      "batch training loss: 0.50008, ssr: 0.99998, lr: 0.00000\n",
      "epoch eval loss:\n",
      "sst: 0.51\n",
      "eval score is improved from 0.51574 to 0.51485, saving model\n",
      "\n",
      "epoch: 3\n",
      "batch training loss: 0.48016, ssr: 0.99996, lr: 0.00001\n",
      "epoch eval loss:\n",
      "sst: 0.51\n",
      "eval score is improved from 0.51485 to 0.51098, saving model\n",
      "\n",
      "epoch: 4\n",
      "batch training loss: 0.46640, ssr: 0.99993, lr: 0.00001\n",
      "epoch eval loss:\n",
      "sst: 0.50\n",
      "eval score is improved from 0.51098 to 0.50114, saving model\n",
      "\n",
      "epoch: 5\n",
      "batch training loss: 0.44917, ssr: 0.99991, lr: 0.00001\n",
      "epoch eval loss:\n",
      "sst: 0.48\n",
      "eval score is improved from 0.50114 to 0.48161, saving model\n",
      "\n",
      "epoch: 6\n",
      "batch training loss: 0.42575, ssr: 0.99989, lr: 0.00002\n",
      "epoch eval loss:\n",
      "sst: 0.45\n",
      "eval score is improved from 0.48161 to 0.44947, saving model\n",
      "\n",
      "epoch: 7\n",
      "batch training loss: 0.38920, ssr: 0.99987, lr: 0.00002\n",
      "epoch eval loss:\n",
      "sst: 0.40\n",
      "eval score is improved from 0.44947 to 0.40161, saving model\n",
      "\n",
      "epoch: 8\n",
      "batch training loss: 0.35851, ssr: 0.99985, lr: 0.00002\n",
      "epoch eval loss:\n",
      "sst: 0.34\n",
      "eval score is improved from 0.40161 to 0.33709, saving model\n",
      "\n",
      "epoch: 9\n",
      "batch training loss: 0.31416, ssr: 0.99983, lr: 0.00003\n",
      "epoch eval loss:\n",
      "sst: 0.27\n",
      "eval score is improved from 0.33709 to 0.26993, saving model\n",
      "\n",
      "epoch: 10\n",
      "batch training loss: 0.26183, ssr: 0.99981, lr: 0.00003\n",
      "epoch eval loss:\n",
      "sst: 0.22\n",
      "eval score is improved from 0.26993 to 0.21571, saving model\n",
      "\n",
      "epoch: 11\n",
      "batch training loss: 0.22550, ssr: 0.99979, lr: 0.00003\n",
      "epoch eval loss:\n",
      "sst: 0.18\n",
      "eval score is improved from 0.21571 to 0.17531, saving model\n",
      "\n",
      "epoch: 12\n",
      "batch training loss: 0.18691, ssr: 0.99977, lr: 0.00004\n",
      "epoch eval loss:\n",
      "sst: 0.15\n",
      "eval score is improved from 0.17531 to 0.14916, saving model\n",
      "\n",
      "epoch: 13\n",
      "batch training loss: 0.15483, ssr: 0.99975, lr: 0.00004\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.14916 to 0.14087, saving model\n",
      "\n",
      "epoch: 14\n",
      "batch training loss: 0.14132, ssr: 0.99972, lr: 0.00004\n",
      "epoch eval loss:\n",
      "sst: 0.15\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 15\n",
      "batch training loss: 0.12350, ssr: 0.99970, lr: 0.00005\n",
      "epoch eval loss:\n",
      "sst: 0.15\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 16\n",
      "batch training loss: 0.12448, ssr: 0.99968, lr: 0.00005\n",
      "epoch eval loss:\n",
      "sst: 0.15\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 17\n",
      "batch training loss: 0.12930, ssr: 0.99966, lr: 0.00005\n",
      "epoch eval loss:\n",
      "sst: 0.15\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 18\n",
      "batch training loss: 0.12099, ssr: 0.99964, lr: 0.00006\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 19\n",
      "batch training loss: 0.11860, ssr: 0.99962, lr: 0.00006\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.14087 to 0.14072, saving model\n",
      "\n",
      "epoch: 20\n",
      "batch training loss: 0.11950, ssr: 0.99960, lr: 0.00006\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.14072 to 0.13843, saving model\n",
      "\n",
      "epoch: 21\n",
      "batch training loss: 0.12043, ssr: 0.99958, lr: 0.00007\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.13843 to 0.13789, saving model\n",
      "\n",
      "epoch: 22\n",
      "batch training loss: 0.11470, ssr: 0.99956, lr: 0.00007\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 23\n",
      "batch training loss: 0.10935, ssr: 0.99954, lr: 0.00007\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.13789 to 0.13772, saving model\n",
      "\n",
      "epoch: 24\n",
      "batch training loss: 0.11562, ssr: 0.99951, lr: 0.00008\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.13772 to 0.13698, saving model\n",
      "\n",
      "epoch: 25\n",
      "batch training loss: 0.11437, ssr: 0.99949, lr: 0.00008\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.13698 to 0.13578, saving model\n",
      "\n",
      "epoch: 26\n",
      "batch training loss: 0.11878, ssr: 0.99947, lr: 0.00008\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.13578 to 0.13458, saving model\n",
      "\n",
      "epoch: 27\n",
      "batch training loss: 0.10617, ssr: 0.99945, lr: 0.00009\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.13458 to 0.13337, saving model\n",
      "\n",
      "epoch: 28\n",
      "batch training loss: 0.11003, ssr: 0.99943, lr: 0.00009\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.13337 to 0.13258, saving model\n",
      "\n",
      "epoch: 29\n",
      "batch training loss: 0.11459, ssr: 0.99941, lr: 0.00009\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.13258 to 0.13119, saving model\n",
      "\n",
      "epoch: 30\n",
      "batch training loss: 0.11144, ssr: 0.99939, lr: 0.00010\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.13119 to 0.12808, saving model\n",
      "\n",
      "epoch: 31\n",
      "batch training loss: 0.11270, ssr: 0.99937, lr: 0.00010\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.12808 to 0.12556, saving model\n",
      "\n",
      "epoch: 32\n",
      "batch training loss: 0.10440, ssr: 0.99935, lr: 0.00010\n",
      "epoch eval loss:\n",
      "sst: 0.12\n",
      "eval score is improved from 0.12556 to 0.12247, saving model\n",
      "\n",
      "epoch: 33\n",
      "batch training loss: 0.09624, ssr: 0.99933, lr: 0.00011\n",
      "epoch eval loss:\n",
      "sst: 0.12\n",
      "eval score is improved from 0.12247 to 0.11817, saving model\n",
      "\n",
      "epoch: 34\n",
      "batch training loss: 0.10837, ssr: 0.99930, lr: 0.00011\n",
      "epoch eval loss:\n",
      "sst: 0.11\n",
      "eval score is improved from 0.11817 to 0.11448, saving model\n",
      "\n",
      "epoch: 35\n",
      "batch training loss: 0.09191, ssr: 0.99928, lr: 0.00011\n",
      "epoch eval loss:\n",
      "sst: 0.11\n",
      "eval score is improved from 0.11448 to 0.11138, saving model\n",
      "\n",
      "epoch: 36\n",
      "batch training loss: 0.10031, ssr: 0.99926, lr: 0.00011\n",
      "epoch eval loss:\n",
      "sst: 0.11\n",
      "eval score is improved from 0.11138 to 0.10930, saving model\n",
      "\n",
      "epoch: 37\n",
      "batch training loss: 0.10068, ssr: 0.99924, lr: 0.00012\n",
      "epoch eval loss:\n",
      "sst: 0.11\n",
      "eval score is improved from 0.10930 to 0.10652, saving model\n",
      "\n",
      "epoch: 38\n",
      "batch training loss: 0.09060, ssr: 0.99922, lr: 0.00012\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is improved from 0.10652 to 0.10431, saving model\n",
      "\n",
      "epoch: 39\n",
      "batch training loss: 0.09197, ssr: 0.99920, lr: 0.00012\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is improved from 0.10431 to 0.10262, saving model\n",
      "\n",
      "epoch: 40\n",
      "batch training loss: 0.09679, ssr: 0.99918, lr: 0.00013\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is improved from 0.10262 to 0.10070, saving model\n",
      "\n",
      "epoch: 41\n",
      "batch training loss: 0.08979, ssr: 0.99916, lr: 0.00013\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is improved from 0.10070 to 0.09866, saving model\n",
      "\n",
      "epoch: 42\n",
      "batch training loss: 0.08892, ssr: 0.99914, lr: 0.00013\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is improved from 0.09866 to 0.09638, saving model\n",
      "\n",
      "epoch: 43\n",
      "batch training loss: 0.08517, ssr: 0.99912, lr: 0.00014\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is improved from 0.09638 to 0.09354, saving model\n",
      "\n",
      "epoch: 44\n",
      "batch training loss: 0.08135, ssr: 0.99909, lr: 0.00014\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is improved from 0.09354 to 0.09020, saving model\n",
      "\n",
      "epoch: 45\n",
      "batch training loss: 0.08188, ssr: 0.99907, lr: 0.00014\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is improved from 0.09020 to 0.08640, saving model\n",
      "\n",
      "epoch: 46\n",
      "batch training loss: 0.07013, ssr: 0.99905, lr: 0.00015\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is improved from 0.08640 to 0.08006, saving model\n",
      "\n",
      "epoch: 47\n",
      "batch training loss: 0.07164, ssr: 0.99903, lr: 0.00015\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is improved from 0.08006 to 0.07549, saving model\n",
      "\n",
      "epoch: 48\n",
      "batch training loss: 0.06622, ssr: 0.99901, lr: 0.00015\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is improved from 0.07549 to 0.07518, saving model\n",
      "\n",
      "epoch: 49\n",
      "batch training loss: 0.06186, ssr: 0.99899, lr: 0.00016\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 50\n",
      "batch training loss: 0.06043, ssr: 0.99897, lr: 0.00016\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 51\n",
      "batch training loss: 0.05711, ssr: 0.99895, lr: 0.00016\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is improved from 0.07518 to 0.07202, saving model\n",
      "\n",
      "epoch: 52\n",
      "batch training loss: 0.05791, ssr: 0.99893, lr: 0.00017\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is improved from 0.07202 to 0.07161, saving model\n",
      "\n",
      "epoch: 53\n",
      "batch training loss: 0.05472, ssr: 0.99891, lr: 0.00017\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is improved from 0.07161 to 0.06071, saving model\n",
      "\n",
      "epoch: 54\n",
      "batch training loss: 0.05506, ssr: 0.99888, lr: 0.00017\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 55\n",
      "batch training loss: 0.05284, ssr: 0.99886, lr: 0.00018\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 56\n",
      "batch training loss: 0.05208, ssr: 0.99884, lr: 0.00018\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is improved from 0.06071 to 0.05471, saving model\n",
      "\n",
      "epoch: 57\n",
      "batch training loss: 0.05288, ssr: 0.99882, lr: 0.00018\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 58\n",
      "batch training loss: 0.05013, ssr: 0.99880, lr: 0.00019\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 59\n",
      "batch training loss: 0.05038, ssr: 0.99878, lr: 0.00019\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 60\n",
      "batch training loss: 0.05039, ssr: 0.99876, lr: 0.00019\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 61\n",
      "batch training loss: 0.05060, ssr: 0.99874, lr: 0.00020\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 62\n",
      "batch training loss: 0.04835, ssr: 0.99872, lr: 0.00020\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 63\n",
      "batch training loss: 0.04781, ssr: 0.99870, lr: 0.00020\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 64\n",
      "batch training loss: 0.04897, ssr: 0.99867, lr: 0.00021\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 65\n",
      "batch training loss: 0.04825, ssr: 0.99865, lr: 0.00021\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is improved from 0.05471 to 0.05280, saving model\n",
      "\n",
      "epoch: 66\n",
      "batch training loss: 0.04701, ssr: 0.99863, lr: 0.00021\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 67\n",
      "batch training loss: 0.04742, ssr: 0.99861, lr: 0.00022\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 68\n",
      "batch training loss: 0.04596, ssr: 0.99859, lr: 0.00022\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 69\n",
      "batch training loss: 0.04547, ssr: 0.99857, lr: 0.00022\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 70\n",
      "batch training loss: 0.04667, ssr: 0.99855, lr: 0.00023\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is improved from 0.05280 to 0.05039, saving model\n",
      "\n",
      "epoch: 71\n",
      "batch training loss: 0.04653, ssr: 0.99853, lr: 0.00023\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is improved from 0.05039 to 0.04805, saving model\n",
      "\n",
      "epoch: 72\n",
      "batch training loss: 0.04446, ssr: 0.99851, lr: 0.00023\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is improved from 0.04805 to 0.04735, saving model\n",
      "\n",
      "epoch: 73\n",
      "batch training loss: 0.04470, ssr: 0.99849, lr: 0.00024\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 74\n",
      "batch training loss: 0.04547, ssr: 0.99846, lr: 0.00024\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 75\n",
      "batch training loss: 0.04426, ssr: 0.99844, lr: 0.00024\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 76\n",
      "batch training loss: 0.04329, ssr: 0.99842, lr: 0.00025\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 77\n",
      "batch training loss: 0.04477, ssr: 0.99840, lr: 0.00025\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 78\n",
      "batch training loss: 0.04536, ssr: 0.99838, lr: 0.00025\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 79\n",
      "batch training loss: 0.04448, ssr: 0.99836, lr: 0.00026\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 80\n",
      "batch training loss: 0.04814, ssr: 0.99834, lr: 0.00026\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 81\n",
      "batch training loss: 0.04486, ssr: 0.99832, lr: 0.00026\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 82\n",
      "batch training loss: 0.04376, ssr: 0.99830, lr: 0.00027\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 83\n",
      "batch training loss: 0.04415, ssr: 0.99828, lr: 0.00027\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 84\n",
      "batch training loss: 0.04523, ssr: 0.99825, lr: 0.00027\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 85\n",
      "batch training loss: 0.04390, ssr: 0.99823, lr: 0.00027\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 86\n",
      "batch training loss: 0.04549, ssr: 0.99821, lr: 0.00028\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 87\n",
      "batch training loss: 0.04463, ssr: 0.99819, lr: 0.00028\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 88\n",
      "batch training loss: 0.04545, ssr: 0.99817, lr: 0.00028\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 89\n",
      "batch training loss: 0.04387, ssr: 0.99815, lr: 0.00029\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 90\n",
      "batch training loss: 0.04386, ssr: 0.99813, lr: 0.00029\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 91\n",
      "batch training loss: 0.04217, ssr: 0.99811, lr: 0.00029\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 92\n",
      "batch training loss: 0.04448, ssr: 0.99809, lr: 0.00030\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 93\n",
      "batch training loss: 0.04262, ssr: 0.99807, lr: 0.00030\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04735 to 0.04317, saving model\n",
      "\n",
      "epoch: 94\n",
      "batch training loss: 0.04281, ssr: 0.99804, lr: 0.00030\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 95\n",
      "batch training loss: 0.04186, ssr: 0.99802, lr: 0.00031\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 96\n",
      "batch training loss: 0.04399, ssr: 0.99800, lr: 0.00031\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 97\n",
      "batch training loss: 0.04295, ssr: 0.99798, lr: 0.00031\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 98\n",
      "batch training loss: 0.04342, ssr: 0.99796, lr: 0.00032\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 99\n",
      "batch training loss: 0.04355, ssr: 0.99794, lr: 0.00032\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 100\n",
      "batch training loss: 0.04711, ssr: 0.99792, lr: 0.00032\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 101\n",
      "batch training loss: 0.04214, ssr: 0.99790, lr: 0.00033\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 102\n",
      "batch training loss: 0.04163, ssr: 0.99788, lr: 0.00033\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 103\n",
      "batch training loss: 0.04278, ssr: 0.99786, lr: 0.00033\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 104\n",
      "batch training loss: 0.04393, ssr: 0.99783, lr: 0.00034\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 105\n",
      "batch training loss: 0.04201, ssr: 0.99781, lr: 0.00034\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 106\n",
      "batch training loss: 0.03977, ssr: 0.99779, lr: 0.00034\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 107\n",
      "batch training loss: 0.04211, ssr: 0.99777, lr: 0.00035\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 108\n",
      "batch training loss: 0.04134, ssr: 0.99775, lr: 0.00035\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 109\n",
      "batch training loss: 0.04117, ssr: 0.99773, lr: 0.00035\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 110\n",
      "batch training loss: 0.04142, ssr: 0.99771, lr: 0.00036\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 111\n",
      "batch training loss: 0.04035, ssr: 0.99769, lr: 0.00036\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 112\n",
      "batch training loss: 0.04219, ssr: 0.99767, lr: 0.00036\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 113\n",
      "batch training loss: 0.04654, ssr: 0.99765, lr: 0.00037\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 114\n",
      "batch training loss: 0.03998, ssr: 0.99762, lr: 0.00037\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 115\n",
      "batch training loss: 0.04070, ssr: 0.99760, lr: 0.00037\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04317 to 0.04189, saving model\n",
      "\n",
      "epoch: 116\n",
      "batch training loss: 0.04152, ssr: 0.99758, lr: 0.00038\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 117\n",
      "batch training loss: 0.04275, ssr: 0.99756, lr: 0.00038\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 118\n",
      "batch training loss: 0.04113, ssr: 0.99754, lr: 0.00038\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 119\n",
      "batch training loss: 0.04382, ssr: 0.99752, lr: 0.00039\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 120\n",
      "batch training loss: 0.04324, ssr: 0.99750, lr: 0.00039\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 121\n",
      "batch training loss: 0.04130, ssr: 0.99748, lr: 0.00039\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 122\n",
      "batch training loss: 0.03890, ssr: 0.99746, lr: 0.00040\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04189 to 0.04134, saving model\n",
      "\n",
      "epoch: 123\n",
      "batch training loss: 0.04071, ssr: 0.99744, lr: 0.00040\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 124\n",
      "batch training loss: 0.04191, ssr: 0.99741, lr: 0.00040\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 125\n",
      "batch training loss: 0.04058, ssr: 0.99739, lr: 0.00041\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 126\n",
      "batch training loss: 0.04001, ssr: 0.99737, lr: 0.00041\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 127\n",
      "batch training loss: 0.03978, ssr: 0.99735, lr: 0.00041\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 128\n",
      "batch training loss: 0.03852, ssr: 0.99733, lr: 0.00042\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04134 to 0.03980, saving model\n",
      "\n",
      "epoch: 129\n",
      "batch training loss: 0.04005, ssr: 0.99731, lr: 0.00042\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.03980 to 0.03972, saving model\n",
      "\n",
      "epoch: 130\n",
      "batch training loss: 0.03960, ssr: 0.99729, lr: 0.00042\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 131\n",
      "batch training loss: 0.04094, ssr: 0.99727, lr: 0.00043\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 132\n",
      "batch training loss: 0.04035, ssr: 0.99725, lr: 0.00043\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 133\n",
      "batch training loss: 0.04001, ssr: 0.99723, lr: 0.00043\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.03972 to 0.03894, saving model\n",
      "\n",
      "epoch: 134\n",
      "batch training loss: 0.04067, ssr: 0.99720, lr: 0.00043\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 135\n",
      "batch training loss: 0.03790, ssr: 0.99718, lr: 0.00044\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 136\n",
      "batch training loss: 0.03816, ssr: 0.99716, lr: 0.00044\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 137\n",
      "batch training loss: 0.03918, ssr: 0.99714, lr: 0.00044\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 138\n",
      "batch training loss: 0.04281, ssr: 0.99712, lr: 0.00045\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 139\n",
      "batch training loss: 0.03799, ssr: 0.99710, lr: 0.00045\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 140\n",
      "batch training loss: 0.04004, ssr: 0.99708, lr: 0.00045\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 141\n",
      "batch training loss: 0.03707, ssr: 0.99706, lr: 0.00046\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 142\n",
      "batch training loss: 0.03711, ssr: 0.99704, lr: 0.00046\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 143\n",
      "batch training loss: 0.04011, ssr: 0.99702, lr: 0.00046\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 144\n",
      "batch training loss: 0.03564, ssr: 0.99699, lr: 0.00047\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 145\n",
      "batch training loss: 0.03718, ssr: 0.99697, lr: 0.00047\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.03894 to 0.03837, saving model\n",
      "\n",
      "epoch: 146\n",
      "batch training loss: 0.03655, ssr: 0.99695, lr: 0.00047\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.03837 to 0.03772, saving model\n",
      "\n",
      "epoch: 147\n",
      "batch training loss: 0.03738, ssr: 0.99693, lr: 0.00048\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 148\n",
      "batch training loss: 0.03658, ssr: 0.99691, lr: 0.00048\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 149\n",
      "batch training loss: 0.03676, ssr: 0.99689, lr: 0.00048\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 150\n",
      "batch training loss: 0.03665, ssr: 0.99687, lr: 0.00049\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 151\n",
      "batch training loss: 0.03721, ssr: 0.99685, lr: 0.00049\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 152\n",
      "batch training loss: 0.03649, ssr: 0.99683, lr: 0.00049\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.03772 to 0.03661, saving model\n",
      "\n",
      "epoch: 153\n",
      "batch training loss: 0.03699, ssr: 0.99681, lr: 0.00050\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 154\n",
      "batch training loss: 0.03434, ssr: 0.99678, lr: 0.00050\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 155\n",
      "batch training loss: 0.03479, ssr: 0.99676, lr: 0.00050\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 156\n",
      "batch training loss: 0.03597, ssr: 0.99674, lr: 0.00051\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 157\n",
      "batch training loss: 0.03507, ssr: 0.99672, lr: 0.00051\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 158\n",
      "batch training loss: 0.03374, ssr: 0.99670, lr: 0.00051\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 159\n",
      "batch training loss: 0.03480, ssr: 0.99668, lr: 0.00052\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.03661 to 0.03629, saving model\n",
      "\n",
      "epoch: 160\n",
      "batch training loss: 0.03451, ssr: 0.99666, lr: 0.00052\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.03629 to 0.03452, saving model\n",
      "\n",
      "epoch: 161\n",
      "batch training loss: 0.03502, ssr: 0.99664, lr: 0.00052\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 162\n",
      "batch training loss: 0.03380, ssr: 0.99662, lr: 0.00053\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 163\n",
      "batch training loss: 0.03218, ssr: 0.99660, lr: 0.00053\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 164\n",
      "batch training loss: 0.03303, ssr: 0.99657, lr: 0.00053\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 165\n",
      "batch training loss: 0.03188, ssr: 0.99655, lr: 0.00054\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.03452 to 0.03345, saving model\n",
      "\n",
      "epoch: 166\n",
      "batch training loss: 0.03397, ssr: 0.99653, lr: 0.00054\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 167\n",
      "batch training loss: 0.03208, ssr: 0.99651, lr: 0.00054\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 168\n",
      "batch training loss: 0.03168, ssr: 0.99649, lr: 0.00055\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 169\n",
      "batch training loss: 0.03241, ssr: 0.99647, lr: 0.00055\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 170\n",
      "batch training loss: 0.03350, ssr: 0.99645, lr: 0.00055\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 171\n",
      "batch training loss: 0.03193, ssr: 0.99643, lr: 0.00056\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 172\n",
      "batch training loss: 0.03278, ssr: 0.99641, lr: 0.00056\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 173\n",
      "batch training loss: 0.03123, ssr: 0.99639, lr: 0.00056\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 174\n",
      "batch training loss: 0.02977, ssr: 0.99636, lr: 0.00057\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 175\n",
      "batch training loss: 0.03499, ssr: 0.99634, lr: 0.00057\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 176\n",
      "batch training loss: 0.03170, ssr: 0.99632, lr: 0.00057\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.03345 to 0.03206, saving model\n",
      "\n",
      "epoch: 177\n",
      "batch training loss: 0.03167, ssr: 0.99630, lr: 0.00058\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 178\n",
      "batch training loss: 0.03082, ssr: 0.99628, lr: 0.00058\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 179\n",
      "batch training loss: 0.03263, ssr: 0.99626, lr: 0.00058\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 180\n",
      "batch training loss: 0.03255, ssr: 0.99624, lr: 0.00059\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 181\n",
      "batch training loss: 0.03121, ssr: 0.99622, lr: 0.00059\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 182\n",
      "batch training loss: 0.02968, ssr: 0.99620, lr: 0.00059\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 183\n",
      "batch training loss: 0.03157, ssr: 0.99618, lr: 0.00060\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 184\n",
      "batch training loss: 0.02991, ssr: 0.99615, lr: 0.00060\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 185\n",
      "batch training loss: 0.02927, ssr: 0.99613, lr: 0.00060\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 186\n",
      "batch training loss: 0.03043, ssr: 0.99611, lr: 0.00060\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 187\n",
      "batch training loss: 0.02990, ssr: 0.99609, lr: 0.00061\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 188\n",
      "batch training loss: 0.03179, ssr: 0.99607, lr: 0.00061\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 189\n",
      "batch training loss: 0.03098, ssr: 0.99605, lr: 0.00061\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 190\n",
      "batch training loss: 0.03189, ssr: 0.99603, lr: 0.00062\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 191\n",
      "batch training loss: 0.03392, ssr: 0.99601, lr: 0.00062\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 192\n",
      "batch training loss: 0.03464, ssr: 0.99599, lr: 0.00062\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 193\n",
      "batch training loss: 0.03081, ssr: 0.99597, lr: 0.00063\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 194\n",
      "batch training loss: 0.02885, ssr: 0.99594, lr: 0.00063\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 195\n",
      "batch training loss: 0.02868, ssr: 0.99592, lr: 0.00063\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 196\n",
      "batch training loss: 0.02988, ssr: 0.99590, lr: 0.00064\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 197\n",
      "batch training loss: 0.02777, ssr: 0.99588, lr: 0.00064\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 198\n",
      "batch training loss: 0.03280, ssr: 0.99586, lr: 0.00064\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 199\n",
      "batch training loss: 0.02742, ssr: 0.99584, lr: 0.00065\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.03206 to 0.03186, saving model\n",
      "\n",
      "epoch: 200\n",
      "batch training loss: 0.03035, ssr: 0.99582, lr: 0.00065\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 201\n",
      "batch training loss: 0.03383, ssr: 0.99580, lr: 0.00065\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 202\n",
      "batch training loss: 0.02960, ssr: 0.99578, lr: 0.00066\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 203\n",
      "batch training loss: 0.03009, ssr: 0.99576, lr: 0.00066\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 204\n",
      "batch training loss: 0.02882, ssr: 0.99573, lr: 0.00066\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 205\n",
      "batch training loss: 0.02720, ssr: 0.99571, lr: 0.00067\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 206\n",
      "batch training loss: 0.02824, ssr: 0.99569, lr: 0.00067\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.03186 to 0.03131, saving model\n",
      "\n",
      "epoch: 207\n",
      "batch training loss: 0.02706, ssr: 0.99567, lr: 0.00067\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 208\n",
      "batch training loss: 0.02734, ssr: 0.99565, lr: 0.00068\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 209\n",
      "batch training loss: 0.03025, ssr: 0.99563, lr: 0.00068\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 210\n",
      "batch training loss: 0.02925, ssr: 0.99561, lr: 0.00068\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 211\n",
      "batch training loss: 0.02865, ssr: 0.99559, lr: 0.00069\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 212\n",
      "batch training loss: 0.03041, ssr: 0.99557, lr: 0.00069\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 213\n",
      "batch training loss: 0.02778, ssr: 0.99555, lr: 0.00069\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 214\n",
      "batch training loss: 0.02714, ssr: 0.99552, lr: 0.00070\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 215\n",
      "batch training loss: 0.02830, ssr: 0.99550, lr: 0.00070\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 216\n",
      "batch training loss: 0.02987, ssr: 0.99548, lr: 0.00070\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 217\n",
      "batch training loss: 0.02879, ssr: 0.99546, lr: 0.00071\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.03131 to 0.03064, saving model\n",
      "\n",
      "epoch: 218\n",
      "batch training loss: 0.02690, ssr: 0.99544, lr: 0.00071\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.03064 to 0.02968, saving model\n",
      "\n",
      "epoch: 219\n",
      "batch training loss: 0.02739, ssr: 0.99542, lr: 0.00071\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 220\n",
      "batch training loss: 0.02804, ssr: 0.99540, lr: 0.00072\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 221\n",
      "batch training loss: 0.02650, ssr: 0.99538, lr: 0.00072\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 222\n",
      "batch training loss: 0.02826, ssr: 0.99536, lr: 0.00072\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02968 to 0.02920, saving model\n",
      "\n",
      "epoch: 223\n",
      "batch training loss: 0.02911, ssr: 0.99534, lr: 0.00073\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 224\n",
      "batch training loss: 0.03219, ssr: 0.99531, lr: 0.00073\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 225\n",
      "batch training loss: 0.02970, ssr: 0.99529, lr: 0.00073\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 226\n",
      "batch training loss: 0.03085, ssr: 0.99527, lr: 0.00074\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 227\n",
      "batch training loss: 0.02839, ssr: 0.99525, lr: 0.00074\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02920 to 0.02882, saving model\n",
      "\n",
      "epoch: 228\n",
      "batch training loss: 0.02985, ssr: 0.99523, lr: 0.00074\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 229\n",
      "batch training loss: 0.02691, ssr: 0.99521, lr: 0.00075\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 230\n",
      "batch training loss: 0.02866, ssr: 0.99519, lr: 0.00075\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 231\n",
      "batch training loss: 0.02761, ssr: 0.99517, lr: 0.00075\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 232\n",
      "batch training loss: 0.02657, ssr: 0.99515, lr: 0.00076\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 233\n",
      "batch training loss: 0.02799, ssr: 0.99513, lr: 0.00076\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 234\n",
      "batch training loss: 0.03072, ssr: 0.99510, lr: 0.00076\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 235\n",
      "batch training loss: 0.02670, ssr: 0.99508, lr: 0.00076\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 236\n",
      "batch training loss: 0.02855, ssr: 0.99506, lr: 0.00077\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 237\n",
      "batch training loss: 0.02898, ssr: 0.99504, lr: 0.00077\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 238\n",
      "batch training loss: 0.02805, ssr: 0.99502, lr: 0.00077\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 239\n",
      "batch training loss: 0.02638, ssr: 0.99500, lr: 0.00078\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 240\n",
      "batch training loss: 0.02721, ssr: 0.99498, lr: 0.00078\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 241\n",
      "batch training loss: 0.02701, ssr: 0.99496, lr: 0.00078\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 242\n",
      "batch training loss: 0.02631, ssr: 0.99494, lr: 0.00079\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 243\n",
      "batch training loss: 0.02636, ssr: 0.99492, lr: 0.00079\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 244\n",
      "batch training loss: 0.02670, ssr: 0.99489, lr: 0.00079\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 245\n",
      "batch training loss: 0.02748, ssr: 0.99487, lr: 0.00080\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 246\n",
      "batch training loss: 0.02611, ssr: 0.99485, lr: 0.00080\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 247\n",
      "batch training loss: 0.02605, ssr: 0.99483, lr: 0.00080\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 248\n",
      "batch training loss: 0.02682, ssr: 0.99481, lr: 0.00081\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 249\n",
      "batch training loss: 0.02776, ssr: 0.99479, lr: 0.00081\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 250\n",
      "batch training loss: 0.02775, ssr: 0.99477, lr: 0.00081\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 251\n",
      "batch training loss: 0.02638, ssr: 0.99475, lr: 0.00082\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 252\n",
      "batch training loss: 0.02802, ssr: 0.99473, lr: 0.00082\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 253\n",
      "batch training loss: 0.02798, ssr: 0.99471, lr: 0.00082\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 254\n",
      "batch training loss: 0.02687, ssr: 0.99468, lr: 0.00083\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 255\n",
      "batch training loss: 0.02827, ssr: 0.99466, lr: 0.00083\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 256\n",
      "batch training loss: 0.02809, ssr: 0.99464, lr: 0.00083\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 257\n",
      "batch training loss: 0.02779, ssr: 0.99462, lr: 0.00084\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 30 epoch\n",
      "\n",
      "epoch: 258\n",
      "batch training loss: 0.02676, ssr: 0.99460, lr: 0.00084\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 31 epoch\n",
      "\n",
      "epoch: 259\n",
      "batch training loss: 0.02695, ssr: 0.99458, lr: 0.00084\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 32 epoch\n",
      "\n",
      "epoch: 260\n",
      "batch training loss: 0.02674, ssr: 0.99456, lr: 0.00085\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 33 epoch\n",
      "\n",
      "epoch: 261\n",
      "batch training loss: 0.02719, ssr: 0.99454, lr: 0.00085\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 34 epoch\n",
      "\n",
      "epoch: 262\n",
      "batch training loss: 0.02808, ssr: 0.99452, lr: 0.00085\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02882 to 0.02743, saving model\n",
      "\n",
      "epoch: 263\n",
      "batch training loss: 0.02643, ssr: 0.99450, lr: 0.00086\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 264\n",
      "batch training loss: 0.02576, ssr: 0.99447, lr: 0.00086\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 265\n",
      "batch training loss: 0.02655, ssr: 0.99445, lr: 0.00086\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 266\n",
      "batch training loss: 0.02672, ssr: 0.99443, lr: 0.00087\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 267\n",
      "batch training loss: 0.02878, ssr: 0.99441, lr: 0.00087\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 268\n",
      "batch training loss: 0.02796, ssr: 0.99439, lr: 0.00087\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 269\n",
      "batch training loss: 0.02802, ssr: 0.99437, lr: 0.00088\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 270\n",
      "batch training loss: 0.03067, ssr: 0.99435, lr: 0.00088\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 271\n",
      "batch training loss: 0.02905, ssr: 0.99433, lr: 0.00088\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 272\n",
      "batch training loss: 0.02864, ssr: 0.99431, lr: 0.00089\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 273\n",
      "batch training loss: 0.02648, ssr: 0.99429, lr: 0.00089\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 274\n",
      "batch training loss: 0.02706, ssr: 0.99426, lr: 0.00089\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 275\n",
      "batch training loss: 0.02554, ssr: 0.99424, lr: 0.00090\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02743 to 0.02695, saving model\n",
      "\n",
      "epoch: 276\n",
      "batch training loss: 0.02739, ssr: 0.99422, lr: 0.00090\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 277\n",
      "batch training loss: 0.02557, ssr: 0.99420, lr: 0.00090\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 278\n",
      "batch training loss: 0.02915, ssr: 0.99418, lr: 0.00091\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 279\n",
      "batch training loss: 0.02687, ssr: 0.99416, lr: 0.00091\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 280\n",
      "batch training loss: 0.02619, ssr: 0.99414, lr: 0.00091\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 281\n",
      "batch training loss: 0.02749, ssr: 0.99412, lr: 0.00092\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 282\n",
      "batch training loss: 0.03064, ssr: 0.99410, lr: 0.00092\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 283\n",
      "batch training loss: 0.02558, ssr: 0.99408, lr: 0.00092\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 284\n",
      "batch training loss: 0.02742, ssr: 0.99405, lr: 0.00092\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 285\n",
      "batch training loss: 0.02640, ssr: 0.99403, lr: 0.00093\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 286\n",
      "batch training loss: 0.03200, ssr: 0.99401, lr: 0.00093\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 287\n",
      "batch training loss: 0.02634, ssr: 0.99399, lr: 0.00093\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 288\n",
      "batch training loss: 0.02818, ssr: 0.99397, lr: 0.00094\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02695 to 0.02649, saving model\n",
      "\n",
      "epoch: 289\n",
      "batch training loss: 0.02698, ssr: 0.99395, lr: 0.00094\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 290\n",
      "batch training loss: 0.02650, ssr: 0.99393, lr: 0.00094\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 291\n",
      "batch training loss: 0.02653, ssr: 0.99391, lr: 0.00095\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 292\n",
      "batch training loss: 0.02717, ssr: 0.99389, lr: 0.00095\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 293\n",
      "batch training loss: 0.02534, ssr: 0.99387, lr: 0.00095\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 294\n",
      "batch training loss: 0.02625, ssr: 0.99384, lr: 0.00096\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 295\n",
      "batch training loss: 0.02632, ssr: 0.99382, lr: 0.00096\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 296\n",
      "batch training loss: 0.02519, ssr: 0.99380, lr: 0.00096\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 297\n",
      "batch training loss: 0.02848, ssr: 0.99378, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 298\n",
      "batch training loss: 0.02483, ssr: 0.99376, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 299\n",
      "batch training loss: 0.02463, ssr: 0.99374, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 300\n",
      "batch training loss: 0.02584, ssr: 0.99372, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 301\n",
      "batch training loss: 0.02922, ssr: 0.99370, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 302\n",
      "batch training loss: 0.02601, ssr: 0.99368, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 303\n",
      "batch training loss: 0.02759, ssr: 0.99366, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 304\n",
      "batch training loss: 0.02654, ssr: 0.99363, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02649 to 0.02636, saving model\n",
      "\n",
      "epoch: 305\n",
      "batch training loss: 0.02711, ssr: 0.99361, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 306\n",
      "batch training loss: 0.02434, ssr: 0.99359, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 307\n",
      "batch training loss: 0.02857, ssr: 0.99357, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 308\n",
      "batch training loss: 0.02552, ssr: 0.99355, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 309\n",
      "batch training loss: 0.02747, ssr: 0.99353, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 310\n",
      "batch training loss: 0.02793, ssr: 0.99351, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 311\n",
      "batch training loss: 0.02614, ssr: 0.99349, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 312\n",
      "batch training loss: 0.02616, ssr: 0.99347, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 313\n",
      "batch training loss: 0.02448, ssr: 0.99345, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 314\n",
      "batch training loss: 0.02693, ssr: 0.99342, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 315\n",
      "batch training loss: 0.02638, ssr: 0.99340, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 316\n",
      "batch training loss: 0.02927, ssr: 0.99338, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 317\n",
      "batch training loss: 0.02598, ssr: 0.99336, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 318\n",
      "batch training loss: 0.02584, ssr: 0.99334, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 319\n",
      "batch training loss: 0.02599, ssr: 0.99332, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 320\n",
      "batch training loss: 0.02644, ssr: 0.99330, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 321\n",
      "batch training loss: 0.02718, ssr: 0.99328, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 322\n",
      "batch training loss: 0.02691, ssr: 0.99326, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 323\n",
      "batch training loss: 0.02571, ssr: 0.99324, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 324\n",
      "batch training loss: 0.02550, ssr: 0.99321, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 325\n",
      "batch training loss: 0.02613, ssr: 0.99319, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 326\n",
      "batch training loss: 0.02528, ssr: 0.99317, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 327\n",
      "batch training loss: 0.02472, ssr: 0.99315, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 328\n",
      "batch training loss: 0.02681, ssr: 0.99313, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 329\n",
      "batch training loss: 0.02565, ssr: 0.99311, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 330\n",
      "batch training loss: 0.02484, ssr: 0.99309, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 331\n",
      "batch training loss: 0.02915, ssr: 0.99307, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 332\n",
      "batch training loss: 0.02644, ssr: 0.99305, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 333\n",
      "batch training loss: 0.02657, ssr: 0.99303, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 334\n",
      "batch training loss: 0.02679, ssr: 0.99300, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 30 epoch\n",
      "\n",
      "epoch: 335\n",
      "batch training loss: 0.02877, ssr: 0.99298, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 31 epoch\n",
      "\n",
      "epoch: 336\n",
      "batch training loss: 0.02675, ssr: 0.99296, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 32 epoch\n",
      "\n",
      "epoch: 337\n",
      "batch training loss: 0.02592, ssr: 0.99294, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 33 epoch\n",
      "\n",
      "epoch: 338\n",
      "batch training loss: 0.02821, ssr: 0.99292, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 34 epoch\n",
      "\n",
      "epoch: 339\n",
      "batch training loss: 0.02437, ssr: 0.99290, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 35 epoch\n",
      "\n",
      "epoch: 340\n",
      "batch training loss: 0.02574, ssr: 0.99288, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 36 epoch\n",
      "\n",
      "epoch: 341\n",
      "batch training loss: 0.02504, ssr: 0.99286, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 37 epoch\n",
      "\n",
      "epoch: 342\n",
      "batch training loss: 0.02507, ssr: 0.99284, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 38 epoch\n",
      "\n",
      "epoch: 343\n",
      "batch training loss: 0.02493, ssr: 0.99282, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02636 to 0.02597, saving model\n",
      "\n",
      "epoch: 344\n",
      "batch training loss: 0.02571, ssr: 0.99279, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 345\n",
      "batch training loss: 0.02510, ssr: 0.99277, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 346\n",
      "batch training loss: 0.02685, ssr: 0.99275, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 347\n",
      "batch training loss: 0.02677, ssr: 0.99273, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 348\n",
      "batch training loss: 0.02476, ssr: 0.99271, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 349\n",
      "batch training loss: 0.02519, ssr: 0.99269, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 350\n",
      "batch training loss: 0.02408, ssr: 0.99267, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 351\n",
      "batch training loss: 0.02532, ssr: 0.99265, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 352\n",
      "batch training loss: 0.02567, ssr: 0.99263, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 353\n",
      "batch training loss: 0.02524, ssr: 0.99261, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 354\n",
      "batch training loss: 0.02647, ssr: 0.99258, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 355\n",
      "batch training loss: 0.02457, ssr: 0.99256, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 356\n",
      "batch training loss: 0.02706, ssr: 0.99254, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 357\n",
      "batch training loss: 0.02805, ssr: 0.99252, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 358\n",
      "batch training loss: 0.02637, ssr: 0.99250, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 359\n",
      "batch training loss: 0.02772, ssr: 0.99248, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 360\n",
      "batch training loss: 0.02585, ssr: 0.99246, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 361\n",
      "batch training loss: 0.03127, ssr: 0.99244, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 362\n",
      "batch training loss: 0.03110, ssr: 0.99242, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 363\n",
      "batch training loss: 0.02423, ssr: 0.99240, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 364\n",
      "batch training loss: 0.02639, ssr: 0.99237, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 365\n",
      "batch training loss: 0.02604, ssr: 0.99235, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 366\n",
      "batch training loss: 0.02455, ssr: 0.99233, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02597 to 0.02582, saving model\n",
      "\n",
      "epoch: 367\n",
      "batch training loss: 0.02945, ssr: 0.99231, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 368\n",
      "batch training loss: 0.02888, ssr: 0.99229, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 369\n",
      "batch training loss: 0.02734, ssr: 0.99227, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 370\n",
      "batch training loss: 0.02584, ssr: 0.99225, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 371\n",
      "batch training loss: 0.02554, ssr: 0.99223, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 372\n",
      "batch training loss: 0.02538, ssr: 0.99221, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 373\n",
      "batch training loss: 0.02552, ssr: 0.99219, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 374\n",
      "batch training loss: 0.02511, ssr: 0.99216, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 375\n",
      "batch training loss: 0.02587, ssr: 0.99214, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02582 to 0.02568, saving model\n",
      "\n",
      "epoch: 376\n",
      "batch training loss: 0.03189, ssr: 0.99212, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 377\n",
      "batch training loss: 0.02794, ssr: 0.99210, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 378\n",
      "batch training loss: 0.02638, ssr: 0.99208, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 379\n",
      "batch training loss: 0.02475, ssr: 0.99206, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 380\n",
      "batch training loss: 0.02902, ssr: 0.99204, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 381\n",
      "batch training loss: 0.02575, ssr: 0.99202, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 382\n",
      "batch training loss: 0.02630, ssr: 0.99200, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 383\n",
      "batch training loss: 0.02613, ssr: 0.99198, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 384\n",
      "batch training loss: 0.02637, ssr: 0.99195, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 385\n",
      "batch training loss: 0.03056, ssr: 0.99193, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 386\n",
      "batch training loss: 0.02707, ssr: 0.99191, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 387\n",
      "batch training loss: 0.02602, ssr: 0.99189, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 388\n",
      "batch training loss: 0.02556, ssr: 0.99187, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 389\n",
      "batch training loss: 0.02490, ssr: 0.99185, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 390\n",
      "batch training loss: 0.02473, ssr: 0.99183, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 391\n",
      "batch training loss: 0.02548, ssr: 0.99181, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 392\n",
      "batch training loss: 0.02647, ssr: 0.99179, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 393\n",
      "batch training loss: 0.02462, ssr: 0.99177, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 394\n",
      "batch training loss: 0.02462, ssr: 0.99174, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 395\n",
      "batch training loss: 0.02459, ssr: 0.99172, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 396\n",
      "batch training loss: 0.02586, ssr: 0.99170, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 397\n",
      "batch training loss: 0.02926, ssr: 0.99168, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 398\n",
      "batch training loss: 0.02545, ssr: 0.99166, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 399\n",
      "batch training loss: 0.02473, ssr: 0.99164, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 400\n",
      "batch training loss: 0.02424, ssr: 0.99162, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 401\n",
      "batch training loss: 0.02612, ssr: 0.99160, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 402\n",
      "batch training loss: 0.02491, ssr: 0.99158, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 403\n",
      "batch training loss: 0.02496, ssr: 0.99156, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 404\n",
      "batch training loss: 0.02436, ssr: 0.99153, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 405\n",
      "batch training loss: 0.02522, ssr: 0.99151, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02568 to 0.02567, saving model\n",
      "\n",
      "epoch: 406\n",
      "batch training loss: 0.02457, ssr: 0.99149, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 407\n",
      "batch training loss: 0.02599, ssr: 0.99147, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 408\n",
      "batch training loss: 0.02432, ssr: 0.99145, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 409\n",
      "batch training loss: 0.02475, ssr: 0.99143, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 410\n",
      "batch training loss: 0.02640, ssr: 0.99141, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02567 to 0.02559, saving model\n",
      "\n",
      "epoch: 411\n",
      "batch training loss: 0.02722, ssr: 0.99139, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 412\n",
      "batch training loss: 0.02516, ssr: 0.99137, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 413\n",
      "batch training loss: 0.02483, ssr: 0.99135, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 414\n",
      "batch training loss: 0.02691, ssr: 0.99132, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 415\n",
      "batch training loss: 0.02676, ssr: 0.99130, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 416\n",
      "batch training loss: 0.02901, ssr: 0.99128, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 417\n",
      "batch training loss: 0.02357, ssr: 0.99126, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 418\n",
      "batch training loss: 0.02738, ssr: 0.99124, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 419\n",
      "batch training loss: 0.02622, ssr: 0.99122, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 420\n",
      "batch training loss: 0.02743, ssr: 0.99120, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 421\n",
      "batch training loss: 0.02428, ssr: 0.99118, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 422\n",
      "batch training loss: 0.02807, ssr: 0.99116, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 423\n",
      "batch training loss: 0.02511, ssr: 0.99114, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 424\n",
      "batch training loss: 0.02529, ssr: 0.99111, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 425\n",
      "batch training loss: 0.02385, ssr: 0.99109, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 426\n",
      "batch training loss: 0.02392, ssr: 0.99107, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 427\n",
      "batch training loss: 0.02407, ssr: 0.99105, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 428\n",
      "batch training loss: 0.02619, ssr: 0.99103, lr: 0.00140\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 429\n",
      "batch training loss: 0.02393, ssr: 0.99101, lr: 0.00140\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02559 to 0.02558, saving model\n",
      "\n",
      "epoch: 430\n",
      "batch training loss: 0.02557, ssr: 0.99099, lr: 0.00140\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 431\n",
      "batch training loss: 0.02499, ssr: 0.99097, lr: 0.00140\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 432\n",
      "batch training loss: 0.02527, ssr: 0.99095, lr: 0.00140\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 433\n",
      "batch training loss: 0.02475, ssr: 0.99093, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 434\n",
      "batch training loss: 0.02427, ssr: 0.99090, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 435\n",
      "batch training loss: 0.02454, ssr: 0.99088, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 436\n",
      "batch training loss: 0.02689, ssr: 0.99086, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 437\n",
      "batch training loss: 0.02283, ssr: 0.99084, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 438\n",
      "batch training loss: 0.02595, ssr: 0.99082, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 439\n",
      "batch training loss: 0.02720, ssr: 0.99080, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 440\n",
      "batch training loss: 0.02893, ssr: 0.99078, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 441\n",
      "batch training loss: 0.02498, ssr: 0.99076, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 442\n",
      "batch training loss: 0.02745, ssr: 0.99074, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 443\n",
      "batch training loss: 0.02586, ssr: 0.99072, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 444\n",
      "batch training loss: 0.02470, ssr: 0.99069, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 445\n",
      "batch training loss: 0.02432, ssr: 0.99067, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 446\n",
      "batch training loss: 0.02470, ssr: 0.99065, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 447\n",
      "batch training loss: 0.02458, ssr: 0.99063, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 448\n",
      "batch training loss: 0.02489, ssr: 0.99061, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 449\n",
      "batch training loss: 0.02406, ssr: 0.99059, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 450\n",
      "batch training loss: 0.02408, ssr: 0.99057, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 451\n",
      "batch training loss: 0.02369, ssr: 0.99055, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 452\n",
      "batch training loss: 0.02456, ssr: 0.99053, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 453\n",
      "batch training loss: 0.02606, ssr: 0.99051, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 454\n",
      "batch training loss: 0.02553, ssr: 0.99048, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 455\n",
      "batch training loss: 0.02591, ssr: 0.99046, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 456\n",
      "batch training loss: 0.02570, ssr: 0.99044, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 457\n",
      "batch training loss: 0.02746, ssr: 0.99042, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 458\n",
      "batch training loss: 0.02426, ssr: 0.99040, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 459\n",
      "batch training loss: 0.02665, ssr: 0.99038, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 30 epoch\n",
      "\n",
      "epoch: 460\n",
      "batch training loss: 0.02375, ssr: 0.99036, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 31 epoch\n",
      "\n",
      "epoch: 461\n",
      "batch training loss: 0.02318, ssr: 0.99034, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 32 epoch\n",
      "\n",
      "epoch: 462\n",
      "batch training loss: 0.02607, ssr: 0.99032, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 33 epoch\n",
      "\n",
      "epoch: 463\n",
      "batch training loss: 0.02895, ssr: 0.99030, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 34 epoch\n",
      "\n",
      "epoch: 464\n",
      "batch training loss: 0.03042, ssr: 0.99027, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 35 epoch\n",
      "\n",
      "epoch: 465\n",
      "batch training loss: 0.02546, ssr: 0.99025, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 36 epoch\n",
      "\n",
      "epoch: 466\n",
      "batch training loss: 0.02406, ssr: 0.99023, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 37 epoch\n",
      "\n",
      "epoch: 467\n",
      "batch training loss: 0.02476, ssr: 0.99021, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 38 epoch\n",
      "\n",
      "epoch: 468\n",
      "batch training loss: 0.02455, ssr: 0.99019, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 39 epoch\n",
      "\n",
      "epoch: 469\n",
      "batch training loss: 0.02433, ssr: 0.99017, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.02558 to 0.02500, saving model\n",
      "\n",
      "epoch: 470\n",
      "batch training loss: 0.02546, ssr: 0.99015, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 471\n",
      "batch training loss: 0.02383, ssr: 0.99013, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 472\n",
      "batch training loss: 0.02403, ssr: 0.99011, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 473\n",
      "batch training loss: 0.02409, ssr: 0.99009, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 474\n",
      "batch training loss: 0.02497, ssr: 0.99006, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 475\n",
      "batch training loss: 0.02399, ssr: 0.99004, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 476\n",
      "batch training loss: 0.02449, ssr: 0.99002, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 477\n",
      "batch training loss: 0.02278, ssr: 0.99000, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 478\n",
      "batch training loss: 0.02365, ssr: 0.98998, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 479\n",
      "batch training loss: 0.02362, ssr: 0.98996, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 480\n",
      "batch training loss: 0.02345, ssr: 0.98994, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 481\n",
      "batch training loss: 0.02304, ssr: 0.98992, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 482\n",
      "batch training loss: 0.02315, ssr: 0.98990, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 483\n",
      "batch training loss: 0.02374, ssr: 0.98988, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 484\n",
      "batch training loss: 0.02317, ssr: 0.98985, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 485\n",
      "batch training loss: 0.02345, ssr: 0.98983, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 486\n",
      "batch training loss: 0.02350, ssr: 0.98981, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 487\n",
      "batch training loss: 0.02401, ssr: 0.98979, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 488\n",
      "batch training loss: 0.02325, ssr: 0.98977, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 489\n",
      "batch training loss: 0.02596, ssr: 0.98975, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 490\n",
      "batch training loss: 0.02512, ssr: 0.98973, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 491\n",
      "batch training loss: 0.02348, ssr: 0.98971, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 492\n",
      "batch training loss: 0.02514, ssr: 0.98969, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 493\n",
      "batch training loss: 0.02483, ssr: 0.98967, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 494\n",
      "batch training loss: 0.02393, ssr: 0.98964, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 495\n",
      "batch training loss: 0.02336, ssr: 0.98962, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 496\n",
      "batch training loss: 0.02415, ssr: 0.98960, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 497\n",
      "batch training loss: 0.02364, ssr: 0.98958, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 498\n",
      "batch training loss: 0.02506, ssr: 0.98956, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 499\n",
      "batch training loss: 0.02486, ssr: 0.98954, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 30 epoch\n",
      "\n",
      "epoch: 500\n",
      "batch training loss: 0.02280, ssr: 0.98952, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 31 epoch\n",
      "\n",
      "epoch: 501\n",
      "batch training loss: 0.02486, ssr: 0.98950, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 32 epoch\n",
      "\n",
      "epoch: 502\n",
      "batch training loss: 0.02310, ssr: 0.98948, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 33 epoch\n",
      "\n",
      "epoch: 503\n",
      "batch training loss: 0.02700, ssr: 0.98946, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 34 epoch\n",
      "\n",
      "epoch: 504\n",
      "batch training loss: 0.02465, ssr: 0.98943, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 35 epoch\n",
      "\n",
      "epoch: 505\n",
      "batch training loss: 0.02385, ssr: 0.98941, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 36 epoch\n",
      "\n",
      "epoch: 506\n",
      "batch training loss: 0.02333, ssr: 0.98939, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 37 epoch\n",
      "\n",
      "epoch: 507\n",
      "batch training loss: 0.02382, ssr: 0.98937, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 38 epoch\n",
      "\n",
      "epoch: 508\n",
      "batch training loss: 0.02447, ssr: 0.98935, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 39 epoch\n",
      "\n",
      "epoch: 509\n",
      "batch training loss: 0.02755, ssr: 0.98933, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 40 epoch\n",
      "\n",
      "epoch: 510\n",
      "batch training loss: 0.02472, ssr: 0.98931, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.02500 to 0.02495, saving model\n",
      "\n",
      "epoch: 511\n",
      "batch training loss: 0.02421, ssr: 0.98929, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 512\n",
      "batch training loss: 0.02848, ssr: 0.98927, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 513\n",
      "batch training loss: 0.02382, ssr: 0.98925, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 514\n",
      "batch training loss: 0.02607, ssr: 0.98922, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 515\n",
      "batch training loss: 0.02607, ssr: 0.98920, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 516\n",
      "batch training loss: 0.02292, ssr: 0.98918, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 517\n",
      "batch training loss: 0.02411, ssr: 0.98916, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 518\n",
      "batch training loss: 0.02498, ssr: 0.98914, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 519\n",
      "batch training loss: 0.02450, ssr: 0.98912, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 520\n",
      "batch training loss: 0.02376, ssr: 0.98910, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 521\n",
      "batch training loss: 0.02366, ssr: 0.98908, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 522\n",
      "batch training loss: 0.02365, ssr: 0.98906, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 523\n",
      "batch training loss: 0.02364, ssr: 0.98904, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 524\n",
      "batch training loss: 0.02269, ssr: 0.98901, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 525\n",
      "batch training loss: 0.02594, ssr: 0.98899, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 526\n",
      "batch training loss: 0.02387, ssr: 0.98897, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 527\n",
      "batch training loss: 0.02456, ssr: 0.98895, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 528\n",
      "batch training loss: 0.02424, ssr: 0.98893, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 529\n",
      "batch training loss: 0.02364, ssr: 0.98891, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 530\n",
      "batch training loss: 0.02547, ssr: 0.98889, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 531\n",
      "batch training loss: 0.02455, ssr: 0.98887, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 532\n",
      "batch training loss: 0.02316, ssr: 0.98885, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 533\n",
      "batch training loss: 0.02391, ssr: 0.98883, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 534\n",
      "batch training loss: 0.02372, ssr: 0.98880, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 535\n",
      "batch training loss: 0.02478, ssr: 0.98878, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 536\n",
      "batch training loss: 0.02395, ssr: 0.98876, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 537\n",
      "batch training loss: 0.02385, ssr: 0.98874, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 538\n",
      "batch training loss: 0.02421, ssr: 0.98872, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 539\n",
      "batch training loss: 0.02425, ssr: 0.98870, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 540\n",
      "batch training loss: 0.02266, ssr: 0.98868, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 30 epoch\n",
      "\n",
      "epoch: 541\n",
      "batch training loss: 0.02312, ssr: 0.98866, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 31 epoch\n",
      "\n",
      "epoch: 542\n",
      "batch training loss: 0.02704, ssr: 0.98864, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 32 epoch\n",
      "\n",
      "epoch: 543\n",
      "batch training loss: 0.02311, ssr: 0.98862, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 33 epoch\n",
      "\n",
      "epoch: 544\n",
      "batch training loss: 0.02394, ssr: 0.98859, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 34 epoch\n",
      "\n",
      "epoch: 545\n",
      "batch training loss: 0.02199, ssr: 0.98857, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 35 epoch\n",
      "\n",
      "epoch: 546\n",
      "batch training loss: 0.02428, ssr: 0.98855, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 36 epoch\n",
      "\n",
      "epoch: 547\n",
      "batch training loss: 0.02469, ssr: 0.98853, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 37 epoch\n",
      "\n",
      "epoch: 548\n",
      "batch training loss: 0.02357, ssr: 0.98851, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 38 epoch\n",
      "\n",
      "epoch: 549\n",
      "batch training loss: 0.03083, ssr: 0.98849, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 39 epoch\n",
      "\n",
      "epoch: 550\n",
      "batch training loss: 0.02347, ssr: 0.98847, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 40 epoch\n",
      "\n",
      "epoch: 551\n",
      "batch training loss: 0.02415, ssr: 0.98845, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 41 epoch\n",
      "\n",
      "epoch: 552\n",
      "batch training loss: 0.02673, ssr: 0.98843, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 42 epoch\n",
      "\n",
      "epoch: 553\n",
      "batch training loss: 0.02480, ssr: 0.98841, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 43 epoch\n",
      "\n",
      "epoch: 554\n",
      "batch training loss: 0.02424, ssr: 0.98838, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 44 epoch\n",
      "\n",
      "epoch: 555\n",
      "batch training loss: 0.02282, ssr: 0.98836, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 45 epoch\n",
      "\n",
      "epoch: 556\n",
      "batch training loss: 0.02289, ssr: 0.98834, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 46 epoch\n",
      "\n",
      "epoch: 557\n",
      "batch training loss: 0.02490, ssr: 0.98832, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 47 epoch\n",
      "\n",
      "epoch: 558\n",
      "batch training loss: 0.02446, ssr: 0.98830, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 48 epoch\n",
      "\n",
      "epoch: 559\n",
      "batch training loss: 0.02313, ssr: 0.98828, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 49 epoch\n",
      "\n",
      "epoch: 560\n",
      "batch training loss: 0.02426, ssr: 0.98826, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 50 epoch\n",
      "\n",
      "epoch: 561\n",
      "batch training loss: 0.02304, ssr: 0.98824, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 51 epoch\n",
      "\n",
      "epoch: 562\n",
      "batch training loss: 0.02309, ssr: 0.98822, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 52 epoch\n",
      "\n",
      "epoch: 563\n",
      "batch training loss: 0.02460, ssr: 0.98820, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 53 epoch\n",
      "\n",
      "epoch: 564\n",
      "batch training loss: 0.02316, ssr: 0.98817, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 54 epoch\n",
      "\n",
      "epoch: 565\n",
      "batch training loss: 0.02366, ssr: 0.98815, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 55 epoch\n",
      "\n",
      "epoch: 566\n",
      "batch training loss: 0.02260, ssr: 0.98813, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 56 epoch\n",
      "\n",
      "epoch: 567\n",
      "batch training loss: 0.02321, ssr: 0.98811, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 57 epoch\n",
      "\n",
      "epoch: 568\n",
      "batch training loss: 0.02481, ssr: 0.98809, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 58 epoch\n",
      "\n",
      "epoch: 569\n",
      "batch training loss: 0.02167, ssr: 0.98807, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 59 epoch\n",
      "\n",
      "epoch: 570\n",
      "batch training loss: 0.02230, ssr: 0.98805, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 60 epoch\n",
      "\n",
      "epoch: 571\n",
      "batch training loss: 0.02365, ssr: 0.98803, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 61 epoch\n",
      "\n",
      "epoch: 572\n",
      "batch training loss: 0.02380, ssr: 0.98801, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 62 epoch\n",
      "\n",
      "epoch: 573\n",
      "batch training loss: 0.02479, ssr: 0.98799, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 63 epoch\n",
      "\n",
      "epoch: 574\n",
      "batch training loss: 0.02343, ssr: 0.98796, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 64 epoch\n",
      "\n",
      "epoch: 575\n",
      "batch training loss: 0.02710, ssr: 0.98794, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 65 epoch\n",
      "\n",
      "epoch: 576\n",
      "batch training loss: 0.02393, ssr: 0.98792, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 66 epoch\n",
      "\n",
      "epoch: 577\n",
      "batch training loss: 0.02303, ssr: 0.98790, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 67 epoch\n",
      "\n",
      "epoch: 578\n",
      "batch training loss: 0.02339, ssr: 0.98788, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 68 epoch\n",
      "\n",
      "epoch: 579\n",
      "batch training loss: 0.02214, ssr: 0.98786, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 69 epoch\n",
      "\n",
      "epoch: 580\n",
      "batch training loss: 0.02335, ssr: 0.98784, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 70 epoch\n",
      "\n",
      "epoch: 581\n",
      "batch training loss: 0.02396, ssr: 0.98782, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 71 epoch\n",
      "\n",
      "epoch: 582\n",
      "batch training loss: 0.02463, ssr: 0.98780, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 72 epoch\n",
      "\n",
      "epoch: 583\n",
      "batch training loss: 0.02389, ssr: 0.98778, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 73 epoch\n",
      "\n",
      "epoch: 584\n",
      "batch training loss: 0.02282, ssr: 0.98775, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 74 epoch\n",
      "\n",
      "epoch: 585\n",
      "batch training loss: 0.02303, ssr: 0.98773, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 75 epoch\n",
      "\n",
      "epoch: 586\n",
      "batch training loss: 0.02236, ssr: 0.98771, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 76 epoch\n",
      "\n",
      "epoch: 587\n",
      "batch training loss: 0.02264, ssr: 0.98769, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 77 epoch\n",
      "\n",
      "epoch: 588\n",
      "batch training loss: 0.02258, ssr: 0.98767, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 78 epoch\n",
      "\n",
      "epoch: 589\n",
      "batch training loss: 0.02311, ssr: 0.98765, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 79 epoch\n",
      "\n",
      "epoch: 590\n",
      "batch training loss: 0.02235, ssr: 0.98763, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 80 epoch\n",
      "\n",
      "epoch: 591\n",
      "batch training loss: 0.02293, ssr: 0.98761, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 81 epoch\n",
      "\n",
      "epoch: 592\n",
      "batch training loss: 0.02303, ssr: 0.98759, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 82 epoch\n",
      "\n",
      "epoch: 593\n",
      "batch training loss: 0.02417, ssr: 0.98757, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 83 epoch\n",
      "\n",
      "epoch: 594\n",
      "batch training loss: 0.02378, ssr: 0.98754, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 84 epoch\n",
      "\n",
      "epoch: 595\n",
      "batch training loss: 0.02410, ssr: 0.98752, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 85 epoch\n",
      "\n",
      "epoch: 596\n",
      "batch training loss: 0.02371, ssr: 0.98750, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 86 epoch\n",
      "\n",
      "epoch: 597\n",
      "batch training loss: 0.02287, ssr: 0.98748, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 87 epoch\n",
      "\n",
      "epoch: 598\n",
      "batch training loss: 0.02268, ssr: 0.98746, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 88 epoch\n",
      "\n",
      "epoch: 599\n",
      "batch training loss: 0.02330, ssr: 0.98744, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 89 epoch\n",
      "\n",
      "epoch: 600\n",
      "batch training loss: 0.02242, ssr: 0.98742, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 90 epoch\n",
      "\n",
      "epoch: 601\n",
      "batch training loss: 0.02223, ssr: 0.98740, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 91 epoch\n",
      "\n",
      "epoch: 602\n",
      "batch training loss: 0.02276, ssr: 0.98738, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 92 epoch\n",
      "\n",
      "epoch: 603\n",
      "batch training loss: 0.02193, ssr: 0.98736, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 93 epoch\n",
      "\n",
      "epoch: 604\n",
      "batch training loss: 0.02477, ssr: 0.98733, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 94 epoch\n",
      "\n",
      "epoch: 605\n",
      "batch training loss: 0.02202, ssr: 0.98731, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 95 epoch\n",
      "\n",
      "epoch: 606\n",
      "batch training loss: 0.02325, ssr: 0.98729, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 96 epoch\n",
      "\n",
      "epoch: 607\n",
      "batch training loss: 0.02426, ssr: 0.98727, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 97 epoch\n",
      "\n",
      "epoch: 608\n",
      "batch training loss: 0.02406, ssr: 0.98725, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 98 epoch\n",
      "\n",
      "epoch: 609\n",
      "batch training loss: 0.02244, ssr: 0.98723, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 99 epoch\n",
      "\n",
      "epoch: 610\n",
      "batch training loss: 0.02258, ssr: 0.98721, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 100 epoch\n",
      "\n",
      "epoch: 611\n",
      "batch training loss: 0.02302, ssr: 0.98719, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 101 epoch\n",
      "\n",
      "epoch: 612\n",
      "batch training loss: 0.02312, ssr: 0.98717, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 102 epoch\n",
      "\n",
      "epoch: 613\n",
      "batch training loss: 0.02261, ssr: 0.98715, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 103 epoch\n",
      "\n",
      "epoch: 614\n",
      "batch training loss: 0.02265, ssr: 0.98712, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 104 epoch\n",
      "\n",
      "epoch: 615\n",
      "batch training loss: 0.02372, ssr: 0.98710, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 105 epoch\n",
      "\n",
      "epoch: 616\n",
      "batch training loss: 0.02434, ssr: 0.98708, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 106 epoch\n",
      "\n",
      "epoch: 617\n",
      "batch training loss: 0.02313, ssr: 0.98706, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 107 epoch\n",
      "\n",
      "epoch: 618\n",
      "batch training loss: 0.02287, ssr: 0.98704, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 108 epoch\n",
      "\n",
      "epoch: 619\n",
      "batch training loss: 0.02277, ssr: 0.98702, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 109 epoch\n",
      "\n",
      "epoch: 620\n",
      "batch training loss: 0.02316, ssr: 0.98700, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 110 epoch\n",
      "\n",
      "epoch: 621\n",
      "batch training loss: 0.02258, ssr: 0.98698, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 111 epoch\n",
      "\n",
      "epoch: 622\n",
      "batch training loss: 0.02251, ssr: 0.98696, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 112 epoch\n",
      "\n",
      "epoch: 623\n",
      "batch training loss: 0.02340, ssr: 0.98694, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 113 epoch\n",
      "\n",
      "epoch: 624\n",
      "batch training loss: 0.02359, ssr: 0.98691, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 114 epoch\n",
      "\n",
      "epoch: 625\n",
      "batch training loss: 0.02310, ssr: 0.98689, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 115 epoch\n",
      "\n",
      "epoch: 626\n",
      "batch training loss: 0.02296, ssr: 0.98687, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 116 epoch\n",
      "\n",
      "epoch: 627\n",
      "batch training loss: 0.02355, ssr: 0.98685, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 117 epoch\n",
      "\n",
      "epoch: 628\n",
      "batch training loss: 0.02264, ssr: 0.98683, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 118 epoch\n",
      "\n",
      "epoch: 629\n",
      "batch training loss: 0.02390, ssr: 0.98681, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 119 epoch\n",
      "\n",
      "epoch: 630\n",
      "batch training loss: 0.02304, ssr: 0.98679, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 120 epoch\n",
      "\n",
      "epoch: 631\n",
      "batch training loss: 0.02557, ssr: 0.98677, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 121 epoch\n",
      "\n",
      "epoch: 632\n",
      "batch training loss: 0.02369, ssr: 0.98675, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 122 epoch\n",
      "\n",
      "epoch: 633\n",
      "batch training loss: 0.02400, ssr: 0.98673, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 123 epoch\n",
      "\n",
      "epoch: 634\n",
      "batch training loss: 0.02301, ssr: 0.98670, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 124 epoch\n",
      "\n",
      "epoch: 635\n",
      "batch training loss: 0.02329, ssr: 0.98668, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 125 epoch\n",
      "\n",
      "epoch: 636\n",
      "batch training loss: 0.02341, ssr: 0.98666, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 126 epoch\n",
      "\n",
      "epoch: 637\n",
      "batch training loss: 0.02332, ssr: 0.98664, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 127 epoch\n",
      "\n",
      "epoch: 638\n",
      "batch training loss: 0.02239, ssr: 0.98662, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 128 epoch\n",
      "\n",
      "epoch: 639\n",
      "batch training loss: 0.02193, ssr: 0.98660, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 129 epoch\n",
      "\n",
      "epoch: 640\n",
      "batch training loss: 0.02288, ssr: 0.98658, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 130 epoch\n",
      "\n",
      "epoch: 641\n",
      "batch training loss: 0.02220, ssr: 0.98656, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 131 epoch\n",
      "\n",
      "epoch: 642\n",
      "batch training loss: 0.02199, ssr: 0.98654, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 132 epoch\n",
      "\n",
      "epoch: 643\n",
      "batch training loss: 0.02083, ssr: 0.98652, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 133 epoch\n",
      "\n",
      "epoch: 644\n",
      "batch training loss: 0.02301, ssr: 0.98649, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 134 epoch\n",
      "\n",
      "epoch: 645\n",
      "batch training loss: 0.02203, ssr: 0.98647, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 135 epoch\n",
      "\n",
      "epoch: 646\n",
      "batch training loss: 0.02354, ssr: 0.98645, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 136 epoch\n",
      "\n",
      "epoch: 647\n",
      "batch training loss: 0.02415, ssr: 0.98643, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 137 epoch\n",
      "\n",
      "epoch: 648\n",
      "batch training loss: 0.02304, ssr: 0.98641, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 138 epoch\n",
      "\n",
      "epoch: 649\n",
      "batch training loss: 0.02264, ssr: 0.98639, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 139 epoch\n",
      "\n",
      "epoch: 650\n",
      "batch training loss: 0.02244, ssr: 0.98637, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 140 epoch\n",
      "\n",
      "epoch: 651\n",
      "batch training loss: 0.02097, ssr: 0.98635, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 141 epoch\n",
      "\n",
      "epoch: 652\n",
      "batch training loss: 0.02240, ssr: 0.98633, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 142 epoch\n",
      "\n",
      "epoch: 653\n",
      "batch training loss: 0.02185, ssr: 0.98631, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 143 epoch\n",
      "\n",
      "epoch: 654\n",
      "batch training loss: 0.02288, ssr: 0.98628, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 144 epoch\n",
      "\n",
      "epoch: 655\n",
      "batch training loss: 0.02190, ssr: 0.98626, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 145 epoch\n",
      "\n",
      "epoch: 656\n",
      "batch training loss: 0.02218, ssr: 0.98624, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 146 epoch\n",
      "\n",
      "epoch: 657\n",
      "batch training loss: 0.02177, ssr: 0.98622, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 147 epoch\n",
      "\n",
      "epoch: 658\n",
      "batch training loss: 0.02368, ssr: 0.98620, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 148 epoch\n",
      "\n",
      "epoch: 659\n",
      "batch training loss: 0.02387, ssr: 0.98618, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 149 epoch\n",
      "\n",
      "epoch: 660\n",
      "batch training loss: 0.02404, ssr: 0.98616, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 150 epoch\n",
      "\n",
      "epoch: 661\n",
      "batch training loss: 0.02579, ssr: 0.98614, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 151 epoch\n",
      "\n",
      "epoch: 662\n",
      "batch training loss: 0.02525, ssr: 0.98612, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 152 epoch\n",
      "\n",
      "epoch: 663\n",
      "batch training loss: 0.02310, ssr: 0.98610, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 153 epoch\n",
      "\n",
      "epoch: 664\n",
      "batch training loss: 0.02261, ssr: 0.98607, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 154 epoch\n",
      "\n",
      "epoch: 665\n",
      "batch training loss: 0.02314, ssr: 0.98605, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 155 epoch\n",
      "\n",
      "epoch: 666\n",
      "batch training loss: 0.02184, ssr: 0.98603, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 156 epoch\n",
      "\n",
      "epoch: 667\n",
      "batch training loss: 0.02351, ssr: 0.98601, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 157 epoch\n",
      "\n",
      "epoch: 668\n",
      "batch training loss: 0.02309, ssr: 0.98599, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 158 epoch\n",
      "\n",
      "epoch: 669\n",
      "batch training loss: 0.02227, ssr: 0.98597, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 159 epoch\n",
      "\n",
      "epoch: 670\n",
      "batch training loss: 0.02332, ssr: 0.98595, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 160 epoch\n",
      "\n",
      "epoch: 671\n",
      "batch training loss: 0.02189, ssr: 0.98593, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 161 epoch\n",
      "\n",
      "epoch: 672\n",
      "batch training loss: 0.02393, ssr: 0.98591, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 162 epoch\n",
      "\n",
      "epoch: 673\n",
      "batch training loss: 0.02058, ssr: 0.98589, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 163 epoch\n",
      "\n",
      "epoch: 674\n",
      "batch training loss: 0.02295, ssr: 0.98586, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 164 epoch\n",
      "\n",
      "epoch: 675\n",
      "batch training loss: 0.02139, ssr: 0.98584, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 165 epoch\n",
      "\n",
      "epoch: 676\n",
      "batch training loss: 0.02202, ssr: 0.98582, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 166 epoch\n",
      "\n",
      "epoch: 677\n",
      "batch training loss: 0.02202, ssr: 0.98580, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 167 epoch\n",
      "\n",
      "epoch: 678\n",
      "batch training loss: 0.02283, ssr: 0.98578, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 168 epoch\n",
      "\n",
      "epoch: 679\n",
      "batch training loss: 0.02113, ssr: 0.98576, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 169 epoch\n",
      "\n",
      "epoch: 680\n",
      "batch training loss: 0.02169, ssr: 0.98574, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 170 epoch\n",
      "\n",
      "epoch: 681\n",
      "batch training loss: 0.02226, ssr: 0.98572, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 171 epoch\n",
      "\n",
      "epoch: 682\n",
      "batch training loss: 0.02219, ssr: 0.98570, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 172 epoch\n",
      "\n",
      "epoch: 683\n",
      "batch training loss: 0.02095, ssr: 0.98568, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 173 epoch\n",
      "\n",
      "epoch: 684\n",
      "batch training loss: 0.02380, ssr: 0.98565, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 174 epoch\n",
      "\n",
      "epoch: 685\n",
      "batch training loss: 0.02518, ssr: 0.98563, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 175 epoch\n",
      "\n",
      "epoch: 686\n",
      "batch training loss: 0.02162, ssr: 0.98561, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 176 epoch\n",
      "\n",
      "epoch: 687\n",
      "batch training loss: 0.02331, ssr: 0.98559, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 177 epoch\n",
      "\n",
      "epoch: 688\n",
      "batch training loss: 0.02247, ssr: 0.98557, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 178 epoch\n",
      "\n",
      "epoch: 689\n",
      "batch training loss: 0.02248, ssr: 0.98555, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 179 epoch\n",
      "\n",
      "epoch: 690\n",
      "batch training loss: 0.02353, ssr: 0.98553, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 180 epoch\n",
      "\n",
      "epoch: 691\n",
      "batch training loss: 0.02345, ssr: 0.98551, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 181 epoch\n",
      "\n",
      "epoch: 692\n",
      "batch training loss: 0.02449, ssr: 0.98549, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 182 epoch\n",
      "\n",
      "epoch: 693\n",
      "batch training loss: 0.02175, ssr: 0.98547, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 183 epoch\n",
      "\n",
      "epoch: 694\n",
      "batch training loss: 0.02153, ssr: 0.98544, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 184 epoch\n",
      "\n",
      "epoch: 695\n",
      "batch training loss: 0.02183, ssr: 0.98542, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 185 epoch\n",
      "\n",
      "epoch: 696\n",
      "batch training loss: 0.02235, ssr: 0.98540, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 186 epoch\n",
      "\n",
      "epoch: 697\n",
      "batch training loss: 0.02247, ssr: 0.98538, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 187 epoch\n",
      "\n",
      "epoch: 698\n",
      "batch training loss: 0.02419, ssr: 0.98536, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 188 epoch\n",
      "\n",
      "epoch: 699\n",
      "batch training loss: 0.02233, ssr: 0.98534, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 189 epoch\n",
      "\n",
      "epoch: 700\n",
      "batch training loss: 0.02180, ssr: 0.98532, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 190 epoch\n",
      "\n",
      "epoch: 701\n",
      "batch training loss: 0.02104, ssr: 0.98530, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 191 epoch\n",
      "\n",
      "epoch: 702\n",
      "batch training loss: 0.02259, ssr: 0.98528, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 192 epoch\n",
      "\n",
      "epoch: 703\n",
      "batch training loss: 0.02136, ssr: 0.98526, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 193 epoch\n",
      "\n",
      "epoch: 704\n",
      "batch training loss: 0.02284, ssr: 0.98523, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 194 epoch\n",
      "\n",
      "epoch: 705\n",
      "batch training loss: 0.02276, ssr: 0.98521, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 195 epoch\n",
      "\n",
      "epoch: 706\n",
      "batch training loss: 0.02331, ssr: 0.98519, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 196 epoch\n",
      "\n",
      "epoch: 707\n",
      "batch training loss: 0.02227, ssr: 0.98517, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 197 epoch\n",
      "\n",
      "epoch: 708\n",
      "batch training loss: 0.02182, ssr: 0.98515, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 198 epoch\n",
      "\n",
      "epoch: 709\n",
      "batch training loss: 0.02204, ssr: 0.98513, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 199 epoch\n",
      "\n",
      "epoch: 710\n",
      "batch training loss: 0.02216, ssr: 0.98511, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 200 epoch\n",
      "\n",
      "epoch: 711\n",
      "batch training loss: 0.02131, ssr: 0.98509, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 201 epoch\n",
      "\n",
      "epoch: 712\n",
      "batch training loss: 0.02026, ssr: 0.98507, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 202 epoch\n",
      "\n",
      "epoch: 713\n",
      "batch training loss: 0.02180, ssr: 0.98505, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 203 epoch\n",
      "\n",
      "epoch: 714\n",
      "batch training loss: 0.02207, ssr: 0.98502, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 204 epoch\n",
      "\n",
      "epoch: 715\n",
      "batch training loss: 0.02171, ssr: 0.98500, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 205 epoch\n",
      "\n",
      "epoch: 716\n",
      "batch training loss: 0.02321, ssr: 0.98498, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 206 epoch\n",
      "\n",
      "epoch: 717\n",
      "batch training loss: 0.02220, ssr: 0.98496, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 207 epoch\n",
      "\n",
      "epoch: 718\n",
      "batch training loss: 0.02118, ssr: 0.98494, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 208 epoch\n",
      "\n",
      "epoch: 719\n",
      "batch training loss: 0.02209, ssr: 0.98492, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 209 epoch\n",
      "\n",
      "epoch: 720\n",
      "batch training loss: 0.02046, ssr: 0.98490, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 210 epoch\n",
      "\n",
      "epoch: 721\n",
      "batch training loss: 0.02122, ssr: 0.98488, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 211 epoch\n",
      "\n",
      "epoch: 722\n",
      "batch training loss: 0.02414, ssr: 0.98486, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 212 epoch\n",
      "\n",
      "epoch: 723\n",
      "batch training loss: 0.02167, ssr: 0.98484, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 213 epoch\n",
      "\n",
      "epoch: 724\n",
      "batch training loss: 0.02138, ssr: 0.98481, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 214 epoch\n",
      "\n",
      "epoch: 725\n",
      "batch training loss: 0.02058, ssr: 0.98479, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 215 epoch\n",
      "\n",
      "epoch: 726\n",
      "batch training loss: 0.02112, ssr: 0.98477, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 216 epoch\n",
      "\n",
      "epoch: 727\n",
      "batch training loss: 0.02186, ssr: 0.98475, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 217 epoch\n",
      "\n",
      "epoch: 728\n",
      "batch training loss: 0.02318, ssr: 0.98473, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 218 epoch\n",
      "\n",
      "epoch: 729\n",
      "batch training loss: 0.02104, ssr: 0.98471, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 219 epoch\n",
      "\n",
      "epoch: 730\n",
      "batch training loss: 0.02153, ssr: 0.98469, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 220 epoch\n",
      "\n",
      "epoch: 731\n",
      "batch training loss: 0.02070, ssr: 0.98467, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 221 epoch\n",
      "\n",
      "epoch: 732\n",
      "batch training loss: 0.02166, ssr: 0.98465, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 222 epoch\n",
      "\n",
      "epoch: 733\n",
      "batch training loss: 0.02048, ssr: 0.98463, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 223 epoch\n",
      "\n",
      "epoch: 734\n",
      "batch training loss: 0.02160, ssr: 0.98460, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 224 epoch\n",
      "\n",
      "epoch: 735\n",
      "batch training loss: 0.02285, ssr: 0.98458, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 225 epoch\n",
      "\n",
      "epoch: 736\n",
      "batch training loss: 0.02073, ssr: 0.98456, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 226 epoch\n",
      "\n",
      "epoch: 737\n",
      "batch training loss: 0.02125, ssr: 0.98454, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 227 epoch\n",
      "\n",
      "epoch: 738\n",
      "batch training loss: 0.02175, ssr: 0.98452, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 228 epoch\n",
      "\n",
      "epoch: 739\n",
      "batch training loss: 0.02084, ssr: 0.98450, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 229 epoch\n",
      "\n",
      "epoch: 740\n",
      "batch training loss: 0.02062, ssr: 0.98448, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 230 epoch\n",
      "\n",
      "epoch: 741\n",
      "batch training loss: 0.02323, ssr: 0.98446, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 231 epoch\n",
      "\n",
      "epoch: 742\n",
      "batch training loss: 0.02200, ssr: 0.98444, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 232 epoch\n",
      "\n",
      "epoch: 743\n",
      "batch training loss: 0.02106, ssr: 0.98442, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 233 epoch\n",
      "\n",
      "epoch: 744\n",
      "batch training loss: 0.02161, ssr: 0.98439, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 234 epoch\n",
      "\n",
      "epoch: 745\n",
      "batch training loss: 0.02225, ssr: 0.98437, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 235 epoch\n",
      "\n",
      "epoch: 746\n",
      "batch training loss: 0.02030, ssr: 0.98435, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 236 epoch\n",
      "\n",
      "epoch: 747\n",
      "batch training loss: 0.02120, ssr: 0.98433, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 237 epoch\n",
      "\n",
      "epoch: 748\n",
      "batch training loss: 0.02208, ssr: 0.98431, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 238 epoch\n",
      "\n",
      "epoch: 749\n",
      "batch training loss: 0.02107, ssr: 0.98429, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 239 epoch\n",
      "\n",
      "epoch: 750\n",
      "batch training loss: 0.02061, ssr: 0.98427, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 240 epoch\n",
      "\n",
      "epoch: 751\n",
      "batch training loss: 0.02112, ssr: 0.98425, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 241 epoch\n",
      "\n",
      "epoch: 752\n",
      "batch training loss: 0.02202, ssr: 0.98423, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 242 epoch\n",
      "\n",
      "epoch: 753\n",
      "batch training loss: 0.02104, ssr: 0.98421, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 243 epoch\n",
      "\n",
      "epoch: 754\n",
      "batch training loss: 0.02238, ssr: 0.98418, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 244 epoch\n",
      "\n",
      "epoch: 755\n",
      "batch training loss: 0.02117, ssr: 0.98416, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 245 epoch\n",
      "\n",
      "epoch: 756\n",
      "batch training loss: 0.02124, ssr: 0.98414, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 246 epoch\n",
      "\n",
      "epoch: 757\n",
      "batch training loss: 0.02104, ssr: 0.98412, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 247 epoch\n",
      "\n",
      "epoch: 758\n",
      "batch training loss: 0.02049, ssr: 0.98410, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 248 epoch\n",
      "\n",
      "epoch: 759\n",
      "batch training loss: 0.02067, ssr: 0.98408, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 249 epoch\n",
      "\n",
      "epoch: 760\n",
      "batch training loss: 0.02108, ssr: 0.98406, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 250 epoch\n",
      "\n",
      "epoch: 761\n",
      "batch training loss: 0.01959, ssr: 0.98404, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 251 epoch\n",
      "\n",
      "epoch: 762\n",
      "batch training loss: 0.02084, ssr: 0.98402, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 252 epoch\n",
      "\n",
      "epoch: 763\n",
      "batch training loss: 0.02144, ssr: 0.98400, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 253 epoch\n",
      "\n",
      "epoch: 764\n",
      "batch training loss: 0.02042, ssr: 0.98397, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 254 epoch\n",
      "\n",
      "epoch: 765\n",
      "batch training loss: 0.02015, ssr: 0.98395, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 255 epoch\n",
      "\n",
      "epoch: 766\n",
      "batch training loss: 0.01993, ssr: 0.98393, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 256 epoch\n",
      "\n",
      "epoch: 767\n",
      "batch training loss: 0.02041, ssr: 0.98391, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 257 epoch\n",
      "\n",
      "epoch: 768\n",
      "batch training loss: 0.01973, ssr: 0.98389, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 258 epoch\n",
      "\n",
      "epoch: 769\n",
      "batch training loss: 0.02136, ssr: 0.98387, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 259 epoch\n",
      "\n",
      "epoch: 770\n",
      "batch training loss: 0.02126, ssr: 0.98385, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 260 epoch\n",
      "\n",
      "epoch: 771\n",
      "batch training loss: 0.02188, ssr: 0.98383, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 261 epoch\n",
      "\n",
      "epoch: 772\n",
      "batch training loss: 0.01956, ssr: 0.98381, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 262 epoch\n",
      "\n",
      "epoch: 773\n",
      "batch training loss: 0.02065, ssr: 0.98379, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 263 epoch\n",
      "\n",
      "epoch: 774\n",
      "batch training loss: 0.02073, ssr: 0.98376, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 264 epoch\n",
      "\n",
      "epoch: 775\n",
      "batch training loss: 0.01954, ssr: 0.98374, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 265 epoch\n",
      "\n",
      "epoch: 776\n",
      "batch training loss: 0.02347, ssr: 0.98372, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 266 epoch\n",
      "\n",
      "epoch: 777\n",
      "batch training loss: 0.02200, ssr: 0.98370, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 267 epoch\n",
      "\n",
      "epoch: 778\n",
      "batch training loss: 0.02175, ssr: 0.98368, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 268 epoch\n",
      "\n",
      "epoch: 779\n",
      "batch training loss: 0.02172, ssr: 0.98366, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 269 epoch\n",
      "\n",
      "epoch: 780\n",
      "batch training loss: 0.01982, ssr: 0.98364, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 270 epoch\n",
      "\n",
      "epoch: 781\n",
      "batch training loss: 0.02201, ssr: 0.98362, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 271 epoch\n",
      "\n",
      "epoch: 782\n",
      "batch training loss: 0.02037, ssr: 0.98360, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 272 epoch\n",
      "\n",
      "epoch: 783\n",
      "batch training loss: 0.01988, ssr: 0.98358, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 273 epoch\n",
      "\n",
      "epoch: 784\n",
      "batch training loss: 0.01992, ssr: 0.98355, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 274 epoch\n",
      "\n",
      "epoch: 785\n",
      "batch training loss: 0.02050, ssr: 0.98353, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 275 epoch\n",
      "\n",
      "epoch: 786\n",
      "batch training loss: 0.02461, ssr: 0.98351, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 276 epoch\n",
      "\n",
      "epoch: 787\n",
      "batch training loss: 0.02017, ssr: 0.98349, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 277 epoch\n",
      "\n",
      "epoch: 788\n",
      "batch training loss: 0.01976, ssr: 0.98347, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 278 epoch\n",
      "\n",
      "epoch: 789\n",
      "batch training loss: 0.02160, ssr: 0.98345, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 279 epoch\n",
      "\n",
      "epoch: 790\n",
      "batch training loss: 0.02226, ssr: 0.98343, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 280 epoch\n",
      "\n",
      "epoch: 791\n",
      "batch training loss: 0.02147, ssr: 0.98341, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 281 epoch\n",
      "\n",
      "epoch: 792\n",
      "batch training loss: 0.02066, ssr: 0.98339, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 282 epoch\n",
      "\n",
      "epoch: 793\n",
      "batch training loss: 0.02070, ssr: 0.98337, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 283 epoch\n",
      "\n",
      "epoch: 794\n",
      "batch training loss: 0.02115, ssr: 0.98334, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 284 epoch\n",
      "\n",
      "epoch: 795\n",
      "batch training loss: 0.02046, ssr: 0.98332, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 285 epoch\n",
      "\n",
      "epoch: 796\n",
      "batch training loss: 0.02116, ssr: 0.98330, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 286 epoch\n",
      "\n",
      "epoch: 797\n",
      "batch training loss: 0.02101, ssr: 0.98328, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 287 epoch\n",
      "\n",
      "epoch: 798\n",
      "batch training loss: 0.02096, ssr: 0.98326, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 288 epoch\n",
      "\n",
      "epoch: 799\n",
      "batch training loss: 0.02127, ssr: 0.98324, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 289 epoch\n",
      "\n",
      "epoch: 800\n",
      "batch training loss: 0.02156, ssr: 0.98322, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 290 epoch\n",
      "\n",
      "epoch: 801\n",
      "batch training loss: 0.02104, ssr: 0.98320, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 291 epoch\n",
      "\n",
      "epoch: 802\n",
      "batch training loss: 0.02009, ssr: 0.98318, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 292 epoch\n",
      "\n",
      "epoch: 803\n",
      "batch training loss: 0.02139, ssr: 0.98316, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 293 epoch\n",
      "\n",
      "epoch: 804\n",
      "batch training loss: 0.02244, ssr: 0.98313, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 294 epoch\n",
      "\n",
      "epoch: 805\n",
      "batch training loss: 0.01899, ssr: 0.98311, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 295 epoch\n",
      "\n",
      "epoch: 806\n",
      "batch training loss: 0.01927, ssr: 0.98309, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 296 epoch\n",
      "\n",
      "epoch: 807\n",
      "batch training loss: 0.01960, ssr: 0.98307, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 297 epoch\n",
      "\n",
      "epoch: 808\n",
      "batch training loss: 0.02152, ssr: 0.98305, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 298 epoch\n",
      "\n",
      "epoch: 809\n",
      "batch training loss: 0.02015, ssr: 0.98303, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 299 epoch\n",
      "\n",
      "epoch: 810\n",
      "batch training loss: 0.02037, ssr: 0.98301, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 300 epoch\n",
      "\n",
      "epoch: 811\n",
      "batch training loss: 0.02210, ssr: 0.98299, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 301 epoch\n",
      "\n",
      "epoch: 812\n",
      "batch training loss: 0.02028, ssr: 0.98297, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 302 epoch\n",
      "\n",
      "epoch: 813\n",
      "batch training loss: 0.02057, ssr: 0.98295, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 303 epoch\n",
      "\n",
      "epoch: 814\n",
      "batch training loss: 0.02139, ssr: 0.98292, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 304 epoch\n",
      "\n",
      "epoch: 815\n",
      "batch training loss: 0.02206, ssr: 0.98290, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 305 epoch\n",
      "\n",
      "epoch: 816\n",
      "batch training loss: 0.01950, ssr: 0.98288, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 306 epoch\n",
      "\n",
      "epoch: 817\n",
      "batch training loss: 0.01962, ssr: 0.98286, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 307 epoch\n",
      "\n",
      "epoch: 818\n",
      "batch training loss: 0.02303, ssr: 0.98284, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 308 epoch\n",
      "\n",
      "epoch: 819\n",
      "batch training loss: 0.02009, ssr: 0.98282, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 309 epoch\n",
      "\n",
      "epoch: 820\n",
      "batch training loss: 0.01978, ssr: 0.98280, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 310 epoch\n",
      "\n",
      "epoch: 821\n",
      "batch training loss: 0.01943, ssr: 0.98278, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 311 epoch\n",
      "\n",
      "epoch: 822\n",
      "batch training loss: 0.01995, ssr: 0.98276, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 312 epoch\n",
      "\n",
      "epoch: 823\n",
      "batch training loss: 0.02133, ssr: 0.98274, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 313 epoch\n",
      "\n",
      "epoch: 824\n",
      "batch training loss: 0.02100, ssr: 0.98271, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 314 epoch\n",
      "\n",
      "epoch: 825\n",
      "batch training loss: 0.01972, ssr: 0.98269, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 315 epoch\n",
      "\n",
      "epoch: 826\n",
      "batch training loss: 0.02184, ssr: 0.98267, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 316 epoch\n",
      "\n",
      "epoch: 827\n",
      "batch training loss: 0.02127, ssr: 0.98265, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 317 epoch\n",
      "\n",
      "epoch: 828\n",
      "batch training loss: 0.01978, ssr: 0.98263, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 318 epoch\n",
      "\n",
      "epoch: 829\n",
      "batch training loss: 0.02179, ssr: 0.98261, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 319 epoch\n",
      "\n",
      "epoch: 830\n",
      "batch training loss: 0.02052, ssr: 0.98259, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 320 epoch\n",
      "\n",
      "epoch: 831\n",
      "batch training loss: 0.02089, ssr: 0.98257, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 321 epoch\n",
      "\n",
      "epoch: 832\n",
      "batch training loss: 0.01999, ssr: 0.98255, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 322 epoch\n",
      "\n",
      "epoch: 833\n",
      "batch training loss: 0.02013, ssr: 0.98253, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 323 epoch\n",
      "\n",
      "epoch: 834\n",
      "batch training loss: 0.01972, ssr: 0.98250, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 324 epoch\n",
      "\n",
      "epoch: 835\n",
      "batch training loss: 0.02137, ssr: 0.98248, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 325 epoch\n",
      "\n",
      "epoch: 836\n",
      "batch training loss: 0.01948, ssr: 0.98246, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 326 epoch\n",
      "\n",
      "epoch: 837\n",
      "batch training loss: 0.01985, ssr: 0.98244, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 327 epoch\n",
      "\n",
      "epoch: 838\n",
      "batch training loss: 0.02006, ssr: 0.98242, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 328 epoch\n",
      "\n",
      "epoch: 839\n",
      "batch training loss: 0.02041, ssr: 0.98240, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 329 epoch\n",
      "\n",
      "epoch: 840\n",
      "batch training loss: 0.02029, ssr: 0.98238, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 330 epoch\n",
      "\n",
      "epoch: 841\n",
      "batch training loss: 0.01854, ssr: 0.98236, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 331 epoch\n",
      "\n",
      "epoch: 842\n",
      "batch training loss: 0.02208, ssr: 0.98234, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 332 epoch\n",
      "\n",
      "epoch: 843\n",
      "batch training loss: 0.02143, ssr: 0.98232, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 333 epoch\n",
      "\n",
      "epoch: 844\n",
      "batch training loss: 0.01988, ssr: 0.98229, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 334 epoch\n",
      "\n",
      "epoch: 845\n",
      "batch training loss: 0.01892, ssr: 0.98227, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 335 epoch\n",
      "\n",
      "epoch: 846\n",
      "batch training loss: 0.01916, ssr: 0.98225, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 336 epoch\n",
      "\n",
      "epoch: 847\n",
      "batch training loss: 0.02034, ssr: 0.98223, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 337 epoch\n",
      "\n",
      "epoch: 848\n",
      "batch training loss: 0.01895, ssr: 0.98221, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 338 epoch\n",
      "\n",
      "epoch: 849\n",
      "batch training loss: 0.02063, ssr: 0.98219, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 339 epoch\n",
      "\n",
      "epoch: 850\n",
      "batch training loss: 0.02022, ssr: 0.98217, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 340 epoch\n",
      "\n",
      "epoch: 851\n",
      "batch training loss: 0.01935, ssr: 0.98215, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 341 epoch\n",
      "\n",
      "epoch: 852\n",
      "batch training loss: 0.01933, ssr: 0.98213, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 342 epoch\n",
      "\n",
      "epoch: 853\n",
      "batch training loss: 0.02203, ssr: 0.98211, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 343 epoch\n",
      "\n",
      "epoch: 854\n",
      "batch training loss: 0.02030, ssr: 0.98208, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 344 epoch\n",
      "\n",
      "epoch: 855\n",
      "batch training loss: 0.01989, ssr: 0.98206, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 345 epoch\n",
      "\n",
      "epoch: 856\n",
      "batch training loss: 0.01974, ssr: 0.98204, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 346 epoch\n",
      "\n",
      "epoch: 857\n",
      "batch training loss: 0.02040, ssr: 0.98202, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 347 epoch\n",
      "\n",
      "epoch: 858\n",
      "batch training loss: 0.01986, ssr: 0.98200, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 348 epoch\n",
      "\n",
      "epoch: 859\n",
      "batch training loss: 0.02050, ssr: 0.98198, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 349 epoch\n",
      "\n",
      "epoch: 860\n",
      "batch training loss: 0.01991, ssr: 0.98196, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 350 epoch\n",
      "\n",
      "epoch: 861\n",
      "batch training loss: 0.02091, ssr: 0.98194, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 351 epoch\n",
      "\n",
      "epoch: 862\n",
      "batch training loss: 0.02104, ssr: 0.98192, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 352 epoch\n",
      "\n",
      "epoch: 863\n",
      "batch training loss: 0.01969, ssr: 0.98190, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 353 epoch\n",
      "\n",
      "epoch: 864\n",
      "batch training loss: 0.02046, ssr: 0.98187, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 354 epoch\n",
      "\n",
      "epoch: 865\n",
      "batch training loss: 0.02128, ssr: 0.98185, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 355 epoch\n",
      "\n",
      "epoch: 866\n",
      "batch training loss: 0.02079, ssr: 0.98183, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 356 epoch\n",
      "\n",
      "epoch: 867\n",
      "batch training loss: 0.01900, ssr: 0.98181, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 357 epoch\n",
      "\n",
      "epoch: 868\n",
      "batch training loss: 0.02035, ssr: 0.98179, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 358 epoch\n",
      "\n",
      "epoch: 869\n",
      "batch training loss: 0.01963, ssr: 0.98177, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 359 epoch\n",
      "\n",
      "epoch: 870\n",
      "batch training loss: 0.01957, ssr: 0.98175, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 360 epoch\n",
      "\n",
      "epoch: 871\n",
      "batch training loss: 0.01921, ssr: 0.98173, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 361 epoch\n",
      "\n",
      "epoch: 872\n",
      "batch training loss: 0.01959, ssr: 0.98171, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 362 epoch\n",
      "\n",
      "epoch: 873\n",
      "batch training loss: 0.01987, ssr: 0.98169, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 363 epoch\n",
      "\n",
      "epoch: 874\n",
      "batch training loss: 0.01909, ssr: 0.98166, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 364 epoch\n",
      "\n",
      "epoch: 875\n",
      "batch training loss: 0.01977, ssr: 0.98164, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 365 epoch\n",
      "\n",
      "epoch: 876\n",
      "batch training loss: 0.01966, ssr: 0.98162, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 366 epoch\n",
      "\n",
      "epoch: 877\n",
      "batch training loss: 0.01934, ssr: 0.98160, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 367 epoch\n",
      "\n",
      "epoch: 878\n",
      "batch training loss: 0.01908, ssr: 0.98158, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 368 epoch\n",
      "\n",
      "epoch: 879\n",
      "batch training loss: 0.02032, ssr: 0.98156, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 369 epoch\n",
      "\n",
      "epoch: 880\n",
      "batch training loss: 0.01939, ssr: 0.98154, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 370 epoch\n",
      "\n",
      "epoch: 881\n",
      "batch training loss: 0.01854, ssr: 0.98152, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 371 epoch\n",
      "\n",
      "epoch: 882\n",
      "batch training loss: 0.02197, ssr: 0.98150, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 372 epoch\n",
      "\n",
      "epoch: 883\n",
      "batch training loss: 0.01988, ssr: 0.98148, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 373 epoch\n",
      "\n",
      "epoch: 884\n",
      "batch training loss: 0.01932, ssr: 0.98145, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 374 epoch\n",
      "\n",
      "epoch: 885\n",
      "batch training loss: 0.01993, ssr: 0.98143, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 375 epoch\n",
      "\n",
      "epoch: 886\n",
      "batch training loss: 0.02123, ssr: 0.98141, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 376 epoch\n",
      "\n",
      "epoch: 887\n",
      "batch training loss: 0.01834, ssr: 0.98139, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 377 epoch\n",
      "\n",
      "epoch: 888\n",
      "batch training loss: 0.01809, ssr: 0.98137, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 378 epoch\n",
      "\n",
      "epoch: 889\n",
      "batch training loss: 0.02122, ssr: 0.98135, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 379 epoch\n",
      "\n",
      "epoch: 890\n",
      "batch training loss: 0.01815, ssr: 0.98133, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 380 epoch\n",
      "\n",
      "epoch: 891\n",
      "batch training loss: 0.02064, ssr: 0.98131, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 381 epoch\n",
      "\n",
      "epoch: 892\n",
      "batch training loss: 0.01978, ssr: 0.98129, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 382 epoch\n",
      "\n",
      "epoch: 893\n",
      "batch training loss: 0.02018, ssr: 0.98127, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 383 epoch\n",
      "\n",
      "epoch: 894\n",
      "batch training loss: 0.01941, ssr: 0.98124, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 384 epoch\n",
      "\n",
      "epoch: 895\n",
      "batch training loss: 0.01873, ssr: 0.98122, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 385 epoch\n",
      "\n",
      "epoch: 896\n",
      "batch training loss: 0.02070, ssr: 0.98120, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 386 epoch\n",
      "\n",
      "epoch: 897\n",
      "batch training loss: 0.01937, ssr: 0.98118, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 387 epoch\n",
      "\n",
      "epoch: 898\n",
      "batch training loss: 0.01845, ssr: 0.98116, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 388 epoch\n",
      "\n",
      "epoch: 899\n",
      "batch training loss: 0.01924, ssr: 0.98114, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 389 epoch\n",
      "\n",
      "epoch: 900\n",
      "batch training loss: 0.01831, ssr: 0.98112, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 390 epoch\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataset_train, dataset_eval, 'checkpoint.chk')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T02:45:04.131977Z",
     "start_time": "2024-08-14T02:45:02.843038Z"
    }
   },
   "source": [
    "dataset_test = cmip_dataset(X_test, true_test)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=configs.batch_size_test, shuffle=False)\n",
    "print(dataset_test.GetDataShape())"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cmip_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataset_test \u001B[38;5;241m=\u001B[39m \u001B[43mcmip_dataset\u001B[49m(X_test, true_test)\n\u001B[0;32m      3\u001B[0m dataloader_test \u001B[38;5;241m=\u001B[39m DataLoader(dataset_test, batch_size\u001B[38;5;241m=\u001B[39mconfigs\u001B[38;5;241m.\u001B[39mbatch_size_test, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataset_test\u001B[38;5;241m.\u001B[39mGetDataShape(),dataset_test)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'cmip_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk = torch.load('/media/aita-ocean/data/XGX_NEW/argo/cf/depth1/checkpoint.chk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.network.load_state_dict(chk['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test, test_pred, test_true = trainer.infer_test(dataset=dataset_test, dataloader=dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024893008172512054\n"
     ]
    }
   ],
   "source": [
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.7228, 0.7781, 0.7867,  ..., 0.6825, 0.7067, 0.7597],\n",
      "          [0.7021, 0.7360, 0.7465,  ..., 0.7015, 0.7428, 0.7890],\n",
      "          [0.7090, 0.7292, 0.7412,  ..., 0.7353, 0.7749, 0.7771],\n",
      "          ...,\n",
      "          [0.2445, 0.3037, 0.3320,  ..., 0.4725, 0.4556, 0.4470],\n",
      "          [0.2213, 0.2858, 0.3038,  ..., 0.4671, 0.4659, 0.4710],\n",
      "          [0.1922, 0.2362, 0.2250,  ..., 0.4668, 0.4748, 0.4830]]],\n",
      "\n",
      "\n",
      "        [[[0.7174, 0.7370, 0.7444,  ..., 0.8547, 0.8290, 0.8178],\n",
      "          [0.6974, 0.7022, 0.6970,  ..., 0.8264, 0.7967, 0.7854],\n",
      "          [0.6745, 0.6661, 0.6565,  ..., 0.8014, 0.7789, 0.7713],\n",
      "          ...,\n",
      "          [0.1105, 0.1539, 0.1827,  ..., 0.4269, 0.4298, 0.4393],\n",
      "          [0.0759, 0.1124, 0.1322,  ..., 0.4351, 0.4319, 0.4278],\n",
      "          [0.0569, 0.0884, 0.1039,  ..., 0.4398, 0.4309, 0.4103]]],\n",
      "\n",
      "\n",
      "        [[[0.7214, 0.7036, 0.6929,  ..., 0.9266, 0.8732, 0.8456],\n",
      "          [0.7257, 0.6918, 0.6792,  ..., 0.8920, 0.8455, 0.8317],\n",
      "          [0.7069, 0.6627, 0.6485,  ..., 0.8590, 0.8367, 0.8372],\n",
      "          ...,\n",
      "          [0.0972, 0.1329, 0.1598,  ..., 0.3868, 0.3902, 0.3767],\n",
      "          [0.0623, 0.1114, 0.1411,  ..., 0.3756, 0.3671, 0.3470],\n",
      "          [0.0462, 0.0972, 0.1223,  ..., 0.3608, 0.3375, 0.3115]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.2632, 0.2820, 0.2889,  ..., 0.2350, 0.2844, 0.2558],\n",
      "          [0.2488, 0.2686, 0.2854,  ..., 0.2770, 0.3470, 0.3436],\n",
      "          [0.2324, 0.2377, 0.2775,  ..., 0.3265, 0.3774, 0.3806],\n",
      "          ...,\n",
      "          [0.7606, 0.8060, 0.8287,  ..., 0.6348, 0.6433, 0.6519],\n",
      "          [0.8081, 0.8469, 0.8555,  ..., 0.6408, 0.6477, 0.6577],\n",
      "          [0.8514, 0.8676, 0.8417,  ..., 0.6453, 0.6540, 0.6692]]],\n",
      "\n",
      "\n",
      "        [[[0.4508, 0.4769, 0.4859,  ..., 0.3856, 0.3731, 0.3623],\n",
      "          [0.3783, 0.4357, 0.4698,  ..., 0.3887, 0.3860, 0.3871],\n",
      "          [0.3811, 0.4032, 0.4368,  ..., 0.3851, 0.4253, 0.4394],\n",
      "          ...,\n",
      "          [0.5644, 0.5698, 0.5811,  ..., 0.5460, 0.5502, 0.5505],\n",
      "          [0.5857, 0.5907, 0.6026,  ..., 0.5647, 0.5709, 0.5794],\n",
      "          [0.6118, 0.6107, 0.6309,  ..., 0.5766, 0.5841, 0.5979]]],\n",
      "\n",
      "\n",
      "        [[[0.5877, 0.5278, 0.4739,  ..., 0.6818, 0.7258, 0.7459],\n",
      "          [0.6165, 0.5776, 0.5249,  ..., 0.6973, 0.6965, 0.7048],\n",
      "          [0.6118, 0.5889, 0.5698,  ..., 0.6116, 0.5970, 0.5967],\n",
      "          ...,\n",
      "          [0.4166, 0.4156, 0.4126,  ..., 0.4900, 0.4758, 0.4777],\n",
      "          [0.4053, 0.4001, 0.3844,  ..., 0.4892, 0.4790, 0.4760],\n",
      "          [0.3858, 0.3595, 0.3192,  ..., 0.4564, 0.4555, 0.4594]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# print(test_pred)\n",
    "print(test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred.cpu().numpy()\n",
    "test_true = test_true.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = unscaler(np.array(test_pred),test_min,test_scale)\n",
    "test_true = unscaler(np.array(test_true),test_min,test_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'find_stack_level' from 'pandas.util._exceptions' (/home/aita-ocean/anaconda3/envs/pytorch/lib/python3.8/site-packages/pandas/util/_exceptions.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_17790/3222754748.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mxarray\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mxr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvtype\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdepth\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mds\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mxr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/BOA_Argo_annual.nc'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdecode_times\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mannual\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mvtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdepth\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m49\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m109\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m159\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m239\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/xarray/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtesting\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtutorial\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mufuncs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m from .backends.api import (\n\u001B[1;32m      3\u001B[0m     \u001B[0mload_dataarray\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mload_dataset\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mopen_dataarray\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/xarray/testing.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mxarray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mduck_array_ops\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformatting\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mxarray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataarray\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDataArray\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mxarray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/xarray/core/duck_array_ops.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mall\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0marray_all\u001B[0m  \u001B[0;31m# noqa\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0many\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0marray_any\u001B[0m  \u001B[0;31m# noqa\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pandas/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig_init\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m from pandas.core.api import (\n\u001B[0m\u001B[1;32m     52\u001B[0m     \u001B[0;31m# dtype\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m     \u001B[0mInt8Dtype\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pandas/core/api.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtypes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmissing\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0misna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0misnull\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnotna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnotnull\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malgorithms\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfactorize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0munique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue_counts\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marrays\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mCategorical\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marrays\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mboolean\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mBooleanDtype\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pandas/core/algorithms.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconstruction\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mextract_array\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 62\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindexers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mvalidate_indices\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mTYPE_CHECKING\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pandas/core/indexers/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m from pandas.core.indexers.utils import (\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mcheck_array_indexer\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mcheck_key_length\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mcheck_setitem_lengths\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mdeprecate_ndim_indexing\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pandas/core/indexers/utils.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_typing\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mAnyArrayLike\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_exceptions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfind_stack_level\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m from pandas.core.dtypes.common import (\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'find_stack_level' from 'pandas.util._exceptions' (/home/aita-ocean/anaconda3/envs/pytorch/lib/python3.8/site-packages/pandas/util/_exceptions.py)"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "def add(vtype,depth,data):\n",
    "    \n",
    "    ds=xr.open_dataset('/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/BOA_Argo_annual.nc',decode_times=False)\n",
    "    annual = ds[vtype].data[0,depth,49:109,159:239]\n",
    "    for i in range(0,data.shape[0]):\n",
    "        data[i,0,:,:] = data[i,0,:,:]+ annual\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = add('temp', 1, test_pred)\n",
    "test_true = add('temp', 1, test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/media/aita-ocean/data/XGX_NEW/argo/cf/depth1/data/test_pred.npy\",test_pred)\n",
    "np.save(\"/media/aita-ocean/data/XGX_NEW/argo/cf/depth1/data/test_true.npy\",test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[24.831146 24.82215  24.760616 ... 23.053291 23.12151  22.960583]\n",
      "   [25.422419 25.221653 25.158115 ... 23.570454 23.773727 23.84261 ]\n",
      "   [26.03207  25.828678 25.855268 ... 24.20781  24.276356 24.54636 ]\n",
      "   ...\n",
      "   [23.932186 23.945717 23.930984 ... 19.911354 19.984371 19.905792]\n",
      "   [23.052986 23.372112 23.253838 ... 19.434721 19.50663  19.48384 ]\n",
      "   [22.270794 22.400993 22.340464 ... 18.861683 19.029083 18.901756]]]\n",
      "\n",
      "\n",
      " [[[24.250736 24.320234 24.16678  ... 24.69166  24.437672 23.971155]\n",
      "   [24.65546  24.776817 24.3988   ... 25.192526 25.097076 25.156584]\n",
      "   [25.024347 24.992512 24.807245 ... 25.796003 25.705742 25.667938]\n",
      "   ...\n",
      "   [22.696402 22.951254 22.920082 ... 19.645567 19.572939 19.43991 ]\n",
      "   [22.249321 22.230875 22.23655  ... 19.151426 19.123842 19.010544]\n",
      "   [21.585955 21.287045 21.369219 ... 18.580114 18.621231 18.434664]]]\n",
      "\n",
      "\n",
      " [[[24.309141 24.301329 24.254578 ... 24.950348 24.740314 24.164265]\n",
      "   [24.657267 24.77455  24.635685 ... 25.422657 25.258049 25.338362]\n",
      "   [25.030615 25.184498 25.018658 ... 26.02629  25.83805  25.732496]\n",
      "   ...\n",
      "   [22.357872 22.367638 22.502699 ... 19.015701 19.08938  18.908056]\n",
      "   [22.020878 21.568298 21.924152 ... 18.45755  18.427866 18.480738]\n",
      "   [21.333397 20.913776 21.192442 ... 17.882158 17.935234 17.790085]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[20.206224 19.974388 19.907913 ... 19.639156 19.870289 19.699125]\n",
      "   [20.596582 20.51648  20.340809 ... 20.357811 20.71918  20.808672]\n",
      "   [20.94997  20.813139 20.97388  ... 21.2181   21.363749 21.531624]\n",
      "   ...\n",
      "   [28.77726  28.6534   28.568535 ... 21.68704  21.805239 21.765858]\n",
      "   [28.570711 28.427399 28.464859 ... 21.252726 21.343407 21.136599]\n",
      "   [27.763325 27.999035 27.99009  ... 20.692217 20.603498 20.563133]]]\n",
      "\n",
      "\n",
      " [[[21.981487 21.683147 21.62415  ... 20.551352 20.799181 20.726984]\n",
      "   [22.223625 22.118061 21.857273 ... 21.139868 21.581184 21.721838]\n",
      "   [22.536585 22.406649 22.324635 ... 21.994576 22.182646 22.423027]\n",
      "   ...\n",
      "   [26.898457 26.87293  27.052574 ... 20.573446 20.769598 20.715801]\n",
      "   [26.53937  26.476244 26.608017 ... 20.306149 20.468071 20.369524]\n",
      "   [25.970867 25.922672 26.48183  ... 19.894419 19.915274 19.962929]]]\n",
      "\n",
      "\n",
      " [[[22.999117 22.596878 22.264856 ... 22.795485 22.765438 22.598131]\n",
      "   [23.414543 23.119734 22.685844 ... 23.24606  23.416378 23.495129]\n",
      "   [23.88386  23.636744 23.582607 ... 23.67528  23.744438 23.958683]\n",
      "   ...\n",
      "   [24.918335 24.934673 24.388561 ... 20.014462 20.009726 19.850843]\n",
      "   [24.170324 24.145424 23.778915 ... 19.645426 19.64401  19.545246]\n",
      "   [23.221323 23.589285 23.382515 ... 19.00912  19.104353 19.075098]]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[24.3787 24.697  24.6267 ... 23.407  23.7284 24.2155]\n",
      "   [24.6471 24.7984 24.7353 ... 24.2561 24.7275 25.1566]\n",
      "   [25.2197 25.3035 25.2542 ... 25.1214 25.5444 25.587 ]\n",
      "   ...\n",
      "   [23.5988 24.087  24.2293 ... 19.8717 19.7274 19.5949]\n",
      "   [22.7709 23.3265 23.4223 ... 19.3198 19.2733 19.2527]\n",
      "   [21.8666 22.2601 22.1415 ... 18.7913 18.7537 18.7393]]]\n",
      "\n",
      "\n",
      " [[[24.3277 24.3122 24.23   ... 25.0199 24.8735 24.76  ]\n",
      "   [24.6028 24.4824 24.2722 ... 25.4253 25.2318 25.1227]\n",
      "   [24.897  24.7116 24.4608 ... 25.7411 25.582  25.5328]\n",
      "   ...\n",
      "   [22.3434 22.6838 22.8307 ... 19.4446 19.4853 19.5221]\n",
      "   [21.4089 21.7027 21.8145 ... 19.0198 18.9544 18.8481]\n",
      "   [20.599  20.8757 21.0069 ... 18.5386 18.3432 18.0581]]]\n",
      "\n",
      "\n",
      " [[[24.3651 23.9992 23.7482 ... 25.6926 25.2881 25.0205]\n",
      "   [24.8676 24.385  24.1053 ... 26.0398 25.6895 25.5561]\n",
      "   [25.2008 24.68   24.3857 ... 26.2805 26.1233 26.1499]\n",
      "   ...\n",
      "   [22.2188 22.4874 22.6163 ... 19.0683 19.1147 18.9364]\n",
      "   [21.2817 21.6927 21.898  ... 18.4632 18.3477 18.0918]\n",
      "   [20.4989 20.9587 21.179  ... 17.7981 17.4683 17.1327]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[20.074  20.0499 19.9639 ... 19.2148 19.7729 19.4954]\n",
      "   [20.4008 20.4207 20.4167 ... 20.2792 21.0205 20.9847]\n",
      "   [20.7559 20.699  20.911  ... 21.2924 21.8217 21.8732]\n",
      "   ...\n",
      "   [28.4325 28.7918 28.881  ... 21.3912 21.4847 21.5137]\n",
      "   [28.2667 28.5819 28.5898 ... 20.9467 20.9761 21.0017]\n",
      "   [28.0416 28.1748 27.9183 ... 20.4634 20.4324 20.4837]]]\n",
      "\n",
      "\n",
      " [[[21.8309 21.8756 21.8088 ... 20.626  20.603  20.4933]\n",
      "   [21.6142 21.9863 22.1441 ... 21.3261 21.3854 21.3916]\n",
      "   [22.1487 22.2493 22.4027 ... 21.8415 22.2702 22.4242]\n",
      "   ...\n",
      "   [26.5944 26.579  26.5625 ... 20.5594 20.6132 20.5644]\n",
      "   [26.1843 26.1827 26.2212 ... 20.2343 20.2567 20.2682]\n",
      "   [25.7972 25.7685 25.9432 ... 19.8196 19.7781 19.8152]]]\n",
      "\n",
      "\n",
      " [[[23.1134 22.3524 21.697  ... 23.3998 23.9069 24.0862]\n",
      "   [23.8451 23.3147 22.6598 ... 24.216  24.2937 24.3682]\n",
      "   [24.3095 23.9887 23.6487 ... 23.9633 23.8788 23.8973]\n",
      "   ...\n",
      "   [25.2103 25.1346 24.984  ... 20.0354 19.9162 19.8821]\n",
      "   [24.4938 24.3977 24.1766 ... 19.5271 19.3958 19.2995]\n",
      "   [23.6802 23.4156 23.0234 ... 18.6938 18.5729 18.5179]]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(vtype, depth, test_pred, test_true):\n",
    "    test_preds = np.array(test_pred, copy=True)\n",
    "    test_trues = np.array(test_true, copy=True)\n",
    "\n",
    "    test_preds = np.squeeze(test_preds)\n",
    "    test_trues = np.squeeze(test_trues)\n",
    "\n",
    "    test_preds[np.isnan(test_preds)] = 0\n",
    "    test_trues[np.isnan(test_trues)] = 0\n",
    "    mask = np.load('/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/' + vtype + '_' + str(depth) + '_ano.npy')\n",
    "    #     mask = np.squeeze(mask)\n",
    "    mask = mask[0]\n",
    "\n",
    "    total = mask.shape[0] * mask.shape[1]\n",
    "    total_nan = len(mask[np.isnan(mask)])\n",
    "    total_real = total - total_nan\n",
    "    #     print('Total NaN:',total_nan)#统计数据中的nan值\n",
    "    #     print('Total Real:',total_real)#统计数据中的nan值\n",
    "    #     #nan：0 values ：1\n",
    "    mask[~np.isnan(mask)] = 1\n",
    "    mask[np.isnan(mask)] = 0\n",
    "    rmse = []\n",
    "    rmse_temp = []\n",
    "    nrmse = []\n",
    "    nrmse_temp = []\n",
    "    mae = []\n",
    "    mae_temp = []\n",
    "    for i in range(0, test_preds.shape[0]):\n",
    "        final_temp = mask * test_preds[i]\n",
    "        test_temp = mask * test_trues[i]\n",
    "        # np.sum((y_actual - y_predicted) ** 2)\n",
    "        sse = np.sum((test_temp - final_temp) ** 2)\n",
    "        mse_temp = sse / total_real\n",
    "        rmse_temp = np.sqrt(mse_temp)\n",
    "        nrmse_temp = rmse_temp / np.mean(test_temp)\n",
    "        rmse.append(rmse_temp)\n",
    "        nrmse.append(nrmse_temp)\n",
    "        mae_temp = mean_absolute_error(test_temp, final_temp) * total / total_real\n",
    "\n",
    "        mae.append(mae_temp)\n",
    "    #     print('NAN:',len(test_pred[np.isnan(test_pred)]))\n",
    "    #     print('TEST NANMIN',np.nanmin(test_pred))\n",
    "    #     print('TEST MIN',test_pred.min())\n",
    "    # print(str(depth)+'层')\n",
    "    RMSE = np.sum(rmse) / len(rmse)\n",
    "    MAE = np.sum(mae) / len(mae)\n",
    "    NRMSE = np.sum(nrmse) / len(nrmse)\n",
    "    # NRMSE = nrmse\n",
    "    print(str(depth) + '层:' + 'NRMSE RESULT:\\n', NRMSE)\n",
    "\n",
    "    #     print('MAE RESULT:\\n',MAE)\n",
    "\n",
    "    return NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 60, 80, 1)\n",
      "(12, 60, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_pred.shape)\n",
    "print(test_true.shape)\n",
    "12,80,1,60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred.transpose(0,2,3,1)\n",
    "test_true = test_true.transpose(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[24.831146]\n",
      "   [24.82215 ]\n",
      "   [24.760616]\n",
      "   ...\n",
      "   [23.053291]\n",
      "   [23.12151 ]\n",
      "   [22.960583]]\n",
      "\n",
      "  [[25.422419]\n",
      "   [25.221653]\n",
      "   [25.158115]\n",
      "   ...\n",
      "   [23.570454]\n",
      "   [23.773727]\n",
      "   [23.84261 ]]\n",
      "\n",
      "  [[26.03207 ]\n",
      "   [25.828678]\n",
      "   [25.855268]\n",
      "   ...\n",
      "   [24.20781 ]\n",
      "   [24.276356]\n",
      "   [24.54636 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[23.932186]\n",
      "   [23.945717]\n",
      "   [23.930984]\n",
      "   ...\n",
      "   [19.911354]\n",
      "   [19.984371]\n",
      "   [19.905792]]\n",
      "\n",
      "  [[23.052986]\n",
      "   [23.372112]\n",
      "   [23.253838]\n",
      "   ...\n",
      "   [19.434721]\n",
      "   [19.50663 ]\n",
      "   [19.48384 ]]\n",
      "\n",
      "  [[22.270794]\n",
      "   [22.400993]\n",
      "   [22.340464]\n",
      "   ...\n",
      "   [18.861683]\n",
      "   [19.029083]\n",
      "   [18.901756]]]\n",
      "\n",
      "\n",
      " [[[24.250736]\n",
      "   [24.320234]\n",
      "   [24.16678 ]\n",
      "   ...\n",
      "   [24.69166 ]\n",
      "   [24.437672]\n",
      "   [23.971155]]\n",
      "\n",
      "  [[24.65546 ]\n",
      "   [24.776817]\n",
      "   [24.3988  ]\n",
      "   ...\n",
      "   [25.192526]\n",
      "   [25.097076]\n",
      "   [25.156584]]\n",
      "\n",
      "  [[25.024347]\n",
      "   [24.992512]\n",
      "   [24.807245]\n",
      "   ...\n",
      "   [25.796003]\n",
      "   [25.705742]\n",
      "   [25.667938]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[22.696402]\n",
      "   [22.951254]\n",
      "   [22.920082]\n",
      "   ...\n",
      "   [19.645567]\n",
      "   [19.572939]\n",
      "   [19.43991 ]]\n",
      "\n",
      "  [[22.249321]\n",
      "   [22.230875]\n",
      "   [22.23655 ]\n",
      "   ...\n",
      "   [19.151426]\n",
      "   [19.123842]\n",
      "   [19.010544]]\n",
      "\n",
      "  [[21.585955]\n",
      "   [21.287045]\n",
      "   [21.369219]\n",
      "   ...\n",
      "   [18.580114]\n",
      "   [18.621231]\n",
      "   [18.434664]]]\n",
      "\n",
      "\n",
      " [[[24.309141]\n",
      "   [24.301329]\n",
      "   [24.254578]\n",
      "   ...\n",
      "   [24.950348]\n",
      "   [24.740314]\n",
      "   [24.164265]]\n",
      "\n",
      "  [[24.657267]\n",
      "   [24.77455 ]\n",
      "   [24.635685]\n",
      "   ...\n",
      "   [25.422657]\n",
      "   [25.258049]\n",
      "   [25.338362]]\n",
      "\n",
      "  [[25.030615]\n",
      "   [25.184498]\n",
      "   [25.018658]\n",
      "   ...\n",
      "   [26.02629 ]\n",
      "   [25.83805 ]\n",
      "   [25.732496]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[22.357872]\n",
      "   [22.367638]\n",
      "   [22.502699]\n",
      "   ...\n",
      "   [19.015701]\n",
      "   [19.08938 ]\n",
      "   [18.908056]]\n",
      "\n",
      "  [[22.020878]\n",
      "   [21.568298]\n",
      "   [21.924152]\n",
      "   ...\n",
      "   [18.45755 ]\n",
      "   [18.427866]\n",
      "   [18.480738]]\n",
      "\n",
      "  [[21.333397]\n",
      "   [20.913776]\n",
      "   [21.192442]\n",
      "   ...\n",
      "   [17.882158]\n",
      "   [17.935234]\n",
      "   [17.790085]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[20.206224]\n",
      "   [19.974388]\n",
      "   [19.907913]\n",
      "   ...\n",
      "   [19.639156]\n",
      "   [19.870289]\n",
      "   [19.699125]]\n",
      "\n",
      "  [[20.596582]\n",
      "   [20.51648 ]\n",
      "   [20.340809]\n",
      "   ...\n",
      "   [20.357811]\n",
      "   [20.71918 ]\n",
      "   [20.808672]]\n",
      "\n",
      "  [[20.94997 ]\n",
      "   [20.813139]\n",
      "   [20.97388 ]\n",
      "   ...\n",
      "   [21.2181  ]\n",
      "   [21.363749]\n",
      "   [21.531624]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[28.77726 ]\n",
      "   [28.6534  ]\n",
      "   [28.568535]\n",
      "   ...\n",
      "   [21.68704 ]\n",
      "   [21.805239]\n",
      "   [21.765858]]\n",
      "\n",
      "  [[28.570711]\n",
      "   [28.427399]\n",
      "   [28.464859]\n",
      "   ...\n",
      "   [21.252726]\n",
      "   [21.343407]\n",
      "   [21.136599]]\n",
      "\n",
      "  [[27.763325]\n",
      "   [27.999035]\n",
      "   [27.99009 ]\n",
      "   ...\n",
      "   [20.692217]\n",
      "   [20.603498]\n",
      "   [20.563133]]]\n",
      "\n",
      "\n",
      " [[[21.981487]\n",
      "   [21.683147]\n",
      "   [21.62415 ]\n",
      "   ...\n",
      "   [20.551352]\n",
      "   [20.799181]\n",
      "   [20.726984]]\n",
      "\n",
      "  [[22.223625]\n",
      "   [22.118061]\n",
      "   [21.857273]\n",
      "   ...\n",
      "   [21.139868]\n",
      "   [21.581184]\n",
      "   [21.721838]]\n",
      "\n",
      "  [[22.536585]\n",
      "   [22.406649]\n",
      "   [22.324635]\n",
      "   ...\n",
      "   [21.994576]\n",
      "   [22.182646]\n",
      "   [22.423027]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[26.898457]\n",
      "   [26.87293 ]\n",
      "   [27.052574]\n",
      "   ...\n",
      "   [20.573446]\n",
      "   [20.769598]\n",
      "   [20.715801]]\n",
      "\n",
      "  [[26.53937 ]\n",
      "   [26.476244]\n",
      "   [26.608017]\n",
      "   ...\n",
      "   [20.306149]\n",
      "   [20.468071]\n",
      "   [20.369524]]\n",
      "\n",
      "  [[25.970867]\n",
      "   [25.922672]\n",
      "   [26.48183 ]\n",
      "   ...\n",
      "   [19.894419]\n",
      "   [19.915274]\n",
      "   [19.962929]]]\n",
      "\n",
      "\n",
      " [[[22.999117]\n",
      "   [22.596878]\n",
      "   [22.264856]\n",
      "   ...\n",
      "   [22.795485]\n",
      "   [22.765438]\n",
      "   [22.598131]]\n",
      "\n",
      "  [[23.414543]\n",
      "   [23.119734]\n",
      "   [22.685844]\n",
      "   ...\n",
      "   [23.24606 ]\n",
      "   [23.416378]\n",
      "   [23.495129]]\n",
      "\n",
      "  [[23.88386 ]\n",
      "   [23.636744]\n",
      "   [23.582607]\n",
      "   ...\n",
      "   [23.67528 ]\n",
      "   [23.744438]\n",
      "   [23.958683]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[24.918335]\n",
      "   [24.934673]\n",
      "   [24.388561]\n",
      "   ...\n",
      "   [20.014462]\n",
      "   [20.009726]\n",
      "   [19.850843]]\n",
      "\n",
      "  [[24.170324]\n",
      "   [24.145424]\n",
      "   [23.778915]\n",
      "   ...\n",
      "   [19.645426]\n",
      "   [19.64401 ]\n",
      "   [19.545246]]\n",
      "\n",
      "  [[23.221323]\n",
      "   [23.589285]\n",
      "   [23.382515]\n",
      "   ...\n",
      "   [19.00912 ]\n",
      "   [19.104353]\n",
      "   [19.075098]]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[24.3787]\n",
      "   [24.697 ]\n",
      "   [24.6267]\n",
      "   ...\n",
      "   [23.407 ]\n",
      "   [23.7284]\n",
      "   [24.2155]]\n",
      "\n",
      "  [[24.6471]\n",
      "   [24.7984]\n",
      "   [24.7353]\n",
      "   ...\n",
      "   [24.2561]\n",
      "   [24.7275]\n",
      "   [25.1566]]\n",
      "\n",
      "  [[25.2197]\n",
      "   [25.3035]\n",
      "   [25.2542]\n",
      "   ...\n",
      "   [25.1214]\n",
      "   [25.5444]\n",
      "   [25.587 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[23.5988]\n",
      "   [24.087 ]\n",
      "   [24.2293]\n",
      "   ...\n",
      "   [19.8717]\n",
      "   [19.7274]\n",
      "   [19.5949]]\n",
      "\n",
      "  [[22.7709]\n",
      "   [23.3265]\n",
      "   [23.4223]\n",
      "   ...\n",
      "   [19.3198]\n",
      "   [19.2733]\n",
      "   [19.2527]]\n",
      "\n",
      "  [[21.8666]\n",
      "   [22.2601]\n",
      "   [22.1415]\n",
      "   ...\n",
      "   [18.7913]\n",
      "   [18.7537]\n",
      "   [18.7393]]]\n",
      "\n",
      "\n",
      " [[[24.3277]\n",
      "   [24.3122]\n",
      "   [24.23  ]\n",
      "   ...\n",
      "   [25.0199]\n",
      "   [24.8735]\n",
      "   [24.76  ]]\n",
      "\n",
      "  [[24.6028]\n",
      "   [24.4824]\n",
      "   [24.2722]\n",
      "   ...\n",
      "   [25.4253]\n",
      "   [25.2318]\n",
      "   [25.1227]]\n",
      "\n",
      "  [[24.897 ]\n",
      "   [24.7116]\n",
      "   [24.4608]\n",
      "   ...\n",
      "   [25.7411]\n",
      "   [25.582 ]\n",
      "   [25.5328]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[22.3434]\n",
      "   [22.6838]\n",
      "   [22.8307]\n",
      "   ...\n",
      "   [19.4446]\n",
      "   [19.4853]\n",
      "   [19.5221]]\n",
      "\n",
      "  [[21.4089]\n",
      "   [21.7027]\n",
      "   [21.8145]\n",
      "   ...\n",
      "   [19.0198]\n",
      "   [18.9544]\n",
      "   [18.8481]]\n",
      "\n",
      "  [[20.599 ]\n",
      "   [20.8757]\n",
      "   [21.0069]\n",
      "   ...\n",
      "   [18.5386]\n",
      "   [18.3432]\n",
      "   [18.0581]]]\n",
      "\n",
      "\n",
      " [[[24.3651]\n",
      "   [23.9992]\n",
      "   [23.7482]\n",
      "   ...\n",
      "   [25.6926]\n",
      "   [25.2881]\n",
      "   [25.0205]]\n",
      "\n",
      "  [[24.8676]\n",
      "   [24.385 ]\n",
      "   [24.1053]\n",
      "   ...\n",
      "   [26.0398]\n",
      "   [25.6895]\n",
      "   [25.5561]]\n",
      "\n",
      "  [[25.2008]\n",
      "   [24.68  ]\n",
      "   [24.3857]\n",
      "   ...\n",
      "   [26.2805]\n",
      "   [26.1233]\n",
      "   [26.1499]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[22.2188]\n",
      "   [22.4874]\n",
      "   [22.6163]\n",
      "   ...\n",
      "   [19.0683]\n",
      "   [19.1147]\n",
      "   [18.9364]]\n",
      "\n",
      "  [[21.2817]\n",
      "   [21.6927]\n",
      "   [21.898 ]\n",
      "   ...\n",
      "   [18.4632]\n",
      "   [18.3477]\n",
      "   [18.0918]]\n",
      "\n",
      "  [[20.4989]\n",
      "   [20.9587]\n",
      "   [21.179 ]\n",
      "   ...\n",
      "   [17.7981]\n",
      "   [17.4683]\n",
      "   [17.1327]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[20.074 ]\n",
      "   [20.0499]\n",
      "   [19.9639]\n",
      "   ...\n",
      "   [19.2148]\n",
      "   [19.7729]\n",
      "   [19.4954]]\n",
      "\n",
      "  [[20.4008]\n",
      "   [20.4207]\n",
      "   [20.4167]\n",
      "   ...\n",
      "   [20.2792]\n",
      "   [21.0205]\n",
      "   [20.9847]]\n",
      "\n",
      "  [[20.7559]\n",
      "   [20.699 ]\n",
      "   [20.911 ]\n",
      "   ...\n",
      "   [21.2924]\n",
      "   [21.8217]\n",
      "   [21.8732]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[28.4325]\n",
      "   [28.7918]\n",
      "   [28.881 ]\n",
      "   ...\n",
      "   [21.3912]\n",
      "   [21.4847]\n",
      "   [21.5137]]\n",
      "\n",
      "  [[28.2667]\n",
      "   [28.5819]\n",
      "   [28.5898]\n",
      "   ...\n",
      "   [20.9467]\n",
      "   [20.9761]\n",
      "   [21.0017]]\n",
      "\n",
      "  [[28.0416]\n",
      "   [28.1748]\n",
      "   [27.9183]\n",
      "   ...\n",
      "   [20.4634]\n",
      "   [20.4324]\n",
      "   [20.4837]]]\n",
      "\n",
      "\n",
      " [[[21.8309]\n",
      "   [21.8756]\n",
      "   [21.8088]\n",
      "   ...\n",
      "   [20.626 ]\n",
      "   [20.603 ]\n",
      "   [20.4933]]\n",
      "\n",
      "  [[21.6142]\n",
      "   [21.9863]\n",
      "   [22.1441]\n",
      "   ...\n",
      "   [21.3261]\n",
      "   [21.3854]\n",
      "   [21.3916]]\n",
      "\n",
      "  [[22.1487]\n",
      "   [22.2493]\n",
      "   [22.4027]\n",
      "   ...\n",
      "   [21.8415]\n",
      "   [22.2702]\n",
      "   [22.4242]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[26.5944]\n",
      "   [26.579 ]\n",
      "   [26.5625]\n",
      "   ...\n",
      "   [20.5594]\n",
      "   [20.6132]\n",
      "   [20.5644]]\n",
      "\n",
      "  [[26.1843]\n",
      "   [26.1827]\n",
      "   [26.2212]\n",
      "   ...\n",
      "   [20.2343]\n",
      "   [20.2567]\n",
      "   [20.2682]]\n",
      "\n",
      "  [[25.7972]\n",
      "   [25.7685]\n",
      "   [25.9432]\n",
      "   ...\n",
      "   [19.8196]\n",
      "   [19.7781]\n",
      "   [19.8152]]]\n",
      "\n",
      "\n",
      " [[[23.1134]\n",
      "   [22.3524]\n",
      "   [21.697 ]\n",
      "   ...\n",
      "   [23.3998]\n",
      "   [23.9069]\n",
      "   [24.0862]]\n",
      "\n",
      "  [[23.8451]\n",
      "   [23.3147]\n",
      "   [22.6598]\n",
      "   ...\n",
      "   [24.216 ]\n",
      "   [24.2937]\n",
      "   [24.3682]]\n",
      "\n",
      "  [[24.3095]\n",
      "   [23.9887]\n",
      "   [23.6487]\n",
      "   ...\n",
      "   [23.9633]\n",
      "   [23.8788]\n",
      "   [23.8973]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[25.2103]\n",
      "   [25.1346]\n",
      "   [24.984 ]\n",
      "   ...\n",
      "   [20.0354]\n",
      "   [19.9162]\n",
      "   [19.8821]]\n",
      "\n",
      "  [[24.4938]\n",
      "   [24.3977]\n",
      "   [24.1766]\n",
      "   ...\n",
      "   [19.5271]\n",
      "   [19.3958]\n",
      "   [19.2995]]\n",
      "\n",
      "  [[23.6802]\n",
      "   [23.4156]\n",
      "   [23.0234]\n",
      "   ...\n",
      "   [18.6938]\n",
      "   [18.5729]\n",
      "   [18.5179]]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.squeeze(test_pred)\n",
    "test_true = np.squeeze(test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cha = (test_true[0] - test_pred[0]) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred[np.isnan(test_pred)] = 0\n",
    "test_true[np.isnan(test_true)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "371.09576\n",
      "4800\n",
      "265.58588\n",
      "4800\n",
      "210.63751\n",
      "4800\n",
      "245.75691\n",
      "4800\n",
      "221.24115\n",
      "4800\n",
      "254.67433\n",
      "4800\n",
      "284.3286\n",
      "4800\n",
      "253.85689\n",
      "4800\n",
      "260.96286\n",
      "4800\n",
      "263.95312\n",
      "4800\n",
      "313.80795\n",
      "4800\n",
      "196.68466\n"
     ]
    }
   ],
   "source": [
    "rmse = []\n",
    "corr = []\n",
    "for i in range(test_pred.shape[0]):\n",
    "    predict_result = test_pred[i]\n",
    "    #print(predict_result)\n",
    "    true_result = test_true[i]\n",
    "    total = predict_result.shape[0] * predict_result.shape[1] \n",
    "    print(total)\n",
    "    sse = np.sum((true_result - predict_result) ** 2)\n",
    "    print(sse)\n",
    "    rmse_temp = np.sqrt(sse / total)\n",
    "    '''\n",
    "    if i == 0:\n",
    "        print(total)\n",
    "        print(sse)\n",
    "        print(rmse_temp)\n",
    "    '''\n",
    "    #print( np.sum(rmse_temp) / len(rmse_temp))\n",
    "    rmse.append(rmse_temp)\n",
    "    \n",
    "    predict_result_f = predict_result.flatten()\n",
    "    true_result_f = true_result.flatten()\n",
    "    corr_temp = np.corrcoef(predict_result_f, true_result_f)[0, -1]\n",
    "    corr.append(corr_temp)\n",
    "RMSE = np.sum(rmse) / len(rmse)\n",
    "CORR = np.sum(corr) / len(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23276634998727241\n",
      "0.9971275964146765\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "print(RMSE)\n",
    "print(CORR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24.831146 24.82215  24.760616 ... 23.053291 23.12151  22.960583]\n",
      " [25.422419 25.221653 25.158115 ... 23.570454 23.773727 23.84261 ]\n",
      " [26.03207  25.828678 25.855268 ... 24.20781  24.276356 24.54636 ]\n",
      " ...\n",
      " [23.932186 23.945717 23.930984 ... 19.911354 19.984371 19.905792]\n",
      " [23.052986 23.372112 23.253838 ... 19.434721 19.50663  19.48384 ]\n",
      " [22.270794 22.400993 22.340464 ... 18.861683 19.029083 18.901756]]\n"
     ]
    }
   ],
   "source": [
    "test_pred.shape\n",
    "print(test_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24.3787 24.697  24.6267 ... 23.407  23.7284 24.2155]\n",
      " [24.6471 24.7984 24.7353 ... 24.2561 24.7275 25.1566]\n",
      " [25.2197 25.3035 25.2542 ... 25.1214 25.5444 25.587 ]\n",
      " ...\n",
      " [23.5988 24.087  24.2293 ... 19.8717 19.7274 19.5949]\n",
      " [22.7709 23.3265 23.4223 ... 19.3198 19.2733 19.2527]\n",
      " [21.8666 22.2601 22.1415 ... 18.7913 18.7537 18.7393]]\n"
     ]
    }
   ],
   "source": [
    "print(test_true[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1层:NRMSE RESULT:\n",
      " 0.0088005597923783\n"
     ]
    }
   ],
   "source": [
    "nrmse = loss('temp', 1, test_pred, test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0088005597923783\n"
     ]
    }
   ],
   "source": [
    "print(nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(vtype,depth,test_pred,test_true):\n",
    "    test_preds = np.array(test_pred,copy=True)\n",
    "    test_trues = np.array(test_true,copy=True)\n",
    "    \n",
    "    test_preds = np.squeeze(test_preds)\n",
    "    test_trues = np.squeeze(test_trues)\n",
    "\n",
    "    test_preds[np.isnan(test_preds)] = 0\n",
    "    test_trues[np.isnan(test_trues)] = 0\n",
    "    mask = np.load('/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/'+vtype+'_'+str(depth)+'_ano.npy')\n",
    "#     mask = np.squeeze(mask)\n",
    "    mask = mask[0]\n",
    "\n",
    "    total = mask.shape[0]* mask.shape[1]\n",
    "    total_nan = len(mask[np.isnan(mask)])\n",
    "    total_real = total - total_nan\n",
    "#     print('Total NaN:',total_nan)#统计数据中的nan值\n",
    "#     print('Total Real:',total_real)#统计数据中的nan值\n",
    "#     #nan：0 values ：1\n",
    "    mask[~np.isnan(mask)] = 1\n",
    "    mask[np.isnan(mask)] = 0\n",
    "    CORR = []\n",
    "    corr = []\n",
    "    corr_temp = []\n",
    "    for i in range(0,test_preds.shape[0]):\n",
    "\n",
    "        final_temp = mask * test_preds[i]\n",
    "        final_temp_f = final_temp.flatten()\n",
    "        test_temp = mask * test_trues[i]\n",
    "        test_temp_f = test_temp.flatten()\n",
    "        corr_temp = np.corrcoef(final_temp_f,test_temp_f)[0,-1]\n",
    "        # print(corr_temp)\n",
    "        corr.append(corr_temp)\n",
    "#     print('NAN:',len(test_pred[np.isnan(test_pred)]))\n",
    "#     print('TEST NANMIN',np.nanmin(test_pred))\n",
    "#     print('TEST MIN',test_pred.min())\n",
    "    # print(str(depth)+'层')\n",
    "    CORR = np.sum(corr)/len(corr)\n",
    "    # CORR = corr\n",
    "    print(str(depth)+'层:'+'CORR RESULT:\\n',CORR)\n",
    "\n",
    "#     print('MAE RESULT:\\n',MAE)\n",
    "\n",
    "    return CORR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1层:CORR RESULT:\n",
      " 0.9972644330212276\n"
     ]
    }
   ],
   "source": [
    "cor = corr('temp', 1, test_pred, test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1, 60, 80)\n",
      "[[[[24.3787 24.697  24.6267 ... 23.407  23.7284 24.2155]\n",
      "   [24.6471 24.7984 24.7353 ... 24.2561 24.7275 25.1566]\n",
      "   [25.2197 25.3035 25.2542 ... 25.1214 25.5444 25.587 ]\n",
      "   ...\n",
      "   [23.5988 24.087  24.2293 ... 19.8717 19.7274 19.5949]\n",
      "   [22.7709 23.3265 23.4223 ... 19.3198 19.2733 19.2527]\n",
      "   [21.8666 22.2601 22.1415 ... 18.7913 18.7537 18.7393]]]\n",
      "\n",
      "\n",
      " [[[24.3277 24.3122 24.23   ... 25.0199 24.8735 24.76  ]\n",
      "   [24.6028 24.4824 24.2722 ... 25.4253 25.2318 25.1227]\n",
      "   [24.897  24.7116 24.4608 ... 25.7411 25.582  25.5328]\n",
      "   ...\n",
      "   [22.3434 22.6838 22.8307 ... 19.4446 19.4853 19.5221]\n",
      "   [21.4089 21.7027 21.8145 ... 19.0198 18.9544 18.8481]\n",
      "   [20.599  20.8757 21.0069 ... 18.5386 18.3432 18.0581]]]\n",
      "\n",
      "\n",
      " [[[24.3651 23.9992 23.7482 ... 25.6926 25.2881 25.0205]\n",
      "   [24.8676 24.385  24.1053 ... 26.0398 25.6895 25.5561]\n",
      "   [25.2008 24.68   24.3857 ... 26.2805 26.1233 26.1499]\n",
      "   ...\n",
      "   [22.2188 22.4874 22.6163 ... 19.0683 19.1147 18.9364]\n",
      "   [21.2817 21.6927 21.898  ... 18.4632 18.3477 18.0918]\n",
      "   [20.4989 20.9587 21.179  ... 17.7981 17.4683 17.1327]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[20.074  20.0499 19.9639 ... 19.2148 19.7729 19.4954]\n",
      "   [20.4008 20.4207 20.4167 ... 20.2792 21.0205 20.9847]\n",
      "   [20.7559 20.699  20.911  ... 21.2924 21.8217 21.8732]\n",
      "   ...\n",
      "   [28.4325 28.7918 28.881  ... 21.3912 21.4847 21.5137]\n",
      "   [28.2667 28.5819 28.5898 ... 20.9467 20.9761 21.0017]\n",
      "   [28.0416 28.1748 27.9183 ... 20.4634 20.4324 20.4837]]]\n",
      "\n",
      "\n",
      " [[[21.8309 21.8756 21.8088 ... 20.626  20.603  20.4933]\n",
      "   [21.6142 21.9863 22.1441 ... 21.3261 21.3854 21.3916]\n",
      "   [22.1487 22.2493 22.4027 ... 21.8415 22.2702 22.4242]\n",
      "   ...\n",
      "   [26.5944 26.579  26.5625 ... 20.5594 20.6132 20.5644]\n",
      "   [26.1843 26.1827 26.2212 ... 20.2343 20.2567 20.2682]\n",
      "   [25.7972 25.7685 25.9432 ... 19.8196 19.7781 19.8152]]]\n",
      "\n",
      "\n",
      " [[[23.1134 22.3524 21.697  ... 23.3998 23.9069 24.0862]\n",
      "   [23.8451 23.3147 22.6598 ... 24.216  24.2937 24.3682]\n",
      "   [24.3095 23.9887 23.6487 ... 23.9633 23.8788 23.8973]\n",
      "   ...\n",
      "   [25.2103 25.1346 24.984  ... 20.0354 19.9162 19.8821]\n",
      "   [24.4938 24.3977 24.1766 ... 19.5271 19.3958 19.2995]\n",
      "   [23.6802 23.4156 23.0234 ... 18.6938 18.5729 18.5179]]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_true.shape)\n",
    "print(test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "nan\n",
      "4800\n",
      "nan\n",
      "4800\n",
      "nan\n",
      "4800\n",
      "nan\n",
      "4800\n",
      "nan\n",
      "4800\n",
      "nan\n",
      "4800\n",
      "nan\n",
      "4800\n",
      "nan\n",
      "4800\n",
      "nan\n",
      "4800\n",
      "nan\n",
      "4800\n",
      "nan\n",
      "4800\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "rmse = []\n",
    "corr = []\n",
    "for i in range(test_pred.shape[0]):\n",
    "    predict_result = test_pred[i][0]\n",
    "    #print(predict_result)\n",
    "    true_result = test_true[i][0]\n",
    "    total = predict_result.shape[0] * predict_result.shape[1] \n",
    "    print(total)\n",
    "    sse = np.sum((true_result - predict_result) ** 2)\n",
    "    print(sse)\n",
    "    rmse_temp = np.sqrt(sse / total)\n",
    "    '''\n",
    "    if i == 0:\n",
    "        print(total)\n",
    "        print(sse)\n",
    "        print(rmse_temp)\n",
    "    '''\n",
    "    #print( np.sum(rmse_temp) / len(rmse_temp))\n",
    "    rmse.append(rmse_temp)\n",
    "    \n",
    "    predict_result_f = predict_result.flatten()\n",
    "    true_result_f = true_result.flatten()\n",
    "    corr_temp = np.corrcoef(predict_result_f, true_result_f)[0, -1]\n",
    "    corr.append(corr_temp)\n",
    "RMSE = np.sum(rmse) / len(rmse)\n",
    "CORR = np.sum(corr) / len(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  nan\n",
      "CORR :  nan\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE : \",RMSE)\n",
    "print(\"CORR : \", CORR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

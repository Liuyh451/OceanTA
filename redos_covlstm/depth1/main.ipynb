{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-15T05:38:38.181515Z",
     "start_time": "2024-08-15T05:38:36.651221Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 这样写可以减少重复打开文件\n",
    "def load_all_nc_data(path, start_year, end_year):\n",
    "    data_dict = {}\n",
    "    current_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year + 1, 1, 1)\n",
    "    while current_date < end_date:\n",
    "        date_str = current_date.strftime('%Y%m%d')\n",
    "        nc_file = path + '/subset_' + date_str + '.nc'\n",
    "        # 当月份大于2时停止循环\n",
    "        if current_date.month > 2:\n",
    "            print(\"月份大于2，停止循环。\")\n",
    "            break\n",
    "        # 读取 NetCDF 文件\n",
    "        dataset = xr.open_dataset(nc_file)\n",
    "        \n",
    "        for var_name in dataset.variables:\n",
    "            if var_name not in data_dict:\n",
    "                data_dict[var_name] = []\n",
    "            data = dataset[var_name].values\n",
    "            data[data == -32768.0] = np.nan  # 替换标记值\n",
    "            data_dict[var_name].append(data)\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    # 将列表转换为数组\n",
    "    for var_name in data_dict:\n",
    "        data_dict[var_name] = np.array(data_dict[var_name])\n",
    "\n",
    "    return data_dict"
   ],
   "id": "ce3ad4451b2e2eed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_layer_data(data_dict, var_type, depth):\n",
    "    if var_type in data_dict:\n",
    "        data = data_dict[var_type]\n",
    "        #zeta为海表没有深度，所以永远为0\n",
    "        if var_type=='zeta':\n",
    "            data_type = data[ :,:, :]\n",
    "        else:\n",
    "            data_type = data[:,depth, :, :]\n",
    "        return data_type\n",
    "    else:\n",
    "        raise ValueError(f\"Variable '{var_type}' not found in data dictionary.\")\n",
    "\n",
    "def create_dataset(data, time_step):\n",
    "    dataX = []\n",
    "    for i in range(data.shape[0] - time_step + 1):\n",
    "        dataX.append(data[i:i + time_step])\n",
    "    return np.array(dataX)\n",
    "\n",
    "def read_raw_data(var_type, depth, time_step, data_dict):\n",
    "    # 提取层数据\n",
    "    raw_data = extract_layer_data(data_dict, var_type, depth)\n",
    "    print(raw_data[0])\n",
    "    width = raw_data.shape[2]  # 经度\n",
    "    length = raw_data.shape[1]  # 纬度\n",
    "\n",
    "    X = create_dataset(raw_data, time_step)\n",
    "    X = X.reshape(X.shape[0], time_step, length, width, 1)\n",
    "    Y = raw_data[time_step - 1:] \n",
    "    Y = Y.reshape(Y.shape[0], length, width, 1)\n",
    "\n",
    "    # 转置维度\n",
    "    X = X.transpose(0, 1, 4, 2, 3)\n",
    "    Y = Y.transpose(0, 3, 1, 2)\n",
    "\n",
    "    return X, Y, raw_data\n"
   ],
   "id": "37b3833bda647501",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T06:18:56.589289Z",
     "start_time": "2024-08-15T06:18:56.567070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用示例\n",
    "file_path = 'E:/DataSet/redos/Subset_1.0_1995'\n",
    "data_dict=load_all_nc_data(file_path,1995,1995)\n",
    "train_sssa, _, _ = read_raw_data('s', 0, 3, data_dict)\n",
    "train_ssha, _, _ = read_raw_data('zeta', 0, 3, data_dict)\n",
    "train_sswu, _, _ = read_raw_data('u', 0, 3, data_dict)\n",
    "train_sswv, _, _ = read_raw_data('v', 0, 3, data_dict)\n",
    "train_argo, label_argo, data_mask_t = read_raw_data('t', 1, 3, data_dict)\n",
    "del data_dict  # 删除字典对象"
   ],
   "id": "f396e8acb93c75fa",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:07.905905Z",
     "start_time": "2024-08-14T13:49:07.886904Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b81fa10d5f578690",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T06:27:37.270255Z",
     "start_time": "2024-08-15T06:27:37.255257Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f2879410f524d19f",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "46d748e23247a1ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T06:21:29.191380Z",
     "start_time": "2024-08-15T06:21:29.172378Z"
    }
   },
   "cell_type": "code",
   "source": "train_argo.shape",
   "id": "ab42e2adf54dfb06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57, 3, 1, 28, 52), (57, 3, 1, 28, 52))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T06:21:33.870786Z",
     "start_time": "2024-08-15T06:21:33.858787Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "40fd7f973d210458",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The arrays are different.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.609156Z",
     "start_time": "2024-08-14T13:49:10.596154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scaler(data):\n",
    "    #normalise [0,1]\n",
    "    data_max = np.nanmax(data)\n",
    "    data_min = np.nanmin(data)\n",
    "    data_scale = data_max - data_min\n",
    "    data_std = (data - data_min) / data_scale\n",
    "    # data_std = data_std * (2)  -1\n",
    "    data_std [np.isnan(data_std)] = 0\n",
    "    return data_std,data_min,data_scale\n",
    "\n",
    "#反归一化\n",
    "def unscaler(data, data_min, data_scale):\n",
    "    data_inv = (data * data_scale) + data_min\n",
    "    return data_inv"
   ],
   "id": "982481e7d6a7a5e4",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.639666Z",
     "start_time": "2024-08-14T13:49:10.613159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#对数据进行归一化\n",
    "sta_train,_,_ = scaler(train_argo[:-12,:])\n",
    "ssa_train,_,_  = scaler(train_sssa[:-12,:])\n",
    "ssha_train,_,_ = scaler(train_ssha[:-12,:])\n",
    "sswu_train,_,_ = scaler(train_sswu[:-12,:])\n",
    "sswv_train,_,_ = scaler(train_sswv[:-12,:])\n",
    "true_train,_,_ = scaler(label_argo[:-12,:])"
   ],
   "id": "e0d14250205e86e3",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.654671Z",
     "start_time": "2024-08-14T13:49:10.641669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#用倒数12个数据作为验证集\n",
    "sta_test,_,_ = scaler(train_argo[-12:])\n",
    "ssa_test,_,_  = scaler(train_sssa[-12:])\n",
    "ssha_test,_,_ = scaler(train_ssha[-12:])\n",
    "sswu_test,_,_ = scaler(train_sswu[-12:])\n",
    "sswv_test,_,_ = scaler(train_sswv[-12:])"
   ],
   "id": "179355ca045252ba",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.670720Z",
     "start_time": "2024-08-14T13:49:10.657669Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "44e734c1c8f79d52",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.686228Z",
     "start_time": "2024-08-14T13:49:10.673228Z"
    }
   },
   "cell_type": "code",
   "source": "sta_test.shape,ssa_test.shape,ssha_test.shape,sswu_test.shape,sswv_test.shape,label_argo.shape",
   "id": "11bdc4fdb816244b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 3, 1, 28, 52),\n",
       " (12, 3, 1, 28, 52),\n",
       " (12, 3, 1, 28, 52),\n",
       " (12, 3, 1, 28, 52),\n",
       " (12, 3, 1, 28, 52),\n",
       " (57, 1, 28, 52))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.702228Z",
     "start_time": "2024-08-14T13:49:10.689230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#将多个不同类型的训练数据和测试数据沿着指定轴进行拼接，axis=2即增加特征的数量（即通道或变量的数量）\n",
    "sta_train = np.concatenate((sta_train,ssa_train,ssha_train,sswu_train,sswv_train),axis = 2 )\n",
    "sta_test = np.concatenate((sta_test,ssa_test,ssha_test,sswu_test,sswv_test),axis = 2)"
   ],
   "id": "637aaabe12ea4b19",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.718230Z",
     "start_time": "2024-08-14T13:49:10.706233Z"
    }
   },
   "cell_type": "code",
   "source": "true_train.shape",
   "id": "ed2ef039110e3d21",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 1, 28, 52)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.734232Z",
     "start_time": "2024-08-14T13:49:10.720231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_test,test_min,test_scale = scaler(label_argo[-12:])\n",
    "print(\"true_test:::::\",true_test,\"true_min:::::\",test_min,\"true_scale:::::\",test_scale)\n",
    "#true_test是归一化后的 label_argo 数据，对应于最后 12 个时间步的标签数据\n",
    "#test_min是 label_argo[-12:] 数据中的最小值，在归一化过程中用作偏移量。\n",
    "#test_scale是 label_argo[-12:] 数据的范围，即最大值与最小值的差值。在归一化过程中用于缩放数据"
   ],
   "id": "6570683b81737540",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_test::::: [[[[0.99999404 0.99999404 0.999995   ... 0.9999958  0.9999945\n",
      "    0.99999344]\n",
      "   [0.9999937  0.9999932  0.99999356 ... 0.99999523 0.9999945\n",
      "    0.99999344]\n",
      "   [0.9999931  0.99999297 0.9999931  ... 0.99999464 0.9999945\n",
      "    0.9999937 ]\n",
      "   ...\n",
      "   [0.9999653  0.999966   0.9999676  ... 0.99998426 0.999985\n",
      "    0.9999852 ]\n",
      "   [0.99996805 0.9999683  0.99996877 ... 0.999984   0.99998486\n",
      "    0.99998534]\n",
      "   [0.99997056 0.9999703  0.9999702  ... 0.99998415 0.9999851\n",
      "    0.99998546]]]\n",
      "\n",
      "\n",
      " [[[0.99999356 0.99999475 0.999995   ... 0.99999523 0.99999416\n",
      "    0.9999927 ]\n",
      "   [0.99999344 0.9999939  0.9999939  ... 0.99999464 0.9999938\n",
      "    0.9999926 ]\n",
      "   [0.9999931  0.99999356 0.9999939  ... 0.99999404 0.99999344\n",
      "    0.9999927 ]\n",
      "   ...\n",
      "   [0.9999647  0.99996614 0.99996865 ... 0.9999845  0.9999851\n",
      "    0.999985  ]\n",
      "   [0.9999676  0.9999682  0.9999691  ... 0.9999845  0.9999851\n",
      "    0.99998534]\n",
      "   [0.9999702  0.9999702  0.9999707  ... 0.9999846  0.9999852\n",
      "    0.9999856 ]]]\n",
      "\n",
      "\n",
      " [[[0.99999344 0.9999931  0.9999926  ... 0.9999949  0.9999937\n",
      "    0.9999926 ]\n",
      "   [0.9999926  0.9999931  0.9999938  ... 0.9999944  0.9999937\n",
      "    0.9999925 ]\n",
      "   [0.9999926  0.9999926  0.99999285 ... 0.9999938  0.9999933\n",
      "    0.9999924 ]\n",
      "   ...\n",
      "   [0.99996555 0.9999678  0.9999697  ... 0.9999851  0.9999857\n",
      "    0.9999858 ]\n",
      "   [0.9999676  0.99996924 0.99997044 ... 0.9999851  0.9999858\n",
      "    0.9999862 ]\n",
      "   [0.9999696  0.99996984 0.9999713  ... 0.9999851  0.99998593\n",
      "    0.9999865 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.999992   0.9999902  0.9999871  ... 0.99999154 0.9999914\n",
      "    0.9999913 ]\n",
      "   [0.9999907  0.99999106 0.9999908  ... 0.99999034 0.9999907\n",
      "    0.9999908 ]\n",
      "   [0.99998903 0.9999902  0.9999901  ... 0.9999895  0.9999901\n",
      "    0.99999034]\n",
      "   ...\n",
      "   [0.9999701  0.9999728  0.99997604 ... 0.99998116 0.99998176\n",
      "    0.9999819 ]\n",
      "   [0.9999728  0.99997497 0.9999776  ... 0.99998164 0.99998224\n",
      "    0.9999826 ]\n",
      "   [0.9999747  0.9999764  0.9999784  ... 0.9999819  0.9999826\n",
      "    0.9999832 ]]]\n",
      "\n",
      "\n",
      " [[[0.99999106 0.9999888  0.99998736 ... 0.99999166 0.9999912\n",
      "    0.99999034]\n",
      "   [0.9999906  0.9999906  0.9999906  ... 0.9999906  0.9999906\n",
      "    0.9999901 ]\n",
      "   [0.99998903 0.99998915 0.99998975 ... 0.9999895  0.99998975\n",
      "    0.99998975]\n",
      "   ...\n",
      "   [0.99997056 0.99997354 0.999977   ... 0.999982   0.9999826\n",
      "    0.9999827 ]\n",
      "   [0.9999732  0.99997556 0.9999783  ... 0.9999825  0.9999831\n",
      "    0.99998343]\n",
      "   [0.99997497 0.999977   0.99997926 ... 0.9999826  0.99998343\n",
      "    0.9999839 ]]]\n",
      "\n",
      "\n",
      " [[[0.9999918  0.9999889  0.9999877  ... 0.9999913  0.9999914\n",
      "    0.9999913 ]\n",
      "   [0.99999106 0.9999914  0.99999106 ... 0.99999034 0.9999907\n",
      "    0.99999046]\n",
      "   [0.9999896  0.9999901  0.9999901  ... 0.9999895  0.99998987\n",
      "    0.99998975]\n",
      "   ...\n",
      "   [0.999972   0.9999747  0.9999778  ... 0.9999815  0.99998236\n",
      "    0.99998283]\n",
      "   [0.99997425 0.99997675 0.9999795  ... 0.9999821  0.99998295\n",
      "    0.99998355]\n",
      "   [0.9999758  0.99997807 0.99998045 ... 0.9999825  0.99998343\n",
      "    0.999984  ]]]] true_min::::: -32768.0 true_scale::::: 32773.836\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.750232Z",
     "start_time": "2024-08-14T13:49:10.736235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#将拼接后的数据作为训练集\n",
    "X_train = sta_train\n",
    "#训练集的标签\n",
    "true_train = true_train\n",
    "\n",
    "#训练集上用于评估\n",
    "X_eval = sta_test\n",
    "#\n",
    "true_eval = true_test\n",
    "X_test = sta_test\n",
    "true_test = true_test\n",
    "true_test.shape,X_eval.shape"
   ],
   "id": "b77d32c0d5755c87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 1, 28, 52), (12, 3, 5, 28, 52))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.766224Z",
     "start_time": "2024-08-14T13:49:10.753234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset"
   ],
   "id": "6a7cfe733af2e182",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.798226Z",
     "start_time": "2024-08-14T13:49:10.772222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Configs:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "configs = Configs()\n",
    "\n",
    "# trainer related\n",
    "configs.vtype = 't'\n",
    "# configs.depth = 11\n",
    "# configs.time_step = 1\n",
    "configs.n_cpu = 0\n",
    "# configs.device = torch.device('cpu')\n",
    "configs.device = torch.device('cuda:0')\n",
    "configs.batch_size_test = 100\n",
    "configs.batch_size = 5\n",
    "#configs.lr = 0.001\n",
    "configs.weight_decay = 0\n",
    "configs.display_interval = 100\n",
    "configs.num_epochs = 100\n",
    "#这是早停的耐心参数。即使模型在900个epoch内没有改善性能，训练仍会继续。如果在900个epoch内性能没有改善，训练将停止\n",
    "configs.early_stopping = True\n",
    "configs.patience = 100\n",
    "#禁用梯度裁剪（Gradient Clipping）。梯度裁剪用于防止梯度爆炸问题，但在这里未启用\n",
    "configs.gradient_clipping = False\n",
    "#设置梯度裁剪的阈值为1。如果梯度裁剪启用，梯度的最大值将被限制为1。不过在这种配置下，由于梯度裁剪被禁用，这个参数实际上不会生效\n",
    "configs.clipping_threshold = 1.\n",
    "\n",
    "# lr warmup\n",
    "#这是学习率预热的步数设置。在训练的前3000步内，学习率将逐渐从一个较小的值线性增加到预设的学习率。这种技术通常用于训练的初始阶段，以帮助模型更稳定地开始训练，减少初期的震荡。\n",
    "configs.warmup = 300\n",
    "\n",
    "# data related\n",
    "#这是输入数据的维度设置。这通常取决于你使用的数据的特征数或通道数\n",
    "configs.input_dim = 1 # 4 #这里应该是5吧 但是写的1我总感觉是5\n",
    "'''\n",
    "人家这个1是对的这个模型就是要保证输入通道和输出通道得一样\n",
    "默认为1\n",
    "'''\n",
    "configs.output_dim = 1\n",
    "#表示模型的输入序列长度为5，即模型在预测时会使用前5个时间步的数据作为输入\n",
    "configs.input_length = 5\n",
    "#表示模型的输出长度为1，即模型预测一个时间步的值。通常用于单步预测\n",
    "configs.output_length = 1\n",
    "#表示输入序列中的数据点之间的时间间隔为1。即数据是逐步连续的，没有跳跃\n",
    "configs.input_gap = 1\n",
    "#表示预测的时间偏移量为24。这可能意味着模型的目标是预测未来24个时间步后的数据点\n",
    "configs.pred_shift = 24\n",
    "#这个列表包含了一系列的深度值，这可能与模型的层次结构或者不同深度的输入特征相关联\n",
    "configs.depth = [5,6,11,16,20,25,30,34,36,38,40,42,44,46,48,50,51,52,53,54,55,57]\n",
    "#这个列表可能对应于不同深度的索引或层次级别。每个索引可能用于定位或选择特定深度的特征或数据\n",
    "configs.depthindex = [30,50, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900]\n",
    "\n",
    "# model\n",
    "#表示模型的维度即每个输入数据在模型中的表示为256维\n",
    "configs.d_model = 256\n",
    "#表示模型处理数据时的patch（小块）的大小为5×5。这通常用于图像或序列数据的分块处理\n",
    "configs.patch_size = (5,5)\n",
    "#表示嵌入的空间尺寸。这里12*16可能是表示最终嵌入的特征图的尺寸（例如视觉模型中的特征图大小）\n",
    "configs.emb_spatial_size = 12*16\n",
    "#表示多头注意力机制中的头数为4。多头注意力允许模型从不同的角度“看”数据，从而捕捉不同的关系\n",
    "configs.nheads = 4\n",
    "#表示前馈神经网络的维度用于增加模型的表达能力\n",
    "configs.dim_feedforward =512\n",
    "#表示在模型中使用的dropout率为0.3。Dropout是一种正则化技术，用于减少过拟合。\n",
    "configs.dropout = 0.3\n",
    "#表示编码器的层数为4。这意味着模型有4个堆叠的编码器层\n",
    "configs.num_encoder_layers = 4\n",
    "configs.num_decoder_layers = 4\n",
    "#这可能是学习率的衰减率（scheduler decay rate），用来控制模型训练过程中学习率的递减速度，以便在训练的后期进行更细致的优化\n",
    "configs.ssr_decay_rate = 3.e-6\n",
    "\n",
    "\n",
    "# plot 表示绘图的分辨率为600 DPI\n",
    "configs.plot_dpi = 600"
   ],
   "id": "237be25d7eb6364e",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.814225Z",
     "start_time": "2024-08-14T13:49:10.800225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils import  cmip_dataset\n",
    "from utils import  Trainer"
   ],
   "id": "92ecc8ccc612c761",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.830226Z",
     "start_time": "2024-08-14T13:49:10.817228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_train = cmip_dataset(X_train,true_train)\n",
    "print(dataset_train.GetDataShape())"
   ],
   "id": "1d9e727cf354fade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst input': (45, 3, 5, 28, 52), 'sst target': (45, 1, 28, 52)}\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.845229Z",
     "start_time": "2024-08-14T13:49:10.832230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_eval = cmip_dataset(X_eval,true_eval)\n",
    "print(dataset_eval.GetDataShape())"
   ],
   "id": "3726ce816ba35504",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst input': (12, 3, 5, 28, 52), 'sst target': (12, 1, 28, 52)}\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:49:10.893233Z",
     "start_time": "2024-08-14T13:49:10.847231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(configs)\n",
    "trainer.save_configs('config_train.pkl')\n"
   ],
   "id": "d557c5956dbda86f",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.338342Z",
     "start_time": "2024-08-14T13:49:10.895233Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(dataset_train, dataset_eval, 'checkpoint.chk')",
   "id": "49f2b703cde2b432",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1\n",
      "batch training loss: 0.94191, ssr: 1.00000, lr: 0.00000\n",
      "epoch eval loss:\n",
      "sst: 1.02\n",
      "eval score is improved from inf to 1.02039, saving model\n",
      "\n",
      "epoch: 2\n",
      "batch training loss: 0.90168, ssr: 0.99997, lr: 0.00005\n",
      "epoch eval loss:\n",
      "sst: 1.01\n",
      "eval score is improved from 1.02039 to 1.01261, saving model\n",
      "\n",
      "epoch: 3\n",
      "batch training loss: 0.79329, ssr: 0.99994, lr: 0.00009\n",
      "epoch eval loss:\n",
      "sst: 0.97\n",
      "eval score is improved from 1.01261 to 0.97206, saving model\n",
      "\n",
      "epoch: 4\n",
      "batch training loss: 0.65925, ssr: 0.99992, lr: 0.00013\n",
      "epoch eval loss:\n",
      "sst: 0.83\n",
      "eval score is improved from 0.97206 to 0.82845, saving model\n",
      "\n",
      "epoch: 5\n",
      "batch training loss: 0.52259, ssr: 0.99989, lr: 0.00017\n",
      "epoch eval loss:\n",
      "sst: 0.56\n",
      "eval score is improved from 0.82845 to 0.55731, saving model\n",
      "\n",
      "epoch: 6\n",
      "batch training loss: 0.41163, ssr: 0.99986, lr: 0.00021\n",
      "epoch eval loss:\n",
      "sst: 0.38\n",
      "eval score is improved from 0.55731 to 0.37958, saving model\n",
      "\n",
      "epoch: 7\n",
      "batch training loss: 0.34687, ssr: 0.99984, lr: 0.00026\n",
      "epoch eval loss:\n",
      "sst: 0.31\n",
      "eval score is improved from 0.37958 to 0.31237, saving model\n",
      "\n",
      "epoch: 8\n",
      "batch training loss: 0.29253, ssr: 0.99981, lr: 0.00030\n",
      "epoch eval loss:\n",
      "sst: 0.27\n",
      "eval score is improved from 0.31237 to 0.26796, saving model\n",
      "\n",
      "epoch: 9\n",
      "batch training loss: 0.24532, ssr: 0.99978, lr: 0.00034\n",
      "epoch eval loss:\n",
      "sst: 0.23\n",
      "eval score is improved from 0.26796 to 0.23003, saving model\n",
      "\n",
      "epoch: 10\n",
      "batch training loss: 0.20612, ssr: 0.99975, lr: 0.00038\n",
      "epoch eval loss:\n",
      "sst: 0.20\n",
      "eval score is improved from 0.23003 to 0.19827, saving model\n",
      "\n",
      "epoch: 11\n",
      "batch training loss: 0.17709, ssr: 0.99973, lr: 0.00042\n",
      "epoch eval loss:\n",
      "sst: 0.17\n",
      "eval score is improved from 0.19827 to 0.17030, saving model\n",
      "\n",
      "epoch: 12\n",
      "batch training loss: 0.15783, ssr: 0.99970, lr: 0.00047\n",
      "epoch eval loss:\n",
      "sst: 0.15\n",
      "eval score is improved from 0.17030 to 0.14955, saving model\n",
      "\n",
      "epoch: 13\n",
      "batch training loss: 0.14584, ssr: 0.99967, lr: 0.00051\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.14955 to 0.13852, saving model\n",
      "\n",
      "epoch: 14\n",
      "batch training loss: 0.13809, ssr: 0.99965, lr: 0.00055\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.13852 to 0.13305, saving model\n",
      "\n",
      "epoch: 15\n",
      "batch training loss: 0.13312, ssr: 0.99962, lr: 0.00059\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.13305 to 0.12965, saving model\n",
      "\n",
      "epoch: 16\n",
      "batch training loss: 0.12980, ssr: 0.99959, lr: 0.00063\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.12965 to 0.12713, saving model\n",
      "\n",
      "epoch: 17\n",
      "batch training loss: 0.12730, ssr: 0.99957, lr: 0.00068\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.12713 to 0.12512, saving model\n",
      "\n",
      "epoch: 18\n",
      "batch training loss: 0.12528, ssr: 0.99954, lr: 0.00072\n",
      "epoch eval loss:\n",
      "sst: 0.12\n",
      "eval score is improved from 0.12512 to 0.12331, saving model\n",
      "\n",
      "epoch: 19\n",
      "batch training loss: 0.12329, ssr: 0.99951, lr: 0.00076\n",
      "epoch eval loss:\n",
      "sst: 0.12\n",
      "eval score is improved from 0.12331 to 0.12087, saving model\n",
      "\n",
      "epoch: 20\n",
      "batch training loss: 0.12084, ssr: 0.99948, lr: 0.00080\n",
      "epoch eval loss:\n",
      "sst: 0.11\n",
      "eval score is improved from 0.12087 to 0.11464, saving model\n",
      "\n",
      "epoch: 21\n",
      "batch training loss: 0.11389, ssr: 0.99946, lr: 0.00084\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is improved from 0.11464 to 0.08889, saving model\n",
      "\n",
      "epoch: 22\n",
      "batch training loss: 0.08702, ssr: 0.99943, lr: 0.00089\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is improved from 0.08889 to 0.06343, saving model\n",
      "\n",
      "epoch: 23\n",
      "batch training loss: 0.05842, ssr: 0.99940, lr: 0.00093\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is improved from 0.06343 to 0.05177, saving model\n",
      "\n",
      "epoch: 24\n",
      "batch training loss: 0.04437, ssr: 0.99938, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.05177 to 0.04109, saving model\n",
      "\n",
      "epoch: 25\n",
      "batch training loss: 0.03602, ssr: 0.99935, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.04109 to 0.03232, saving model\n",
      "\n",
      "epoch: 26\n",
      "batch training loss: 0.03000, ssr: 0.99932, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.03232 to 0.02619, saving model\n",
      "\n",
      "epoch: 27\n",
      "batch training loss: 0.02578, ssr: 0.99930, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.02619 to 0.02119, saving model\n",
      "\n",
      "epoch: 28\n",
      "batch training loss: 0.02190, ssr: 0.99927, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.02119 to 0.01909, saving model\n",
      "\n",
      "epoch: 29\n",
      "batch training loss: 0.01908, ssr: 0.99924, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.01909 to 0.01613, saving model\n",
      "\n",
      "epoch: 30\n",
      "batch training loss: 0.01683, ssr: 0.99921, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.01613 to 0.01488, saving model\n",
      "\n",
      "epoch: 31\n",
      "batch training loss: 0.01512, ssr: 0.99919, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.01488 to 0.01400, saving model\n",
      "\n",
      "epoch: 32\n",
      "batch training loss: 0.01402, ssr: 0.99916, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.01400 to 0.01254, saving model\n",
      "\n",
      "epoch: 33\n",
      "batch training loss: 0.01291, ssr: 0.99913, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.01254 to 0.01171, saving model\n",
      "\n",
      "epoch: 34\n",
      "batch training loss: 0.01183, ssr: 0.99911, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.01171 to 0.01087, saving model\n",
      "\n",
      "epoch: 35\n",
      "batch training loss: 0.01107, ssr: 0.99908, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.01087 to 0.01033, saving model\n",
      "\n",
      "epoch: 36\n",
      "batch training loss: 0.01047, ssr: 0.99905, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.01033 to 0.00995, saving model\n",
      "\n",
      "epoch: 37\n",
      "batch training loss: 0.00998, ssr: 0.99903, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00995 to 0.00954, saving model\n",
      "\n",
      "epoch: 38\n",
      "batch training loss: 0.00958, ssr: 0.99900, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00954 to 0.00913, saving model\n",
      "\n",
      "epoch: 39\n",
      "batch training loss: 0.00927, ssr: 0.99897, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00913 to 0.00905, saving model\n",
      "\n",
      "epoch: 40\n",
      "batch training loss: 0.00910, ssr: 0.99894, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00905 to 0.00872, saving model\n",
      "\n",
      "epoch: 41\n",
      "batch training loss: 0.00869, ssr: 0.99892, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00872 to 0.00847, saving model\n",
      "\n",
      "epoch: 42\n",
      "batch training loss: 0.00849, ssr: 0.99889, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00847 to 0.00827, saving model\n",
      "\n",
      "epoch: 43\n",
      "batch training loss: 0.00828, ssr: 0.99886, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00827 to 0.00809, saving model\n",
      "\n",
      "epoch: 44\n",
      "batch training loss: 0.00808, ssr: 0.99884, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00809 to 0.00792, saving model\n",
      "\n",
      "epoch: 45\n",
      "batch training loss: 0.00792, ssr: 0.99881, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00792 to 0.00774, saving model\n",
      "\n",
      "epoch: 46\n",
      "batch training loss: 0.00778, ssr: 0.99878, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00774 to 0.00759, saving model\n",
      "\n",
      "epoch: 47\n",
      "batch training loss: 0.00765, ssr: 0.99876, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00759 to 0.00752, saving model\n",
      "\n",
      "epoch: 48\n",
      "batch training loss: 0.00754, ssr: 0.99873, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00752 to 0.00737, saving model\n",
      "\n",
      "epoch: 49\n",
      "batch training loss: 0.00739, ssr: 0.99870, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00737 to 0.00731, saving model\n",
      "\n",
      "epoch: 50\n",
      "batch training loss: 0.00729, ssr: 0.99867, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00731 to 0.00721, saving model\n",
      "\n",
      "epoch: 51\n",
      "batch training loss: 0.00718, ssr: 0.99865, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00721 to 0.00708, saving model\n",
      "\n",
      "epoch: 52\n",
      "batch training loss: 0.00710, ssr: 0.99862, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00708 to 0.00702, saving model\n",
      "\n",
      "epoch: 53\n",
      "batch training loss: 0.00707, ssr: 0.99859, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00702 to 0.00695, saving model\n",
      "\n",
      "epoch: 54\n",
      "batch training loss: 0.00693, ssr: 0.99857, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00695 to 0.00690, saving model\n",
      "\n",
      "epoch: 55\n",
      "batch training loss: 0.00691, ssr: 0.99854, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00690 to 0.00681, saving model\n",
      "\n",
      "epoch: 56\n",
      "batch training loss: 0.00689, ssr: 0.99851, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00681 to 0.00675, saving model\n",
      "\n",
      "epoch: 57\n",
      "batch training loss: 0.00675, ssr: 0.99849, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00675 to 0.00668, saving model\n",
      "\n",
      "epoch: 58\n",
      "batch training loss: 0.00668, ssr: 0.99846, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00668 to 0.00662, saving model\n",
      "\n",
      "epoch: 59\n",
      "batch training loss: 0.00663, ssr: 0.99843, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00662 to 0.00661, saving model\n",
      "\n",
      "epoch: 60\n",
      "batch training loss: 0.00658, ssr: 0.99840, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00661 to 0.00654, saving model\n",
      "\n",
      "epoch: 61\n",
      "batch training loss: 0.00653, ssr: 0.99838, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00654 to 0.00650, saving model\n",
      "\n",
      "epoch: 62\n",
      "batch training loss: 0.00655, ssr: 0.99835, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00650 to 0.00644, saving model\n",
      "\n",
      "epoch: 63\n",
      "batch training loss: 0.00647, ssr: 0.99832, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00644 to 0.00639, saving model\n",
      "\n",
      "epoch: 64\n",
      "batch training loss: 0.00642, ssr: 0.99830, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00639 to 0.00635, saving model\n",
      "\n",
      "epoch: 65\n",
      "batch training loss: 0.00635, ssr: 0.99827, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00635 to 0.00634, saving model\n",
      "\n",
      "epoch: 66\n",
      "batch training loss: 0.00632, ssr: 0.99824, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00634 to 0.00633, saving model\n",
      "\n",
      "epoch: 67\n",
      "batch training loss: 0.00630, ssr: 0.99822, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00633 to 0.00627, saving model\n",
      "\n",
      "epoch: 68\n",
      "batch training loss: 0.00625, ssr: 0.99819, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00627 to 0.00623, saving model\n",
      "\n",
      "epoch: 69\n",
      "batch training loss: 0.00622, ssr: 0.99816, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00623 to 0.00620, saving model\n",
      "\n",
      "epoch: 70\n",
      "batch training loss: 0.00624, ssr: 0.99813, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00620 to 0.00619, saving model\n",
      "\n",
      "epoch: 71\n",
      "batch training loss: 0.00618, ssr: 0.99811, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00619 to 0.00613, saving model\n",
      "\n",
      "epoch: 72\n",
      "batch training loss: 0.00612, ssr: 0.99808, lr: 0.00096\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00613 to 0.00611, saving model\n",
      "\n",
      "epoch: 73\n",
      "batch training loss: 0.00610, ssr: 0.99805, lr: 0.00095\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00611 to 0.00608, saving model\n",
      "\n",
      "epoch: 74\n",
      "batch training loss: 0.00607, ssr: 0.99803, lr: 0.00095\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00608 to 0.00606, saving model\n",
      "\n",
      "epoch: 75\n",
      "batch training loss: 0.00605, ssr: 0.99800, lr: 0.00094\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 76\n",
      "batch training loss: 0.00605, ssr: 0.99797, lr: 0.00093\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00606 to 0.00601, saving model\n",
      "\n",
      "epoch: 77\n",
      "batch training loss: 0.00602, ssr: 0.99795, lr: 0.00093\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00601 to 0.00598, saving model\n",
      "\n",
      "epoch: 78\n",
      "batch training loss: 0.00598, ssr: 0.99792, lr: 0.00092\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00598 to 0.00597, saving model\n",
      "\n",
      "epoch: 79\n",
      "batch training loss: 0.00596, ssr: 0.99789, lr: 0.00091\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00597 to 0.00594, saving model\n",
      "\n",
      "epoch: 80\n",
      "batch training loss: 0.00594, ssr: 0.99786, lr: 0.00091\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00594 to 0.00591, saving model\n",
      "\n",
      "epoch: 81\n",
      "batch training loss: 0.00591, ssr: 0.99784, lr: 0.00090\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00591 to 0.00591, saving model\n",
      "\n",
      "epoch: 82\n",
      "batch training loss: 0.00589, ssr: 0.99781, lr: 0.00090\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00591 to 0.00588, saving model\n",
      "\n",
      "epoch: 83\n",
      "batch training loss: 0.00587, ssr: 0.99778, lr: 0.00089\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00588 to 0.00586, saving model\n",
      "\n",
      "epoch: 84\n",
      "batch training loss: 0.00585, ssr: 0.99776, lr: 0.00089\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00586 to 0.00585, saving model\n",
      "\n",
      "epoch: 85\n",
      "batch training loss: 0.00585, ssr: 0.99773, lr: 0.00088\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00585 to 0.00582, saving model\n",
      "\n",
      "epoch: 86\n",
      "batch training loss: 0.00581, ssr: 0.99770, lr: 0.00088\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 87\n",
      "batch training loss: 0.00583, ssr: 0.99768, lr: 0.00087\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00582 to 0.00580, saving model\n",
      "\n",
      "epoch: 88\n",
      "batch training loss: 0.00581, ssr: 0.99765, lr: 0.00087\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00580 to 0.00579, saving model\n",
      "\n",
      "epoch: 89\n",
      "batch training loss: 0.00578, ssr: 0.99762, lr: 0.00086\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00579 to 0.00576, saving model\n",
      "\n",
      "epoch: 90\n",
      "batch training loss: 0.00576, ssr: 0.99759, lr: 0.00086\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00576 to 0.00576, saving model\n",
      "\n",
      "epoch: 91\n",
      "batch training loss: 0.00575, ssr: 0.99757, lr: 0.00085\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00576 to 0.00575, saving model\n",
      "\n",
      "epoch: 92\n",
      "batch training loss: 0.00574, ssr: 0.99754, lr: 0.00085\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00575 to 0.00572, saving model\n",
      "\n",
      "epoch: 93\n",
      "batch training loss: 0.00572, ssr: 0.99751, lr: 0.00084\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00572 to 0.00571, saving model\n",
      "\n",
      "epoch: 94\n",
      "batch training loss: 0.00571, ssr: 0.99749, lr: 0.00084\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 95\n",
      "batch training loss: 0.00570, ssr: 0.99746, lr: 0.00083\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00571 to 0.00570, saving model\n",
      "\n",
      "epoch: 96\n",
      "batch training loss: 0.00569, ssr: 0.99743, lr: 0.00083\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00570 to 0.00568, saving model\n",
      "\n",
      "epoch: 97\n",
      "batch training loss: 0.00568, ssr: 0.99741, lr: 0.00082\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00568 to 0.00566, saving model\n",
      "\n",
      "epoch: 98\n",
      "batch training loss: 0.00567, ssr: 0.99738, lr: 0.00082\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00566 to 0.00565, saving model\n",
      "\n",
      "epoch: 99\n",
      "batch training loss: 0.00565, ssr: 0.99735, lr: 0.00082\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 100\n",
      "batch training loss: 0.00564, ssr: 0.99732, lr: 0.00081\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.00565 to 0.00563, saving model\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.354343Z",
     "start_time": "2024-08-14T13:50:45.340341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_test = cmip_dataset(X_test, true_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=configs.batch_size_test, shuffle=False)\n",
    "print(dataset_test.GetDataShape())"
   ],
   "id": "3c5f920d25cdcb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst input': (12, 3, 5, 28, 52), 'sst target': (12, 1, 28, 52)}\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.434348Z",
     "start_time": "2024-08-14T13:50:45.357345Z"
    }
   },
   "cell_type": "code",
   "source": "chk = torch.load('./checkpoint.chk')",
   "id": "18d87c1ec9f278e7",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.450352Z",
     "start_time": "2024-08-14T13:50:45.436350Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.network.load_state_dict(chk['net'])",
   "id": "c9903ca7b5ab7f08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.498353Z",
     "start_time": "2024-08-14T13:50:45.452351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_test, test_pred, test_true = trainer.infer_test(dataset=dataset_test, dataloader=dataloader_test)\n",
    "print(dataset_test.target_sst.dtype)"
   ],
   "id": "6bd9daf8affc9069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.514355Z",
     "start_time": "2024-08-14T13:50:45.501355Z"
    }
   },
   "cell_type": "code",
   "source": "print(loss_test)",
   "id": "9bf1b18ea656957a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005631012376397848\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.594360Z",
     "start_time": "2024-08-14T13:50:45.516356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(test_true)\n",
    "print(test_pred)"
   ],
   "id": "c0622fd7844058bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[[0.9881, 0.9938, 0.9937,  ..., 0.9936, 0.9933, 0.9870],\n",
      "          [0.9940, 0.9950, 0.9950,  ..., 0.9949, 0.9949, 0.9942],\n",
      "          [0.9935, 0.9949, 0.9948,  ..., 0.9947, 0.9948, 0.9940],\n",
      "          ...,\n",
      "          [0.9940, 0.9950, 0.9949,  ..., 0.9949, 0.9949, 0.9940],\n",
      "          [0.9939, 0.9950, 0.9950,  ..., 0.9948, 0.9949, 0.9941],\n",
      "          [0.9879, 0.9943, 0.9945,  ..., 0.9940, 0.9937, 0.9882]]],\n",
      "\n",
      "\n",
      "        [[[0.9882, 0.9938, 0.9937,  ..., 0.9936, 0.9933, 0.9870],\n",
      "          [0.9940, 0.9950, 0.9950,  ..., 0.9949, 0.9949, 0.9942],\n",
      "          [0.9935, 0.9949, 0.9948,  ..., 0.9947, 0.9948, 0.9940],\n",
      "          ...,\n",
      "          [0.9940, 0.9950, 0.9949,  ..., 0.9949, 0.9949, 0.9940],\n",
      "          [0.9939, 0.9950, 0.9950,  ..., 0.9948, 0.9949, 0.9941],\n",
      "          [0.9879, 0.9943, 0.9945,  ..., 0.9940, 0.9937, 0.9882]]],\n",
      "\n",
      "\n",
      "        [[[0.9882, 0.9938, 0.9937,  ..., 0.9936, 0.9933, 0.9870],\n",
      "          [0.9940, 0.9950, 0.9950,  ..., 0.9949, 0.9949, 0.9942],\n",
      "          [0.9935, 0.9949, 0.9948,  ..., 0.9947, 0.9948, 0.9940],\n",
      "          ...,\n",
      "          [0.9940, 0.9950, 0.9949,  ..., 0.9949, 0.9949, 0.9940],\n",
      "          [0.9939, 0.9950, 0.9950,  ..., 0.9948, 0.9949, 0.9941],\n",
      "          [0.9880, 0.9943, 0.9945,  ..., 0.9940, 0.9937, 0.9882]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.9881, 0.9938, 0.9938,  ..., 0.9936, 0.9933, 0.9870],\n",
      "          [0.9940, 0.9950, 0.9950,  ..., 0.9949, 0.9949, 0.9942],\n",
      "          [0.9935, 0.9949, 0.9948,  ..., 0.9947, 0.9948, 0.9940],\n",
      "          ...,\n",
      "          [0.9940, 0.9950, 0.9949,  ..., 0.9949, 0.9949, 0.9940],\n",
      "          [0.9939, 0.9950, 0.9950,  ..., 0.9948, 0.9949, 0.9941],\n",
      "          [0.9879, 0.9943, 0.9945,  ..., 0.9939, 0.9937, 0.9883]]],\n",
      "\n",
      "\n",
      "        [[[0.9880, 0.9938, 0.9938,  ..., 0.9936, 0.9933, 0.9870],\n",
      "          [0.9939, 0.9950, 0.9950,  ..., 0.9949, 0.9949, 0.9942],\n",
      "          [0.9935, 0.9949, 0.9948,  ..., 0.9947, 0.9948, 0.9940],\n",
      "          ...,\n",
      "          [0.9940, 0.9950, 0.9949,  ..., 0.9949, 0.9949, 0.9940],\n",
      "          [0.9939, 0.9950, 0.9950,  ..., 0.9948, 0.9949, 0.9941],\n",
      "          [0.9879, 0.9943, 0.9944,  ..., 0.9940, 0.9937, 0.9882]]],\n",
      "\n",
      "\n",
      "        [[[0.9880, 0.9938, 0.9938,  ..., 0.9936, 0.9933, 0.9870],\n",
      "          [0.9939, 0.9950, 0.9950,  ..., 0.9949, 0.9949, 0.9942],\n",
      "          [0.9935, 0.9949, 0.9948,  ..., 0.9947, 0.9948, 0.9939],\n",
      "          ...,\n",
      "          [0.9940, 0.9950, 0.9949,  ..., 0.9949, 0.9949, 0.9940],\n",
      "          [0.9939, 0.9950, 0.9950,  ..., 0.9948, 0.9949, 0.9941],\n",
      "          [0.9880, 0.9943, 0.9944,  ..., 0.9940, 0.9937, 0.9883]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.610361Z",
     "start_time": "2024-08-14T13:50:45.596361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_pred = test_pred.cpu().numpy()\n",
    "test_true = test_true.cpu().numpy()\n"
   ],
   "id": "c664fcfd8036628a",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.641364Z",
     "start_time": "2024-08-14T13:50:45.612363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(test_pred)\n",
    "print(test_true)\n",
    "test_pred = unscaler(np.array(test_pred),test_min,test_scale)\n",
    "test_true = unscaler(np.array(test_true),test_min,test_scale)\n",
    "#应该不用加，推测是用来加新数据的\n",
    "# test_pred = add('temp', 1, test_pred)\n",
    "# test_true = add('temp', 1, test_true)"
   ],
   "id": "8ed6a16b8ae7b817",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.98813564 0.99379355 0.99373716 ... 0.99360424 0.99331903\n",
      "    0.98696387]\n",
      "   [0.99396026 0.994974   0.9949687  ... 0.99490976 0.9949284\n",
      "    0.99419665]\n",
      "   [0.99351394 0.99490184 0.9948444  ... 0.9947351  0.99483824\n",
      "    0.99396604]\n",
      "   ...\n",
      "   [0.9939881  0.99496406 0.9949328  ... 0.9948744  0.9949437\n",
      "    0.99398905]\n",
      "   [0.993913   0.9949778  0.9949719  ... 0.99484    0.9948997\n",
      "    0.9941286 ]\n",
      "   [0.98793435 0.99431187 0.99446607 ... 0.99398845 0.9937423\n",
      "    0.98818666]]]\n",
      "\n",
      "\n",
      " [[[0.9881717  0.99379766 0.99373454 ... 0.99360704 0.9933276\n",
      "    0.9870061 ]\n",
      "   [0.9939695  0.9949744  0.9949685  ... 0.9949104  0.9949296\n",
      "    0.9942036 ]\n",
      "   [0.99352765 0.9949032  0.9948445  ... 0.9947355  0.9948398\n",
      "    0.9939715 ]\n",
      "   ...\n",
      "   [0.9939869  0.9949637  0.99493194 ... 0.99487466 0.9949443\n",
      "    0.9939966 ]\n",
      "   [0.9939132  0.9949773  0.994971   ... 0.9948401  0.9949003\n",
      "    0.9941336 ]\n",
      "   [0.98792964 0.99431044 0.9944623  ... 0.99398637 0.9937429\n",
      "    0.9881989 ]]]\n",
      "\n",
      "\n",
      " [[[0.98818463 0.99380046 0.993737   ... 0.9936053  0.9933226\n",
      "    0.9869855 ]\n",
      "   [0.99397445 0.99497473 0.9949687  ... 0.99490935 0.9949285\n",
      "    0.99419856]\n",
      "   [0.9935374  0.99490416 0.9948453  ... 0.99473286 0.994837\n",
      "    0.9939598 ]\n",
      "   ...\n",
      "   [0.9939831  0.9949632  0.99493223 ... 0.9948763  0.9949455\n",
      "    0.9940037 ]\n",
      "   [0.99391884 0.9949783  0.99497193 ... 0.9948413  0.9949017\n",
      "    0.9941416 ]\n",
      "   [0.9879818  0.99431944 0.9944686  ... 0.99398935 0.99374986\n",
      "    0.9882358 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.98808986 0.9937826  0.9937525  ... 0.99359417 0.9933096\n",
      "    0.9869767 ]\n",
      "   [0.9939512  0.9949722  0.9949689  ... 0.9949067  0.9949276\n",
      "    0.99419594]\n",
      "   [0.9935152  0.99489826 0.994843   ... 0.9947292  0.99483484\n",
      "    0.9939573 ]\n",
      "   ...\n",
      "   [0.99400085 0.9949648  0.9949291  ... 0.994873   0.9949461\n",
      "    0.99401975]\n",
      "   [0.9939213  0.9949772  0.9949685  ... 0.99483156 0.99489844\n",
      "    0.9941498 ]\n",
      "   [0.9879424  0.99430734 0.9944501  ... 0.99394834 0.9937285\n",
      "    0.98825985]]]\n",
      "\n",
      "\n",
      " [[[0.9880494  0.99377424 0.99375784 ... 0.9936002  0.9933152\n",
      "    0.9869967 ]\n",
      "   [0.9939416  0.9949716  0.994969   ... 0.9949073  0.99492806\n",
      "    0.9941951 ]\n",
      "   [0.9935069  0.9948973  0.9948438  ... 0.99473184 0.9948341\n",
      "    0.9939503 ]\n",
      "   ...\n",
      "   [0.99400175 0.9949647  0.99492896 ... 0.9948726  0.9949454\n",
      "    0.9940136 ]\n",
      "   [0.9939224  0.99497736 0.9949684  ... 0.9948321  0.9948981\n",
      "    0.9941462 ]\n",
      "   [0.98794466 0.99430764 0.9944491  ... 0.9939509  0.9937275\n",
      "    0.98824525]]]\n",
      "\n",
      "\n",
      " [[[0.98800576 0.99376976 0.9937596  ... 0.9936083  0.9933272\n",
      "    0.98703   ]\n",
      "   [0.99393195 0.9949713  0.9949692  ... 0.9949083  0.99492913\n",
      "    0.9941973 ]\n",
      "   [0.9934951  0.99489695 0.99484503 ... 0.99473304 0.99483424\n",
      "    0.99394834]\n",
      "   ...\n",
      "   [0.9940114  0.9949659  0.9949298  ... 0.99487203 0.99494505\n",
      "    0.9940138 ]\n",
      "   [0.9939295  0.9949777  0.9949686  ... 0.99483234 0.99489784\n",
      "    0.99414647]\n",
      "   [0.987963   0.9943088  0.9944476  ... 0.99395823 0.99373007\n",
      "    0.9882554 ]]]]\n",
      "[[[[0.99999404 0.99999404 0.999995   ... 0.9999958  0.9999945\n",
      "    0.99999344]\n",
      "   [0.9999937  0.9999932  0.99999356 ... 0.99999523 0.9999945\n",
      "    0.99999344]\n",
      "   [0.9999931  0.99999297 0.9999931  ... 0.99999464 0.9999945\n",
      "    0.9999937 ]\n",
      "   ...\n",
      "   [0.9999653  0.999966   0.9999676  ... 0.99998426 0.999985\n",
      "    0.9999852 ]\n",
      "   [0.99996805 0.9999683  0.99996877 ... 0.999984   0.99998486\n",
      "    0.99998534]\n",
      "   [0.99997056 0.9999703  0.9999702  ... 0.99998415 0.9999851\n",
      "    0.99998546]]]\n",
      "\n",
      "\n",
      " [[[0.99999356 0.99999475 0.999995   ... 0.99999523 0.99999416\n",
      "    0.9999927 ]\n",
      "   [0.99999344 0.9999939  0.9999939  ... 0.99999464 0.9999938\n",
      "    0.9999926 ]\n",
      "   [0.9999931  0.99999356 0.9999939  ... 0.99999404 0.99999344\n",
      "    0.9999927 ]\n",
      "   ...\n",
      "   [0.9999647  0.99996614 0.99996865 ... 0.9999845  0.9999851\n",
      "    0.999985  ]\n",
      "   [0.9999676  0.9999682  0.9999691  ... 0.9999845  0.9999851\n",
      "    0.99998534]\n",
      "   [0.9999702  0.9999702  0.9999707  ... 0.9999846  0.9999852\n",
      "    0.9999856 ]]]\n",
      "\n",
      "\n",
      " [[[0.99999344 0.9999931  0.9999926  ... 0.9999949  0.9999937\n",
      "    0.9999926 ]\n",
      "   [0.9999926  0.9999931  0.9999938  ... 0.9999944  0.9999937\n",
      "    0.9999925 ]\n",
      "   [0.9999926  0.9999926  0.99999285 ... 0.9999938  0.9999933\n",
      "    0.9999924 ]\n",
      "   ...\n",
      "   [0.99996555 0.9999678  0.9999697  ... 0.9999851  0.9999857\n",
      "    0.9999858 ]\n",
      "   [0.9999676  0.99996924 0.99997044 ... 0.9999851  0.9999858\n",
      "    0.9999862 ]\n",
      "   [0.9999696  0.99996984 0.9999713  ... 0.9999851  0.99998593\n",
      "    0.9999865 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.999992   0.9999902  0.9999871  ... 0.99999154 0.9999914\n",
      "    0.9999913 ]\n",
      "   [0.9999907  0.99999106 0.9999908  ... 0.99999034 0.9999907\n",
      "    0.9999908 ]\n",
      "   [0.99998903 0.9999902  0.9999901  ... 0.9999895  0.9999901\n",
      "    0.99999034]\n",
      "   ...\n",
      "   [0.9999701  0.9999728  0.99997604 ... 0.99998116 0.99998176\n",
      "    0.9999819 ]\n",
      "   [0.9999728  0.99997497 0.9999776  ... 0.99998164 0.99998224\n",
      "    0.9999826 ]\n",
      "   [0.9999747  0.9999764  0.9999784  ... 0.9999819  0.9999826\n",
      "    0.9999832 ]]]\n",
      "\n",
      "\n",
      " [[[0.99999106 0.9999888  0.99998736 ... 0.99999166 0.9999912\n",
      "    0.99999034]\n",
      "   [0.9999906  0.9999906  0.9999906  ... 0.9999906  0.9999906\n",
      "    0.9999901 ]\n",
      "   [0.99998903 0.99998915 0.99998975 ... 0.9999895  0.99998975\n",
      "    0.99998975]\n",
      "   ...\n",
      "   [0.99997056 0.99997354 0.999977   ... 0.999982   0.9999826\n",
      "    0.9999827 ]\n",
      "   [0.9999732  0.99997556 0.9999783  ... 0.9999825  0.9999831\n",
      "    0.99998343]\n",
      "   [0.99997497 0.999977   0.99997926 ... 0.9999826  0.99998343\n",
      "    0.9999839 ]]]\n",
      "\n",
      "\n",
      " [[[0.9999918  0.9999889  0.9999877  ... 0.9999913  0.9999914\n",
      "    0.9999913 ]\n",
      "   [0.99999106 0.9999914  0.99999106 ... 0.99999034 0.9999907\n",
      "    0.99999046]\n",
      "   [0.9999896  0.9999901  0.9999901  ... 0.9999895  0.99998987\n",
      "    0.99998975]\n",
      "   ...\n",
      "   [0.999972   0.9999747  0.9999778  ... 0.9999815  0.99998236\n",
      "    0.99998283]\n",
      "   [0.99997425 0.99997675 0.9999795  ... 0.9999821  0.99998295\n",
      "    0.99998355]\n",
      "   [0.9999758  0.99997807 0.99998045 ... 0.9999825  0.99998343\n",
      "    0.999984  ]]]]\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.656365Z",
     "start_time": "2024-08-14T13:50:45.643366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.save(\"./data/test_pred.npy\",test_pred)\n",
    "np.save(\"./data/test_true.npy\",test_true)"
   ],
   "id": "ae10727c039e20eb",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.672397Z",
     "start_time": "2024-08-14T13:50:45.659366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(test_pred.shape)\n",
    "print(test_true.shape)\n",
    "#todo 这里他写的形状突然由(12, 1, 28, 52)变为了(12,28,52,1)没见到改变形状的操作啊\n",
    "#对数组重新塑形，这里是np不是张量\n",
    "test_true = np.transpose(test_true, (0, 3, 1, 2))\n",
    "test_pred = np.transpose(test_pred, (0, 3, 1, 2))\n",
    "print(test_pred.shape)\n",
    "print(test_true.shape)"
   ],
   "id": "a7b696ae13f77014",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1, 28, 52)\n",
      "(12, 1, 28, 52)\n",
      "(12, 52, 1, 28)\n",
      "(12, 52, 1, 28)\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.703906Z",
     "start_time": "2024-08-14T13:50:45.674906Z"
    }
   },
   "cell_type": "code",
   "source": "print(test_pred),print(test_true)",
   "id": "3c28185123c835a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-383.0039  -192.10938 -206.73633 ... -191.19727 -193.6582\n",
      "    -389.60156]]\n",
      "\n",
      "  [[-197.57422 -158.88477 -161.25    ... -159.21094 -158.76172\n",
      "    -180.58594]]\n",
      "\n",
      "  [[-199.42188 -159.0586  -163.13281 ... -160.23633 -158.95508\n",
      "    -175.5332 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-203.77734 -160.99023 -166.71484 ... -162.14844 -163.27539\n",
      "    -191.18555]]\n",
      "\n",
      "  [[-213.125   -160.3789  -163.33398 ... -159.8789  -161.32031\n",
      "    -199.2539 ]]\n",
      "\n",
      "  [[-421.4082  -184.36133 -191.91992 ... -191.16602 -186.59375\n",
      "    -381.33203]]]\n",
      "\n",
      "\n",
      " [[[-381.82227 -191.80664 -206.28711 ... -191.23633 -193.65234\n",
      "    -389.75586]]\n",
      "\n",
      "  [[-197.43945 -158.87305 -161.20508 ... -159.22266 -158.77734\n",
      "    -180.63281]]\n",
      "\n",
      "  [[-199.50781 -159.0664  -163.1289  ... -160.26367 -158.98438\n",
      "    -175.65625]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-203.68555 -160.96875 -166.70312 ... -162.14062 -163.27344\n",
      "    -191.2539 ]]\n",
      "\n",
      "  [[-212.84375 -160.33984 -163.2832  ... -159.85938 -161.30078\n",
      "    -199.23438]]\n",
      "\n",
      "  [[-420.02344 -184.13281 -191.74023 ... -190.91797 -186.42969\n",
      "    -380.93164]]]\n",
      "\n",
      "\n",
      " [[[-381.39844 -191.64453 -205.9668  ... -191.36133 -193.4668\n",
      "    -388.04688]]\n",
      "\n",
      "  [[-197.34766 -158.86133 -161.17383 ... -159.23828 -158.74414\n",
      "    -180.33789]]\n",
      "\n",
      "  [[-199.42773 -159.0586  -163.10352 ... -160.2539  -158.95312\n",
      "    -175.44922]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-203.74219 -161.0039  -166.78906 ... -162.08594 -163.23438\n",
      "    -191.15625]]\n",
      "\n",
      "  [[-213.00781 -160.37695 -163.375   ... -159.81836 -161.2539\n",
      "    -199.00586]]\n",
      "\n",
      "  [[-420.69922 -184.29883 -192.125   ... -190.68555 -186.16797\n",
      "    -379.72266]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-384.50586 -192.40625 -206.69531 ... -190.7793  -193.38672\n",
      "    -389.3379 ]]\n",
      "\n",
      "  [[-197.9336  -158.94336 -161.36719 ... -159.1875  -158.78125\n",
      "    -180.73438]]\n",
      "\n",
      "  [[-198.91992 -159.05273 -163.17773 ... -160.35742 -159.0664\n",
      "    -176.05664]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-204.10742 -161.08984 -166.9082  ... -162.19531 -163.55273\n",
      "    -192.5    ]]\n",
      "\n",
      "  [[-213.4336  -160.40625 -163.44531 ... -159.79883 -161.36133\n",
      "    -199.70508]]\n",
      "\n",
      "  [[-420.98828 -184.38477 -192.20703 ... -190.16016 -185.89648\n",
      "    -378.9336 ]]]\n",
      "\n",
      "\n",
      " [[[-385.83203 -192.7207  -206.9668  ... -190.75    -193.34961\n",
      "    -389.26367]]\n",
      "\n",
      "  [[-198.20703 -158.96484 -161.39844 ... -159.18945 -158.77539\n",
      "    -180.72461]]\n",
      "\n",
      "  [[-198.74414 -159.04883 -163.15234 ... -160.36133 -159.06836\n",
      "    -176.08984]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-203.91016 -161.07031 -166.82227 ... -162.20703 -163.53516\n",
      "    -192.41602]]\n",
      "\n",
      "  [[-213.25    -160.39062 -163.46875 ... -159.82227 -161.37305\n",
      "    -199.73828]]\n",
      "\n",
      "  [[-420.33203 -184.41211 -192.43555 ... -190.36133 -186.01367\n",
      "    -379.4121 ]]]\n",
      "\n",
      "\n",
      " [[[-387.26172 -193.03711 -207.35352 ... -190.4336  -193.11719\n",
      "    -388.6621 ]]\n",
      "\n",
      "  [[-198.35352 -158.97461 -161.41016 ... -159.15039 -158.76367\n",
      "    -180.68555]]\n",
      "\n",
      "  [[-198.6875  -159.04297 -163.11133 ... -160.33398 -159.0625\n",
      "    -176.13867]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-203.64453 -161.03906 -166.7832  ... -162.22656 -163.52734\n",
      "    -192.17578]]\n",
      "\n",
      "  [[-212.85742 -160.35547 -163.46484 ... -159.83398 -161.38086\n",
      "    -199.6543 ]]\n",
      "\n",
      "  [[-419.24023 -184.33984 -192.5     ... -190.35547 -186.00586\n",
      "    -379.08008]]]]\n",
      "[[[[5.640625  5.6289062 5.609375  ... 4.6992188 4.7890625 4.8710938]]\n",
      "\n",
      "  [[5.640625  5.6132812 5.6054688 ... 4.7226562 4.796875  4.8632812]]\n",
      "\n",
      "  [[5.671875  5.625     5.609375  ... 4.7734375 4.8125    4.859375 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[5.6992188 5.6796875 5.6601562 ... 5.3203125 5.3125    5.3164062]]\n",
      "\n",
      "  [[5.65625   5.65625   5.65625   ... 5.34375   5.3398438 5.3476562]]\n",
      "\n",
      "  [[5.6210938 5.6210938 5.6289062 ... 5.3515625 5.3554688 5.359375 ]]]\n",
      "\n",
      "\n",
      " [[[5.625     5.6210938 5.609375  ... 4.6796875 4.7734375 4.859375 ]]\n",
      "\n",
      "  [[5.6640625 5.6367188 5.625     ... 4.7265625 4.7929688 4.859375 ]]\n",
      "\n",
      "  [[5.671875  5.6367188 5.6367188 ... 4.8085938 4.8242188 4.875    ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[5.6796875 5.6601562 5.640625  ... 5.328125  5.328125  5.3320312]]\n",
      "\n",
      "  [[5.6445312 5.6328125 5.6210938 ... 5.3476562 5.3476562 5.3515625]]\n",
      "\n",
      "  [[5.5976562 5.59375   5.5976562 ... 5.34375   5.3554688 5.3632812]]]\n",
      "\n",
      "\n",
      " [[[5.6210938 5.59375   5.59375   ... 4.7070312 4.7734375 4.8398438]]\n",
      "\n",
      "  [[5.609375  5.609375  5.59375   ... 4.78125   4.828125  4.8476562]]\n",
      "\n",
      "  [[5.59375   5.6328125 5.6015625 ... 4.84375   4.8671875 4.8945312]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[5.6679688 5.6523438 5.6328125 ... 5.3476562 5.3476562 5.3476562]]\n",
      "\n",
      "  [[5.6289062 5.6289062 5.6171875 ... 5.3671875 5.3710938 5.375    ]]\n",
      "\n",
      "  [[5.59375   5.5898438 5.5859375 ... 5.3710938 5.3828125 5.3945312]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[5.5742188 5.53125   5.4765625 ... 4.8554688 4.9453125 5.0078125]]\n",
      "\n",
      "  [[5.515625  5.5429688 5.515625  ... 4.9453125 5.015625  5.0625   ]]\n",
      "\n",
      "  [[5.4140625 5.5351562 5.5117188 ... 5.0507812 5.1015625 5.1289062]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[5.5585938 5.5195312 5.4921875 ... 5.21875   5.234375  5.2421875]]\n",
      "\n",
      "  [[5.5546875 5.53125   5.5117188 ... 5.2382812 5.2539062 5.265625 ]]\n",
      "\n",
      "  [[5.5507812 5.5351562 5.5195312 ... 5.2421875 5.265625  5.2851562]]]\n",
      "\n",
      "\n",
      " [[[5.5429688 5.5273438 5.4765625 ... 4.8710938 4.9570312 5.015625 ]]\n",
      "\n",
      "  [[5.46875   5.5273438 5.4804688 ... 4.96875   5.0351562 5.0820312]]\n",
      "\n",
      "  [[5.421875  5.5273438 5.5       ... 5.0820312 5.125     5.15625  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[5.5625    5.5273438 5.4921875 ... 5.2460938 5.2617188 5.265625 ]]\n",
      "\n",
      "  [[5.546875  5.5273438 5.5       ... 5.265625  5.28125   5.2929688]]\n",
      "\n",
      "  [[5.5195312 5.5117188 5.5       ... 5.2695312 5.2929688 5.3085938]]]\n",
      "\n",
      "\n",
      " [[[5.5664062 5.5429688 5.4960938 ... 4.9179688 4.9921875 5.0429688]]\n",
      "\n",
      "  [[5.4726562 5.5546875 5.5117188 ... 5.0078125 5.0742188 5.1171875]]\n",
      "\n",
      "  [[5.4335938 5.5429688 5.5117188 ... 5.109375  5.1640625 5.1953125]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[5.5507812 5.5195312 5.4921875 ... 5.2304688 5.25      5.2617188]]\n",
      "\n",
      "  [[5.5546875 5.53125   5.5039062 ... 5.2578125 5.2773438 5.2929688]]\n",
      "\n",
      "  [[5.5507812 5.5234375 5.5       ... 5.2734375 5.296875  5.3125   ]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.719907Z",
     "start_time": "2024-08-14T13:50:45.706909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_pred = np.squeeze(test_pred)\n",
    "test_true = np.squeeze(test_true)\n",
    "cha = (test_true[0] - test_pred[0]) ** 2\n",
    "test_pred[np.isnan(test_pred)] = 0\n",
    "test_true[np.isnan(test_true)] = 0"
   ],
   "id": "18040523012abb91",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.750912Z",
     "start_time": "2024-08-14T13:50:45.721910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rmse = []\n",
    "corr = []\n",
    "test_pred.shape[0]\n",
    "for i in range(test_pred.shape[0]):\n",
    "    predict_result = test_pred[i]\n",
    "    #print(predict_result)\n",
    "    true_result = test_true[i]\n",
    "    total = predict_result.shape[0] * predict_result.shape[1] \n",
    "    print(total)\n",
    "    sse = np.sum((true_result - predict_result) ** 2)\n",
    "    print(sse)\n",
    "    rmse_temp = np.sqrt(sse / total)\n",
    "    '''\n",
    "    if i == 0:\n",
    "        print(total)\n",
    "        print(sse)\n",
    "        print(rmse_temp)\n",
    "    '''\n",
    "    #print( np.sum(rmse_temp) / len(rmse_temp))\n",
    "    rmse.append(rmse_temp)\n",
    "\n",
    "    predict_result_f = predict_result.flatten()\n",
    "    true_result_f = true_result.flatten()\n",
    "    corr_temp = np.corrcoef(predict_result_f, true_result_f)[0, -1]\n",
    "    corr.append(corr_temp)\n",
    "RMSE = np.sum(rmse) / len(rmse)\n",
    "CORR = np.sum(corr) / len(corr)"
   ],
   "id": "b8acc57906ea2c57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1456\n",
      "49579956.0\n",
      "1456\n",
      "49632424.0\n",
      "1456\n",
      "49608000.0\n",
      "1456\n",
      "49668376.0\n",
      "1456\n",
      "49657892.0\n",
      "1456\n",
      "49635052.0\n",
      "1456\n",
      "49546960.0\n",
      "1456\n",
      "49565124.0\n",
      "1456\n",
      "49498976.0\n",
      "1456\n",
      "49498748.0\n",
      "1456\n",
      "49571910.0\n",
      "1456\n",
      "49610624.0\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.766910Z",
     "start_time": "2024-08-14T13:50:45.753911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(RMSE)\n",
    "print(CORR)"
   ],
   "id": "1f9ff3c5b5c8ca6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184.5500338429\n",
      "0.999983861639083\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.814915Z",
     "start_time": "2024-08-14T13:50:45.770915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def loss(data_mask, depth, test_pred, test_true):\n",
    "    test_preds = np.array(test_pred, copy=True)\n",
    "    test_trues = np.array(test_true, copy=True)\n",
    "\n",
    "\n",
    "    test_preds = np.squeeze(test_preds)\n",
    "    test_trues = np.squeeze(test_trues)\n",
    "\n",
    "    test_preds[np.isnan(test_preds)] = 0\n",
    "    test_trues[np.isnan(test_trues)] = 0\n",
    "    mask = data_mask\n",
    "    print(mask.shape,test_preds.shape, test_trues.shape)\n",
    "    #     mask = np.squeeze(mask)\n",
    "    mask = mask[0]\n",
    "    mask=np.transpose(mask)\n",
    "\n",
    "    total = mask.shape[0] * mask.shape[1]\n",
    "    total_nan = len(mask[np.isnan(mask)])\n",
    "    total_real = total - total_nan\n",
    "    #     print('Total NaN:',total_nan)#统计数据中的nan值\n",
    "    #     print('Total Real:',total_real)#统计数据中的nan值\n",
    "    #     #nan：0 values ：1\n",
    "    mask[~np.isnan(mask)] = 1\n",
    "    mask[np.isnan(mask)] = 0\n",
    "    rmse = []\n",
    "    rmse_temp = []\n",
    "    nrmse = []\n",
    "    nrmse_temp = []\n",
    "    mae = []\n",
    "    mae_temp = []\n",
    "    for i in range(0, test_preds.shape[0]):\n",
    "        final_temp = mask * test_preds[i]\n",
    "        test_temp = mask * test_trues[i]\n",
    "        # np.sum((y_actual - y_predicted) ** 2)\n",
    "        sse = np.sum((test_temp - final_temp) ** 2)\n",
    "        mse_temp = sse / total_real\n",
    "        rmse_temp = np.sqrt(mse_temp)\n",
    "        nrmse_temp = rmse_temp / np.mean(test_temp)\n",
    "        rmse.append(rmse_temp)\n",
    "        nrmse.append(nrmse_temp)\n",
    "        mae_temp = mean_absolute_error(test_temp, final_temp) * total / total_real\n",
    "\n",
    "        mae.append(mae_temp)\n",
    "    #     print('NAN:',len(test_pred[np.isnan(test_pred)]))\n",
    "    #     print('TEST NANMIN',np.nanmin(test_pred))\n",
    "    #     print('TEST MIN',test_pred.min())\n",
    "    # print(str(depth)+'层')\n",
    "    RMSE = np.sum(rmse) / len(rmse)\n",
    "    MAE = np.sum(mae) / len(mae)\n",
    "    NRMSE = np.sum(nrmse) / len(nrmse)\n",
    "    # NRMSE = nrmse\n",
    "    print(str(depth) + '层:' + 'NRMSE RESULT:\\n', NRMSE)\n",
    "\n",
    "    #     print('MAE RESULT:\\n',MAE)\n",
    "\n",
    "    return NRMSE\n",
    "nrmse = loss(data_mask_t, 1, test_pred, test_true)"
   ],
   "id": "6d954248973cb687",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 28, 52) (12, 52, 28) (12, 52, 28)\n",
      "1层:NRMSE RESULT:\n",
      " -0.36022672022224084\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:50:45.830916Z",
     "start_time": "2024-08-14T13:50:45.817918Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fda47df0bea059fe",
   "outputs": [],
   "execution_count": 82
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

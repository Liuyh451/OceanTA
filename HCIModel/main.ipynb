{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, time_step):\n",
    "    dataX = []\n",
    "    for i in range(data.shape[0] - time_step + 1):\n",
    "        dataX.append(data[i:i + time_step])\n",
    "    return np.array(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_data(vtype, depth, time_step):\n",
    "    train_argo = np.load('/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/'+vtype+'_0_ano.npy')#读取数据，指定维度标签\n",
    "    label_argo = np.load('/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/'+vtype+'_'+str(depth)+'_ano.npy')#206,60,80\n",
    "    width = train_argo.shape[2] #对应经度\n",
    "    lenth = train_argo.shape[1] #对应纬度\n",
    "    X = create_dataset(train_argo, time_step)\n",
    "    X = X.reshape(X.shape[0],time_step,lenth,width,1)\n",
    "    Y = label_argo[time_step-1 : label_argo.shape[0]] \n",
    "    Y =Y.reshape(Y.shape[0],lenth,width,1)\n",
    "    X = X.transpose(0,1,4,2,3)\n",
    "    Y = Y.transpose(0,3,1,2)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(data):\n",
    "    #normalise [0,1]\n",
    "    data_max = np.nanmax(data)\n",
    "    data_min = np.nanmin(data)\n",
    "    data_scale = data_max - data_min\n",
    "    data_std = (data - data_min) / data_scale\n",
    "    # data_std = data_std * (2)  -1\n",
    "    data_std [np.isnan(data_std)] = 0\n",
    "    return data_std,data_min,data_scale\n",
    "'''\n",
    "将nan替换为0，也就是说nan的异常为0，即nan的值为均值\n",
    "'''\n",
    "\n",
    "def unscaler(data, data_min, data_scale):\n",
    "    data_inv = (data * data_scale) + data_min\n",
    "    return data_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sssa, _ = read_raw_data('salt', 0, 3)\n",
    "train_ssha,_ = read_raw_data('sla',0,3)\n",
    "train_sswu,_ = read_raw_data('uwnd',0,3)\n",
    "train_sswv,_ = read_raw_data('vwnd',0,3)\n",
    "train_argo, label_argo = read_raw_data('temp', 2, 3)# 要改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sta_train,_,_ = scaler(train_argo[:-12,:])\n",
    "ssa_train,_,_  = scaler(train_sssa[:-12,:])\n",
    "ssha_train,_,_ = scaler(train_ssha[:-12,:])\n",
    "sswu_train,_,_ = scaler(train_sswu[:-12,:])\n",
    "sswv_train,_,_ = scaler(train_sswv[:-12,:])\n",
    "true_train,_,_ = scaler(label_argo[:-12,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 3, 1, 60, 80)\n",
      "(166, 3, 1, 60, 80)\n",
      "(166, 3, 1, 60, 80)\n",
      "(166, 3, 1, 60, 80)\n"
     ]
    }
   ],
   "source": [
    "print(sta_train.shape)\n",
    "print(ssa_train.shape)\n",
    "print(ssha_train.shape)\n",
    "print(sswu_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sta_test,_,_ = scaler(train_argo[-12:])\n",
    "ssa_test,_,_  = scaler(train_sssa[-12:])\n",
    "ssha_test,_,_ = scaler(train_ssha[-12:])\n",
    "sswu_test,_,_ = scaler(train_sswu[-12:])\n",
    "sswv_test,_,_ = scaler(train_sswv[-12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sta_train = np.concatenate((sta_train,ssa_train,ssha_train,sswu_train,sswv_train),axis = 2 )\n",
    "sta_test = np.concatenate((sta_test,ssa_test,ssha_test,sswu_test,sswv_test),axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test,test_min,test_scale = scaler(label_argo[-12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_train,test_min1,test_scale1= scaler(label_argo[:-12,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 1, 60, 80)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sta_train\n",
    "true_train = true_train\n",
    "\n",
    "\n",
    "X_eval = sta_test\n",
    "true_eval = true_test\n",
    "X_test = sta_test\n",
    "true_test = true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "configs = Configs()\n",
    "\n",
    "# trainer related\n",
    "configs.vtype = 'temp'\n",
    "# configs.depth = 11\n",
    "# configs.time_step = 1\n",
    "configs.n_cpu = 0\n",
    "configs.device = torch.device('cuda:0')\n",
    "configs.batch_size_test = 100\n",
    "configs.batch_size = 32\n",
    "#configs.lr = 0.001\n",
    "configs.weight_decay = 0\n",
    "configs.display_interval = 200\n",
    "configs.num_epochs = 900\n",
    "configs.early_stopping = True\n",
    "configs.patience = 900\n",
    "configs.gradient_clipping = False\n",
    "configs.clipping_threshold = 1.\n",
    "\n",
    "# lr warmup\n",
    "configs.warmup = 3000\n",
    "\n",
    "# data related\n",
    "configs.input_dim = 1 # 4 #这里应该是5吧 但是写的1我总感觉是5\n",
    "'''\n",
    "人家这个1是对的这个模型就是要保证输入通道和输出通道得一样\n",
    "默认为1\n",
    "'''\n",
    "configs.output_dim = 1\n",
    "\n",
    "configs.input_length = 5\n",
    "configs.output_length = 1\n",
    "\n",
    "configs.input_gap = 1\n",
    "configs.pred_shift = 32\n",
    "configs.depth = [5,6,11,16,20,25,30,34,36,38,40,42,44,46,48,50,51,52,53,54,55,57]\n",
    "configs.depthindex = [30,50, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900]\n",
    "\n",
    "# model\n",
    "configs.d_model = 256\n",
    "configs.patch_size = (5,5)\n",
    "configs.emb_spatial_size = 12*16\n",
    "configs.nheads = 4\n",
    "configs.dim_feedforward = 512\n",
    "configs.dropout = 0.3\n",
    "configs.num_encoder_layers = 4\n",
    "configs.num_decoder_layers = 4\n",
    "\n",
    "configs.ssr_decay_rate = 3.e-5\n",
    "\n",
    "\n",
    "# plot\n",
    "configs.plot_dpi = 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class covlstmformer(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super().__init__()\n",
    "        self.configs = configs\n",
    "        self.d_model = 25\n",
    "        self.device = configs.device\n",
    "        self.cov1 = Cov(5, 8,3, 5)\n",
    "        self.cov2 = Cov(5, 8,3, 5)\n",
    "        self.encode1 = EncoderLayer(self.d_model, 1, configs.dim_feedforward, configs.dropout)\n",
    "        self.encode2 = EncoderLayer(self.d_model, 1, configs.dim_feedforward, configs.dropout)\n",
    "        self.cov_last  = Cov_last(5, 8,3, 1)\n",
    "    def forward(self,x):\n",
    "        resdual1 = self.cov1(x)\n",
    "        resdual1 = unfold_StackOverChannel(resdual1, (5, 5))\n",
    "        x = resdual1\n",
    "        x = resdual1 + self.encode1(x)\n",
    "        x = fold_tensor(x, (60, 80), (5, 5))\n",
    "        resdual2 = x + self.cov2(x) # xiu gai 的地方在这\n",
    "        resdual2 = unfold_StackOverChannel(resdual2, (5, 5))\n",
    "        x = resdual2\n",
    "        x = resdual2 + self.encode2(x)\n",
    "        x = fold_tensor(x, (60, 80), (5, 5))\n",
    "        x = self.cov_last(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nheads, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.time_attn = MultiHeadedAttention(d_model, nheads, TimeAttention, dropout)\n",
    "        self.space_attn = MultiHeadedAttention(d_model, nheads, SpaceAttention, dropout)\n",
    "        '''\n",
    "        self.net = nn.Sequential(\n",
    "                  nn.Linear(256, 25),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(25, 256),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(256, 512),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(512,256)\n",
    "                  )\n",
    "        '''\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "            )\n",
    "    '''\n",
    "    def divided_space_time_attn(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        Apply space and time attention sequentially\n",
    "        Args:\n",
    "            query (N, S, T, D)\n",
    "            key (N, S, T, D)\n",
    "            value (N, S, T, D)\n",
    "        Returns:\n",
    "            (N, S, T, D)\n",
    "        \"\"\"\n",
    "        m = self.time_attn(query, key, value, mask)\n",
    "        return self.space_attn(m, m, m, mask)\n",
    "    '''\n",
    "    def forward(self, x, mask=None):\n",
    "        # x = self.sublayer[0](x, lambda x: self.divided_space_time_attn(x, x, x, mask))\n",
    "        # x = x + self.net(x)\n",
    "        # return self.sublayer[1](x, self.feed_forward)\n",
    "        x = x + self.time_attn(x, x, x, mask)\n",
    "        x = x+ self.space_attn(x, x, x,mask)\n",
    "        x = x + self.feed_forward(x)\n",
    "        return x\n",
    "class ConvLSTMCell(nn.Module):\n",
    " \n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    " \n",
    "        super(ConvLSTMCell, self).__init__()\n",
    " \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2 # 保证在传递过程中 （h,w）不变\n",
    "        self.bias = bias\n",
    " \n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim, # i门，f门，o门，g门放在一起计算，然后在split开\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    " \n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state # 每个timestamp包含两个状态张量：h和c\n",
    " \n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis # 把输入张量与h状态张量沿通道维度串联\n",
    " \n",
    "        combined_conv = self.conv(combined) # i门，f门，o门，g门放在一起计算，然后在split开\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    " \n",
    "        c_next = f * c_cur + i * g  # c状态张量更新\n",
    "        h_next = o * torch.tanh(c_next) # h状态张量更新\n",
    " \n",
    "        return h_next, c_next # 输出当前timestamp的两个状态张量\n",
    " \n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        \"\"\"\n",
    "        初始状态张量初始化.第一个timestamp的状态张量0初始化\n",
    "        :param batch_size:\n",
    "        :param image_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        height, width = image_size\n",
    "        init_h = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "        init_c = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "        return (init_h,init_c)\n",
    " \n",
    " \n",
    "class ConvLSTM(nn.Module):\n",
    " \n",
    "    \"\"\"\n",
    "    Parameters:参数介绍\n",
    "        input_dim: Number of channels in input# 输入张量的通道数\n",
    "        hidden_dim: Number of hidden channels # h,c两个状态张量的通道数，可以是一个列表\n",
    "        kernel_size: Size of kernel in convolutions # 卷积核的尺寸，默认所有层的卷积核尺寸都是一样的,也可以设定不通lstm层的卷积核尺寸不同\n",
    "        num_layers: Number of LSTM layers stacked on each other # 卷积层的层数，需要与len(hidden_dim)相等\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers # 是否返回所有lstm层的h状态\n",
    "        Note: Will do same padding. # 相同的卷积核尺寸，相同的padding尺寸\n",
    "    Input:输入介绍\n",
    "        A tensor of size [B, T, C, H, W] or [T, B, C, H, W]# 需要是5维的\n",
    "    Output:输出介绍\n",
    "        返回的是两个列表：layer_output_list，last_state_list\n",
    "        列表0：layer_output_list--单层列表，每个元素表示一层LSTM层的输出h状态,每个元素的size=[B,T,hidden_dim,H,W]\n",
    "        列表1：last_state_list--双层列表，每个元素是一个二元列表[h,c],表示每一层的最后一个timestamp的输出状态[h,c],h.size=c.size = [B,hidden_dim,H,W]\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:使用示例\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    " \n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    " \n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers) # 转为列表\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers) # 转为列表\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers: # 判断一致性\n",
    "            raise ValueError('Inconsistent list length.')\n",
    " \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    " \n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers): # 多层LSTM设置\n",
    "            # 当前LSTM层的输入维度\n",
    "            # if i==0:\n",
    "            #     cur_input_dim = self.input_dim\n",
    "            # else:\n",
    "            #     cur_input_dim = self.hidden_dim[i - 1]\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1] # 与上等价\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    " \n",
    "        self.cell_list = nn.ModuleList(cell_list) # 把定义的多个LSTM层串联成网络模型\n",
    " \n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: 5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    " \n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            b, _, _, h, w = input_tensor.size()  # 自动获取 b,h,w信息\n",
    "            hidden_state = self._init_hidden(batch_size=b,image_size=(h, w))\n",
    " \n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    " \n",
    "        seq_len = input_tensor.size(1) # 根据输入张量获取lstm的长度\n",
    "        cur_layer_input = input_tensor\n",
    " \n",
    "        for layer_idx in range(self.num_layers): # 逐层计算\n",
    " \n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len): # 逐个stamp计算\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],cur_state=[h, c])\n",
    "                output_inner.append(h) # 第 layer_idx 层的第t个stamp的输出状态\n",
    " \n",
    "            layer_output = torch.stack(output_inner, dim=1) # 第 layer_idx 层的第所有stamp的输出状态串联\n",
    "            cur_layer_input = layer_output # 准备第layer_idx+1层的输入张量\n",
    " \n",
    "            layer_output_list.append(layer_output) # 当前层的所有timestamp的h状态的串联\n",
    "            last_state_list.append([h, c]) # 当前层的最后一个stamp的输出状态的[h,c]\n",
    " \n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    " \n",
    "        return layer_output_list, last_state_list\n",
    " \n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        \"\"\"\n",
    "        所有lstm层的第一个timestamp的输入状态0初始化\n",
    "        :param batch_size:\n",
    "        :param image_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    " \n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        \"\"\"\n",
    "        检测输入的kernel_size是否符合要求，要求kernel_size的格式是list或tuple\n",
    "        :param kernel_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    " \n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        \"\"\"\n",
    "        扩展到多层lstm情况\n",
    "        :param param:\n",
    "        :param num_layers:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n",
    "\n",
    "class Cov(nn.Module):\n",
    "     def __init__(self, intput_dim, hidden_dim, bn_dim, output_dim):# bn_dim是时间步\n",
    "          super().__init__()\n",
    "          self.cov1 = ConvLSTM(input_dim = intput_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn1 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov2 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn2 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov3 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = output_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "     def forward(self, x):\n",
    "          x,_ = self.cov1(x) # 因为上面的Covlstm返回两个值所以先用_接住第二个用不到的值\n",
    "          x = self.bn1(x[0])\n",
    "          x,_ = self.cov2(x)\n",
    "          x = self.bn2(x[0])\n",
    "          x,_ = self.cov3(x)\n",
    "          x = x[0]\n",
    "          return x\n",
    "     \n",
    "class Cov_last(nn.Module):\n",
    "     def __init__(self, intput_dim, hidden_dim, bn_dim, output_dim):# bn_dim是时间步\n",
    "          super().__init__()\n",
    "          self.cov1 = ConvLSTM(input_dim = intput_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn1 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov2 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = hidden_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "          self.bn2 = nn.BatchNorm3d(bn_dim)\n",
    "          self.cov3 = ConvLSTM(input_dim = hidden_dim,\n",
    "                     hidden_dim = output_dim,\n",
    "                     kernel_size=(3, 3),\n",
    "                     num_layers=1,\n",
    "                     batch_first=True,\n",
    "                     bias = True,\n",
    "                     return_all_layers = True)\n",
    "     def forward(self, x):\n",
    "          x,_ = self.cov1(x) # 因为上面的Covlstm返回两个值所以先用_接住第二个用不到的值\n",
    "          x = self.bn1(x[0])\n",
    "          x,_ = self.cov2(x)\n",
    "          x = self.bn2(x[0])\n",
    "          x,_ = self.cov3(x)\n",
    "          x = x[0]\n",
    "          return x[:, -1] \n",
    "     \n",
    "def unfold_StackOverChannel(img, kernel_size):\n",
    "    \"\"\"\n",
    "    divide the original image to patches, then stack the grids in each patch along the channels\n",
    "    Args:\n",
    "        img (N, *, C, H, W): the last two dimensions must be the spatial dimension\n",
    "        kernel_size: tuple of length 2\n",
    "    Returns:\n",
    "        output (N, *, C*H_k*N_k, H_output, W_output)\n",
    "    \"\"\"\n",
    "    T = img.size(1)\n",
    "    n_dim = len(img.size())\n",
    "    assert n_dim == 4 or n_dim == 5\n",
    "\n",
    "    pt = img.unfold(-2, size=kernel_size[0], step=kernel_size[0])\n",
    "    pt = pt.unfold(-2, size=kernel_size[1], step=kernel_size[1]).flatten(-2)  # (N, *, C, n0, n1, k0*k1)\n",
    "    if n_dim == 4:  # (N, C, H, W)\n",
    "        pt = pt.permute(0, 1, 4, 2, 3).flatten(1, 2)\n",
    "    elif n_dim == 5:  # (N, T, C, H, W)\n",
    "        pt = pt.permute(0, 1, 2, 5, 3, 4).flatten(2, 3)\n",
    "    assert pt.size(-3) == img.size(-3) * kernel_size[0] * kernel_size[1]\n",
    "    pt = pt.reshape(pt.size(0), T, 25, -1).permute(0, 3, 1, 2)\n",
    "    return pt     \n",
    "def fold_tensor(tensor, output_size, kernel_size):\n",
    "    \"\"\"\n",
    "    reconstruct the image from its non-overlapping patches\n",
    "    Args:\n",
    "        input tensor of size (N, *, C*k_h*k_w, n_h, n_w)\n",
    "        output_size of size(H, W), the size of the original image to be reconstructed\n",
    "        kernel_size: (k_h, k_w)\n",
    "        stride is usually equal to kernel_size for non-overlapping sliding window\n",
    "    Returns:\n",
    "        (N, *, C, H=n_h*k_h, W=n_w*k_w)\n",
    "    \"\"\"\n",
    "    tensor = tensor.reshape(-1,192,3,25)\n",
    "    T = tensor.size(2)\n",
    "    tensor = tensor.permute(0, 2, 3, 1)  # (N, T, C_, S)\n",
    "    tensor = tensor.reshape(tensor.size(0), T, 25,\n",
    "                                12, 16)\n",
    "    tensor = tensor.float()\n",
    "    n_dim = len(tensor.size())\n",
    "    assert n_dim == 4 or n_dim == 5\n",
    "    f = tensor.flatten(0, 1) if n_dim == 5 else tensor\n",
    "    folded = F.fold(f.flatten(-2), output_size=output_size, kernel_size=kernel_size, stride=kernel_size)\n",
    "    if n_dim == 5:\n",
    "        folded = folded.reshape(tensor.size(0), tensor.size(1), *folded.size()[1:])\n",
    "    return folded.reshape(-1,T,5,60,80)\n",
    "\n",
    "\n",
    "def TimeAttention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    attention over the time axis\n",
    "    Args:\n",
    "        query, key, value: linearly-transformed query, key, value (N, h, S, T, D)\n",
    "        mask: of size (T (query), T (key)) specifying locations (which key) the query can and cannot attend to\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)  # (N, h, S, T, T)\n",
    "    if mask is not None:\n",
    "        assert mask.dtype == torch.bool\n",
    "        assert len(mask.size()) == 2\n",
    "        scores = scores.masked_fill(mask[None, None, None], float(\"-inf\"))\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value)  # (N, h, S, T, D)\n",
    "\n",
    "\n",
    "def SpaceAttention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    attention over the two space axes\n",
    "    Args:\n",
    "        query, key, value: linearly-transformed query, key, value (N, h, S, T, D)\n",
    "        mask: None (space attention does not need mask), this argument is intentionally set for consistency\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    query = query.transpose(2, 3)  # (N, h, T, S, D)\n",
    "    key = key.transpose(2, 3)  # (N, h, T, S, D)\n",
    "    value = value.transpose(2, 3)  # (N, h, T, S, D)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)  # (N, h, T, S, S)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value).transpose(2, 3)  # (N, h, S, T_q, D)\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, nheads, attn, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model % nheads == 0\n",
    "        self.d_k = d_model // nheads\n",
    "        self.nheads = nheads\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.attn = attn\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Transform the query, key, value into different heads, then apply the attention in parallel\n",
    "        Args:\n",
    "            query, key, value: size (N, S, T, D)\n",
    "        Returns:\n",
    "            (N, S, T, D)\n",
    "        \"\"\"\n",
    "        nbatches = query.size(0)\n",
    "        nspace = query.size(1)\n",
    "        ntime = query.size(2)\n",
    "        # (N, h, S, T, d_k)\n",
    "        query, key, value = \\\n",
    "            [l(x).view(x.size(0), x.size(1), x.size(2), self.nheads, self.d_k).permute(0, 3, 1, 2, 4)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # (N, h, S, T, d_k)\n",
    "        x = self.attn(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # (N, S, T, D)\n",
    "        x = x.permute(0, 2, 3, 1, 4).contiguous() \\\n",
    "             .view(nbatches, nspace, ntime, self.nheads * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dropout,attn):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(192, 192) for _ in range(3)])\n",
    "        self.attn = attn\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query, key, value = \\\n",
    "        [l(x)\n",
    "        for l, x in zip(self.linears, (query, key, value))]\n",
    "        x = self.attn(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"\"\"\n",
    "    learning rate warmup and decay\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "               (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, configs):\n",
    "        self.configs = configs\n",
    "        self.device = configs.device\n",
    "        torch.manual_seed(5)\n",
    "        self.network  = covlstmformer(configs).to(configs.device)\n",
    "        adam = torch.optim.Adam(self.network.parameters(), lr=0, weight_decay=configs.weight_decay)\n",
    "        factor = math.sqrt(configs.d_model * configs.warmup) * 0.0014\n",
    "        self.opt = NoamOpt(configs.d_model, factor, warmup=configs.warmup, optimizer=adam)\n",
    "\n",
    "\n",
    "    def loss_sst(self, y_pred, y_true):\n",
    "        # y_pred/y_true (N, 26, 24, 48)\n",
    "        rmse = torch.mean((y_pred - y_true) ** 2, dim=[2, 3])\n",
    "        rmse = torch.sum(rmse.sqrt().mean(dim=0))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "\n",
    "    def train_once(self, input_sst, sst_true, ssr_ratio):\n",
    "        sst_pred = self.network(input_sst.float().to(self.device))\n",
    "        self.opt.optimizer.zero_grad()\n",
    "        loss_sst = self.loss_sst(sst_pred, sst_true.float().to(self.device))\n",
    "        # loss_nino = self.loss_nino(nino_pred, nino_true.float().to(self.device))\n",
    "        loss_sst.backward()\n",
    "        if configs.gradient_clipping:\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), configs.clipping_threshold)\n",
    "        self.opt.step()\n",
    "        return loss_sst.item()\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        # nino_pred = []\n",
    "        sst_pred = []\n",
    "        with torch.no_grad():\n",
    "            for input_sst, sst_true, in dataloader_test:\n",
    "                sst = self.network(input_sst.float().to(self.device))\n",
    "                # nino_pred.append(nino)\n",
    "                sst_pred.append(sst)\n",
    "\n",
    "        return torch.cat(sst_pred, dim=0)\n",
    "\n",
    "    def infer(self, dataset, dataloader):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            sst_pred = self.test(dataloader)\n",
    "            # nino_true = torch.from_numpy(dataset.target_nino).float().to(self.device)\n",
    "            sst_true = torch.from_numpy(dataset.target_sst).float().to(self.device)\n",
    "            # sc = self.score(nino_pred, nino_true)\n",
    "#             print(sst_pred.shape)\n",
    "#             print(sst_true.shape)\n",
    "            loss_sst = self.loss_sst(sst_pred, sst_true).item()\n",
    "            # loss_nino = self.loss_nino(nino_pred, nino_true).item()\n",
    "        return loss_sst\n",
    "\n",
    "    def infer_test(self, dataset, dataloader):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            sst_pred = self.test(dataloader)\n",
    "            # nino_true = torch.from_numpy(dataset.target_nino).float().to(self.device)\n",
    "            sst_true = torch.from_numpy(dataset.target_sst).float().to(self.device)\n",
    "            # sc = self.score(nino_pred, nino_true)\n",
    "            loss_sst = self.loss_sst(sst_pred, sst_true).item()\n",
    "            # loss_nino = self.loss_nino(nino_pred, nino_true).item()\n",
    "        return loss_sst, sst_pred, sst_true\n",
    "\n",
    "    def train(self, dataset_train, dataset_eval, chk_path):\n",
    "        torch.manual_seed(0)\n",
    "        print('loading train dataloader')\n",
    "        dataloader_train = DataLoader(dataset_train, batch_size=self.configs.batch_size, shuffle=True)\n",
    "        print('loading eval dataloader')\n",
    "        dataloader_eval = DataLoader(dataset_eval, batch_size=self.configs.batch_size_test, shuffle=False)\n",
    "\n",
    "        count = 0\n",
    "        best = math.inf\n",
    "        ssr_ratio = 1\n",
    "        for i in range(self.configs.num_epochs):\n",
    "            print('\\nepoch: {0}'.format(i + 1))\n",
    "            # train\n",
    "            self.network.train()\n",
    "            for j, (input_sst, sst_true) in enumerate(dataloader_train):\n",
    "                if ssr_ratio > 0:\n",
    "                    ssr_ratio = max(ssr_ratio - self.configs.ssr_decay_rate, 0)\n",
    "                loss_sst = self.train_once(input_sst, sst_true, ssr_ratio)  # y_pred for one batch\n",
    "\n",
    "                if j % self.configs.display_interval == 0:\n",
    "\n",
    "                    print('batch training loss: {:.5f}, ssr: {:.5f}, lr: {:.5f}'.format(loss_sst, ssr_ratio, self.opt.rate()))\n",
    "\n",
    "                # increase the number of evaluations in order not to miss the optimal point\n",
    "                # which is feasible because of the less training time of timesformer\n",
    "                if (i + 1 >= 9) and (j + 1) % 300 == 0:\n",
    "                    loss_sst_eval = self.infer(dataset=dataset_eval, dataloader=dataloader_eval)\n",
    "                    print('epoch eval loss: sc: {:.4f}'.format(loss_sst_eval))\n",
    "                    if loss_sst_eval < best:\n",
    "                        self.save_model(chk_path)\n",
    "                        best = loss_sst_eval\n",
    "                        count = 0\n",
    "\n",
    "            # evaluation\n",
    "            loss_sst_eval = self.infer(dataset=dataset_eval, dataloader=dataloader_eval)\n",
    "            print('epoch eval loss:\\nsst: {:.2f}'.format(loss_sst_eval))\n",
    "            if loss_sst_eval >= best:\n",
    "                count += 1\n",
    "                print('eval score is not improved for {} epoch'.format(count))\n",
    "            else:\n",
    "                count = 0\n",
    "                print('eval score is improved from {:.5f} to {:.5f}, saving model'.format(best, loss_sst_eval))\n",
    "                self.save_model(chk_path)\n",
    "                best = loss_sst_eval\n",
    "\n",
    "            if count == self.configs.patience:\n",
    "                print('early stopping reached, best score is {:5f}'.format(best))\n",
    "                break\n",
    "\n",
    "    def save_configs(self, config_path):\n",
    "        with open(config_path, 'wb') as path:\n",
    "            pickle.dump(self.configs, path)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save({'net': self.network.state_dict(),\n",
    "                    'optimizer': self.opt.optimizer.state_dict()}, path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cmip_dataset(Dataset):\n",
    "    def __init__(self, datax,datay):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_sst = datax\n",
    "        self.target_sst = datay\n",
    "\n",
    "\n",
    "    def GetDataShape(self):\n",
    "        return {'sst input': self.input_sst.shape,\n",
    "                'sst target': self.target_sst.shape}\n",
    "\n",
    "    def __len__(self,):\n",
    "        return self.input_sst.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_sst[idx], self.target_sst[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst input': (166, 3, 5, 60, 80), 'sst target': (166, 1, 60, 80)}\n"
     ]
    }
   ],
   "source": [
    "dataset_train = cmip_dataset(X_train,true_train)\n",
    "print(dataset_train.GetDataShape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst input': (12, 3, 5, 60, 80), 'sst target': (12, 1, 60, 80)}\n"
     ]
    }
   ],
   "source": [
    "dataset_eval = cmip_dataset(X_eval,true_eval)\n",
    "print(dataset_eval.GetDataShape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(configs)\n",
    "#trainer.save_configs('config_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train dataloader\n",
      "loading eval dataloader\n",
      "\n",
      "epoch: 1\n",
      "batch training loss: 0.50818, ssr: 0.99997, lr: 0.00000\n",
      "epoch eval loss:\n",
      "sst: 0.52\n",
      "eval score is improved from inf to 0.52345, saving model\n",
      "\n",
      "epoch: 2\n",
      "batch training loss: 0.51119, ssr: 0.99979, lr: 0.00000\n",
      "epoch eval loss:\n",
      "sst: 0.52\n",
      "eval score is improved from 0.52345 to 0.52304, saving model\n",
      "\n",
      "epoch: 3\n",
      "batch training loss: 0.49602, ssr: 0.99961, lr: 0.00001\n",
      "epoch eval loss:\n",
      "sst: 0.52\n",
      "eval score is improved from 0.52304 to 0.52088, saving model\n",
      "\n",
      "epoch: 4\n",
      "batch training loss: 0.48438, ssr: 0.99943, lr: 0.00001\n",
      "epoch eval loss:\n",
      "sst: 0.52\n",
      "eval score is improved from 0.52088 to 0.51543, saving model\n",
      "\n",
      "epoch: 5\n",
      "batch training loss: 0.47274, ssr: 0.99925, lr: 0.00001\n",
      "epoch eval loss:\n",
      "sst: 0.50\n",
      "eval score is improved from 0.51543 to 0.50441, saving model\n",
      "\n",
      "epoch: 6\n",
      "batch training loss: 0.45646, ssr: 0.99907, lr: 0.00001\n",
      "epoch eval loss:\n",
      "sst: 0.49\n",
      "eval score is improved from 0.50441 to 0.48551, saving model\n",
      "\n",
      "epoch: 7\n",
      "batch training loss: 0.42985, ssr: 0.99889, lr: 0.00002\n",
      "epoch eval loss:\n",
      "sst: 0.46\n",
      "eval score is improved from 0.48551 to 0.45706, saving model\n",
      "\n",
      "epoch: 8\n",
      "batch training loss: 0.40768, ssr: 0.99871, lr: 0.00002\n",
      "epoch eval loss:\n",
      "sst: 0.42\n",
      "eval score is improved from 0.45706 to 0.41696, saving model\n",
      "\n",
      "epoch: 9\n",
      "batch training loss: 0.37409, ssr: 0.99853, lr: 0.00002\n",
      "epoch eval loss:\n",
      "sst: 0.36\n",
      "eval score is improved from 0.41696 to 0.36397, saving model\n",
      "\n",
      "epoch: 10\n",
      "batch training loss: 0.32864, ssr: 0.99835, lr: 0.00003\n",
      "epoch eval loss:\n",
      "sst: 0.30\n",
      "eval score is improved from 0.36397 to 0.30478, saving model\n",
      "\n",
      "epoch: 11\n",
      "batch training loss: 0.29283, ssr: 0.99817, lr: 0.00003\n",
      "epoch eval loss:\n",
      "sst: 0.25\n",
      "eval score is improved from 0.30478 to 0.25126, saving model\n",
      "\n",
      "epoch: 12\n",
      "batch training loss: 0.26057, ssr: 0.99799, lr: 0.00003\n",
      "epoch eval loss:\n",
      "sst: 0.21\n",
      "eval score is improved from 0.25126 to 0.20854, saving model\n",
      "\n",
      "epoch: 13\n",
      "batch training loss: 0.22545, ssr: 0.99781, lr: 0.00003\n",
      "epoch eval loss:\n",
      "sst: 0.18\n",
      "eval score is improved from 0.20854 to 0.17543, saving model\n",
      "\n",
      "epoch: 14\n",
      "batch training loss: 0.19748, ssr: 0.99763, lr: 0.00004\n",
      "epoch eval loss:\n",
      "sst: 0.15\n",
      "eval score is improved from 0.17543 to 0.15277, saving model\n",
      "\n",
      "epoch: 15\n",
      "batch training loss: 0.16035, ssr: 0.99745, lr: 0.00004\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.15277 to 0.14341, saving model\n",
      "\n",
      "epoch: 16\n",
      "batch training loss: 0.14419, ssr: 0.99727, lr: 0.00004\n",
      "epoch eval loss:\n",
      "sst: 0.15\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 17\n",
      "batch training loss: 0.13563, ssr: 0.99709, lr: 0.00005\n",
      "epoch eval loss:\n",
      "sst: 0.15\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 18\n",
      "batch training loss: 0.12856, ssr: 0.99691, lr: 0.00005\n",
      "epoch eval loss:\n",
      "sst: 0.16\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 19\n",
      "batch training loss: 0.12394, ssr: 0.99673, lr: 0.00005\n",
      "epoch eval loss:\n",
      "sst: 0.16\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 20\n",
      "batch training loss: 0.12502, ssr: 0.99655, lr: 0.00005\n",
      "epoch eval loss:\n",
      "sst: 0.16\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 21\n",
      "batch training loss: 0.12594, ssr: 0.99637, lr: 0.00006\n",
      "epoch eval loss:\n",
      "sst: 0.15\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 22\n",
      "batch training loss: 0.12254, ssr: 0.99619, lr: 0.00006\n",
      "epoch eval loss:\n",
      "sst: 0.15\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 23\n",
      "batch training loss: 0.11808, ssr: 0.99601, lr: 0.00006\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 24\n",
      "batch training loss: 0.12141, ssr: 0.99583, lr: 0.00006\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.14341 to 0.14280, saving model\n",
      "\n",
      "epoch: 25\n",
      "batch training loss: 0.11817, ssr: 0.99565, lr: 0.00007\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.14280 to 0.14263, saving model\n",
      "\n",
      "epoch: 26\n",
      "batch training loss: 0.12142, ssr: 0.99547, lr: 0.00007\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 27\n",
      "batch training loss: 0.11300, ssr: 0.99529, lr: 0.00007\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.14263 to 0.14255, saving model\n",
      "\n",
      "epoch: 28\n",
      "batch training loss: 0.11845, ssr: 0.99511, lr: 0.00008\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.14255 to 0.14149, saving model\n",
      "\n",
      "epoch: 29\n",
      "batch training loss: 0.12026, ssr: 0.99493, lr: 0.00008\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.14149 to 0.13992, saving model\n",
      "\n",
      "epoch: 30\n",
      "batch training loss: 0.11986, ssr: 0.99475, lr: 0.00008\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.13992 to 0.13893, saving model\n",
      "\n",
      "epoch: 31\n",
      "batch training loss: 0.12294, ssr: 0.99457, lr: 0.00008\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.13893 to 0.13880, saving model\n",
      "\n",
      "epoch: 32\n",
      "batch training loss: 0.11411, ssr: 0.99439, lr: 0.00009\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.13880 to 0.13751, saving model\n",
      "\n",
      "epoch: 33\n",
      "batch training loss: 0.10409, ssr: 0.99421, lr: 0.00009\n",
      "epoch eval loss:\n",
      "sst: 0.14\n",
      "eval score is improved from 0.13751 to 0.13565, saving model\n",
      "\n",
      "epoch: 34\n",
      "batch training loss: 0.11305, ssr: 0.99403, lr: 0.00009\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.13565 to 0.13416, saving model\n",
      "\n",
      "epoch: 35\n",
      "batch training loss: 0.10783, ssr: 0.99385, lr: 0.00010\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.13416 to 0.13143, saving model\n",
      "\n",
      "epoch: 36\n",
      "batch training loss: 0.11480, ssr: 0.99367, lr: 0.00010\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.13143 to 0.12971, saving model\n",
      "\n",
      "epoch: 37\n",
      "batch training loss: 0.10883, ssr: 0.99349, lr: 0.00010\n",
      "epoch eval loss:\n",
      "sst: 0.13\n",
      "eval score is improved from 0.12971 to 0.12583, saving model\n",
      "\n",
      "epoch: 38\n",
      "batch training loss: 0.10409, ssr: 0.99331, lr: 0.00010\n",
      "epoch eval loss:\n",
      "sst: 0.12\n",
      "eval score is improved from 0.12583 to 0.12164, saving model\n",
      "\n",
      "epoch: 39\n",
      "batch training loss: 0.10127, ssr: 0.99313, lr: 0.00011\n",
      "epoch eval loss:\n",
      "sst: 0.12\n",
      "eval score is improved from 0.12164 to 0.11747, saving model\n",
      "\n",
      "epoch: 40\n",
      "batch training loss: 0.10913, ssr: 0.99295, lr: 0.00011\n",
      "epoch eval loss:\n",
      "sst: 0.12\n",
      "eval score is improved from 0.11747 to 0.11610, saving model\n",
      "\n",
      "epoch: 41\n",
      "batch training loss: 0.09991, ssr: 0.99277, lr: 0.00011\n",
      "epoch eval loss:\n",
      "sst: 0.11\n",
      "eval score is improved from 0.11610 to 0.11313, saving model\n",
      "\n",
      "epoch: 42\n",
      "batch training loss: 0.10061, ssr: 0.99259, lr: 0.00012\n",
      "epoch eval loss:\n",
      "sst: 0.11\n",
      "eval score is improved from 0.11313 to 0.11057, saving model\n",
      "\n",
      "epoch: 43\n",
      "batch training loss: 0.09626, ssr: 0.99241, lr: 0.00012\n",
      "epoch eval loss:\n",
      "sst: 0.11\n",
      "eval score is improved from 0.11057 to 0.10799, saving model\n",
      "\n",
      "epoch: 44\n",
      "batch training loss: 0.09281, ssr: 0.99223, lr: 0.00012\n",
      "epoch eval loss:\n",
      "sst: 0.11\n",
      "eval score is improved from 0.10799 to 0.10629, saving model\n",
      "\n",
      "epoch: 45\n",
      "batch training loss: 0.09308, ssr: 0.99205, lr: 0.00012\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is improved from 0.10629 to 0.10436, saving model\n",
      "\n",
      "epoch: 46\n",
      "batch training loss: 0.08602, ssr: 0.99187, lr: 0.00013\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is improved from 0.10436 to 0.10275, saving model\n",
      "\n",
      "epoch: 47\n",
      "batch training loss: 0.09032, ssr: 0.99169, lr: 0.00013\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is improved from 0.10275 to 0.10081, saving model\n",
      "\n",
      "epoch: 48\n",
      "batch training loss: 0.08917, ssr: 0.99151, lr: 0.00013\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is improved from 0.10081 to 0.09924, saving model\n",
      "\n",
      "epoch: 49\n",
      "batch training loss: 0.08669, ssr: 0.99133, lr: 0.00013\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is improved from 0.09924 to 0.09699, saving model\n",
      "\n",
      "epoch: 50\n",
      "batch training loss: 0.08675, ssr: 0.99115, lr: 0.00014\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is improved from 0.09699 to 0.09455, saving model\n",
      "\n",
      "epoch: 51\n",
      "batch training loss: 0.07931, ssr: 0.99097, lr: 0.00014\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is improved from 0.09455 to 0.09116, saving model\n",
      "\n",
      "epoch: 52\n",
      "batch training loss: 0.07965, ssr: 0.99079, lr: 0.00014\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is improved from 0.09116 to 0.08826, saving model\n",
      "\n",
      "epoch: 53\n",
      "batch training loss: 0.07530, ssr: 0.99061, lr: 0.00015\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is improved from 0.08826 to 0.08513, saving model\n",
      "\n",
      "epoch: 54\n",
      "batch training loss: 0.07326, ssr: 0.99043, lr: 0.00015\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is improved from 0.08513 to 0.08068, saving model\n",
      "\n",
      "epoch: 55\n",
      "batch training loss: 0.06653, ssr: 0.99025, lr: 0.00015\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 56\n",
      "batch training loss: 0.06284, ssr: 0.99007, lr: 0.00015\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is improved from 0.08068 to 0.07431, saving model\n",
      "\n",
      "epoch: 57\n",
      "batch training loss: 0.06151, ssr: 0.98989, lr: 0.00016\n",
      "epoch eval loss:\n",
      "sst: 0.12\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 58\n",
      "batch training loss: 0.05958, ssr: 0.98971, lr: 0.00016\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 59\n",
      "batch training loss: 0.05898, ssr: 0.98953, lr: 0.00016\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 60\n",
      "batch training loss: 0.05674, ssr: 0.98935, lr: 0.00017\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is improved from 0.07431 to 0.06045, saving model\n",
      "\n",
      "epoch: 61\n",
      "batch training loss: 0.05765, ssr: 0.98917, lr: 0.00017\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 62\n",
      "batch training loss: 0.05419, ssr: 0.98899, lr: 0.00017\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 63\n",
      "batch training loss: 0.05361, ssr: 0.98881, lr: 0.00017\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is improved from 0.06045 to 0.05874, saving model\n",
      "\n",
      "epoch: 64\n",
      "batch training loss: 0.05262, ssr: 0.98863, lr: 0.00018\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 65\n",
      "batch training loss: 0.05240, ssr: 0.98845, lr: 0.00018\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is improved from 0.05874 to 0.05621, saving model\n",
      "\n",
      "epoch: 66\n",
      "batch training loss: 0.05331, ssr: 0.98827, lr: 0.00018\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 67\n",
      "batch training loss: 0.05252, ssr: 0.98809, lr: 0.00019\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 68\n",
      "batch training loss: 0.04977, ssr: 0.98791, lr: 0.00019\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is improved from 0.05621 to 0.05605, saving model\n",
      "\n",
      "epoch: 69\n",
      "batch training loss: 0.04851, ssr: 0.98773, lr: 0.00019\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 70\n",
      "batch training loss: 0.04914, ssr: 0.98755, lr: 0.00019\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 71\n",
      "batch training loss: 0.04819, ssr: 0.98737, lr: 0.00020\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is improved from 0.05605 to 0.05044, saving model\n",
      "\n",
      "epoch: 72\n",
      "batch training loss: 0.04640, ssr: 0.98719, lr: 0.00020\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 73\n",
      "batch training loss: 0.04616, ssr: 0.98701, lr: 0.00020\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 74\n",
      "batch training loss: 0.04693, ssr: 0.98683, lr: 0.00020\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 75\n",
      "batch training loss: 0.04553, ssr: 0.98665, lr: 0.00021\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 76\n",
      "batch training loss: 0.04520, ssr: 0.98647, lr: 0.00021\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 77\n",
      "batch training loss: 0.04701, ssr: 0.98629, lr: 0.00021\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is improved from 0.05044 to 0.05017, saving model\n",
      "\n",
      "epoch: 78\n",
      "batch training loss: 0.04494, ssr: 0.98611, lr: 0.00022\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 79\n",
      "batch training loss: 0.04504, ssr: 0.98593, lr: 0.00022\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 80\n",
      "batch training loss: 0.04680, ssr: 0.98575, lr: 0.00022\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 81\n",
      "batch training loss: 0.04473, ssr: 0.98557, lr: 0.00022\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 82\n",
      "batch training loss: 0.04546, ssr: 0.98539, lr: 0.00023\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is improved from 0.05017 to 0.04727, saving model\n",
      "\n",
      "epoch: 83\n",
      "batch training loss: 0.04487, ssr: 0.98521, lr: 0.00023\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 84\n",
      "batch training loss: 0.04429, ssr: 0.98503, lr: 0.00023\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is improved from 0.04727 to 0.04530, saving model\n",
      "\n",
      "epoch: 85\n",
      "batch training loss: 0.04398, ssr: 0.98485, lr: 0.00024\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 86\n",
      "batch training loss: 0.04472, ssr: 0.98467, lr: 0.00024\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 87\n",
      "batch training loss: 0.04412, ssr: 0.98449, lr: 0.00024\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 88\n",
      "batch training loss: 0.04391, ssr: 0.98431, lr: 0.00024\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 89\n",
      "batch training loss: 0.04197, ssr: 0.98413, lr: 0.00025\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 90\n",
      "batch training loss: 0.04324, ssr: 0.98395, lr: 0.00025\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 91\n",
      "batch training loss: 0.04219, ssr: 0.98377, lr: 0.00025\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 92\n",
      "batch training loss: 0.04229, ssr: 0.98359, lr: 0.00026\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 93\n",
      "batch training loss: 0.04173, ssr: 0.98341, lr: 0.00026\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 94\n",
      "batch training loss: 0.04203, ssr: 0.98323, lr: 0.00026\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 95\n",
      "batch training loss: 0.04364, ssr: 0.98305, lr: 0.00026\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 96\n",
      "batch training loss: 0.04064, ssr: 0.98287, lr: 0.00027\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 97\n",
      "batch training loss: 0.04265, ssr: 0.98269, lr: 0.00027\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04530 to 0.04321, saving model\n",
      "\n",
      "epoch: 98\n",
      "batch training loss: 0.04594, ssr: 0.98251, lr: 0.00027\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 99\n",
      "batch training loss: 0.04240, ssr: 0.98233, lr: 0.00027\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 100\n",
      "batch training loss: 0.04154, ssr: 0.98215, lr: 0.00028\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 101\n",
      "batch training loss: 0.04291, ssr: 0.98197, lr: 0.00028\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 102\n",
      "batch training loss: 0.04070, ssr: 0.98179, lr: 0.00028\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04321 to 0.04278, saving model\n",
      "\n",
      "epoch: 103\n",
      "batch training loss: 0.04080, ssr: 0.98161, lr: 0.00029\n",
      "epoch eval loss:\n",
      "sst: 0.10\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 104\n",
      "batch training loss: 0.04638, ssr: 0.98143, lr: 0.00029\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 105\n",
      "batch training loss: 0.04217, ssr: 0.98125, lr: 0.00029\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 106\n",
      "batch training loss: 0.04098, ssr: 0.98107, lr: 0.00029\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 107\n",
      "batch training loss: 0.04081, ssr: 0.98089, lr: 0.00030\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 108\n",
      "batch training loss: 0.03900, ssr: 0.98071, lr: 0.00030\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 109\n",
      "batch training loss: 0.03962, ssr: 0.98053, lr: 0.00030\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 110\n",
      "batch training loss: 0.03987, ssr: 0.98035, lr: 0.00031\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 111\n",
      "batch training loss: 0.03873, ssr: 0.98017, lr: 0.00031\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04278 to 0.04277, saving model\n",
      "\n",
      "epoch: 112\n",
      "batch training loss: 0.04158, ssr: 0.97999, lr: 0.00031\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 113\n",
      "batch training loss: 0.04346, ssr: 0.97981, lr: 0.00031\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04277 to 0.04119, saving model\n",
      "\n",
      "epoch: 114\n",
      "batch training loss: 0.04205, ssr: 0.97963, lr: 0.00032\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 115\n",
      "batch training loss: 0.04101, ssr: 0.97945, lr: 0.00032\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 116\n",
      "batch training loss: 0.04184, ssr: 0.97927, lr: 0.00032\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 117\n",
      "batch training loss: 0.04184, ssr: 0.97909, lr: 0.00033\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 118\n",
      "batch training loss: 0.03887, ssr: 0.97891, lr: 0.00033\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 119\n",
      "batch training loss: 0.04024, ssr: 0.97873, lr: 0.00033\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 120\n",
      "batch training loss: 0.04015, ssr: 0.97855, lr: 0.00033\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 121\n",
      "batch training loss: 0.03858, ssr: 0.97837, lr: 0.00034\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 122\n",
      "batch training loss: 0.03764, ssr: 0.97819, lr: 0.00034\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 123\n",
      "batch training loss: 0.03938, ssr: 0.97801, lr: 0.00034\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 124\n",
      "batch training loss: 0.03863, ssr: 0.97783, lr: 0.00034\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 125\n",
      "batch training loss: 0.04019, ssr: 0.97765, lr: 0.00035\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 126\n",
      "batch training loss: 0.03839, ssr: 0.97747, lr: 0.00035\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 127\n",
      "batch training loss: 0.03826, ssr: 0.97729, lr: 0.00035\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04119 to 0.04093, saving model\n",
      "\n",
      "epoch: 128\n",
      "batch training loss: 0.03826, ssr: 0.97711, lr: 0.00036\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04093 to 0.04070, saving model\n",
      "\n",
      "epoch: 129\n",
      "batch training loss: 0.03881, ssr: 0.97693, lr: 0.00036\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 130\n",
      "batch training loss: 0.03615, ssr: 0.97675, lr: 0.00036\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 131\n",
      "batch training loss: 0.03827, ssr: 0.97657, lr: 0.00036\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 132\n",
      "batch training loss: 0.04011, ssr: 0.97639, lr: 0.00037\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 133\n",
      "batch training loss: 0.03826, ssr: 0.97621, lr: 0.00037\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04070 to 0.04016, saving model\n",
      "\n",
      "epoch: 134\n",
      "batch training loss: 0.03905, ssr: 0.97603, lr: 0.00037\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 135\n",
      "batch training loss: 0.03765, ssr: 0.97585, lr: 0.00038\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04016 to 0.04003, saving model\n",
      "\n",
      "epoch: 136\n",
      "batch training loss: 0.03763, ssr: 0.97567, lr: 0.00038\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 137\n",
      "batch training loss: 0.03986, ssr: 0.97549, lr: 0.00038\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 138\n",
      "batch training loss: 0.03852, ssr: 0.97531, lr: 0.00038\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 139\n",
      "batch training loss: 0.03552, ssr: 0.97513, lr: 0.00039\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 140\n",
      "batch training loss: 0.03839, ssr: 0.97495, lr: 0.00039\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 141\n",
      "batch training loss: 0.03573, ssr: 0.97477, lr: 0.00039\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 142\n",
      "batch training loss: 0.03550, ssr: 0.97459, lr: 0.00040\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 143\n",
      "batch training loss: 0.03716, ssr: 0.97441, lr: 0.00040\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 144\n",
      "batch training loss: 0.03531, ssr: 0.97423, lr: 0.00040\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 145\n",
      "batch training loss: 0.03630, ssr: 0.97405, lr: 0.00040\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 146\n",
      "batch training loss: 0.03557, ssr: 0.97387, lr: 0.00041\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 147\n",
      "batch training loss: 0.03543, ssr: 0.97369, lr: 0.00041\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 148\n",
      "batch training loss: 0.03558, ssr: 0.97351, lr: 0.00041\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 149\n",
      "batch training loss: 0.03516, ssr: 0.97333, lr: 0.00041\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 150\n",
      "batch training loss: 0.03488, ssr: 0.97315, lr: 0.00042\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 151\n",
      "batch training loss: 0.03548, ssr: 0.97297, lr: 0.00042\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 152\n",
      "batch training loss: 0.03738, ssr: 0.97279, lr: 0.00042\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.04003 to 0.03687, saving model\n",
      "\n",
      "epoch: 153\n",
      "batch training loss: 0.03787, ssr: 0.97261, lr: 0.00043\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 154\n",
      "batch training loss: 0.03291, ssr: 0.97243, lr: 0.00043\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 155\n",
      "batch training loss: 0.03465, ssr: 0.97225, lr: 0.00043\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 156\n",
      "batch training loss: 0.03397, ssr: 0.97207, lr: 0.00043\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 157\n",
      "batch training loss: 0.03374, ssr: 0.97189, lr: 0.00044\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 158\n",
      "batch training loss: 0.03522, ssr: 0.97171, lr: 0.00044\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 159\n",
      "batch training loss: 0.03448, ssr: 0.97153, lr: 0.00044\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 160\n",
      "batch training loss: 0.03333, ssr: 0.97135, lr: 0.00045\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 161\n",
      "batch training loss: 0.03381, ssr: 0.97117, lr: 0.00045\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 162\n",
      "batch training loss: 0.03329, ssr: 0.97099, lr: 0.00045\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 163\n",
      "batch training loss: 0.03152, ssr: 0.97081, lr: 0.00045\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 164\n",
      "batch training loss: 0.03286, ssr: 0.97063, lr: 0.00046\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is improved from 0.03687 to 0.03525, saving model\n",
      "\n",
      "epoch: 165\n",
      "batch training loss: 0.03223, ssr: 0.97045, lr: 0.00046\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 166\n",
      "batch training loss: 0.04012, ssr: 0.97027, lr: 0.00046\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 167\n",
      "batch training loss: 0.03192, ssr: 0.97009, lr: 0.00047\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 168\n",
      "batch training loss: 0.03271, ssr: 0.96991, lr: 0.00047\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 169\n",
      "batch training loss: 0.03458, ssr: 0.96973, lr: 0.00047\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 170\n",
      "batch training loss: 0.03289, ssr: 0.96955, lr: 0.00047\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 171\n",
      "batch training loss: 0.03163, ssr: 0.96937, lr: 0.00048\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 172\n",
      "batch training loss: 0.03078, ssr: 0.96919, lr: 0.00048\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 173\n",
      "batch training loss: 0.03353, ssr: 0.96901, lr: 0.00048\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 174\n",
      "batch training loss: 0.03061, ssr: 0.96883, lr: 0.00048\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 175\n",
      "batch training loss: 0.03236, ssr: 0.96865, lr: 0.00049\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 176\n",
      "batch training loss: 0.02985, ssr: 0.96847, lr: 0.00049\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.03525 to 0.03285, saving model\n",
      "\n",
      "epoch: 177\n",
      "batch training loss: 0.03054, ssr: 0.96829, lr: 0.00049\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 178\n",
      "batch training loss: 0.02926, ssr: 0.96811, lr: 0.00050\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 179\n",
      "batch training loss: 0.02885, ssr: 0.96793, lr: 0.00050\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 180\n",
      "batch training loss: 0.03648, ssr: 0.96775, lr: 0.00050\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 181\n",
      "batch training loss: 0.03315, ssr: 0.96757, lr: 0.00050\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 182\n",
      "batch training loss: 0.03058, ssr: 0.96739, lr: 0.00051\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 183\n",
      "batch training loss: 0.03401, ssr: 0.96721, lr: 0.00051\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 184\n",
      "batch training loss: 0.02936, ssr: 0.96703, lr: 0.00051\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 185\n",
      "batch training loss: 0.03024, ssr: 0.96685, lr: 0.00052\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 186\n",
      "batch training loss: 0.02988, ssr: 0.96667, lr: 0.00052\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.03285 to 0.03229, saving model\n",
      "\n",
      "epoch: 187\n",
      "batch training loss: 0.03137, ssr: 0.96649, lr: 0.00052\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 188\n",
      "batch training loss: 0.02817, ssr: 0.96631, lr: 0.00052\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 189\n",
      "batch training loss: 0.02735, ssr: 0.96613, lr: 0.00053\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 190\n",
      "batch training loss: 0.02693, ssr: 0.96595, lr: 0.00053\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 191\n",
      "batch training loss: 0.02848, ssr: 0.96577, lr: 0.00053\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 192\n",
      "batch training loss: 0.03106, ssr: 0.96559, lr: 0.00054\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 193\n",
      "batch training loss: 0.02723, ssr: 0.96541, lr: 0.00054\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 194\n",
      "batch training loss: 0.02826, ssr: 0.96523, lr: 0.00054\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 195\n",
      "batch training loss: 0.02641, ssr: 0.96505, lr: 0.00054\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.03229 to 0.02998, saving model\n",
      "\n",
      "epoch: 196\n",
      "batch training loss: 0.02987, ssr: 0.96487, lr: 0.00055\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 197\n",
      "batch training loss: 0.02416, ssr: 0.96469, lr: 0.00055\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 198\n",
      "batch training loss: 0.02975, ssr: 0.96451, lr: 0.00055\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02998 to 0.02886, saving model\n",
      "\n",
      "epoch: 199\n",
      "batch training loss: 0.02675, ssr: 0.96433, lr: 0.00055\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 200\n",
      "batch training loss: 0.02882, ssr: 0.96415, lr: 0.00056\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 201\n",
      "batch training loss: 0.02706, ssr: 0.96397, lr: 0.00056\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 202\n",
      "batch training loss: 0.02656, ssr: 0.96379, lr: 0.00056\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 203\n",
      "batch training loss: 0.02575, ssr: 0.96361, lr: 0.00057\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 204\n",
      "batch training loss: 0.02438, ssr: 0.96343, lr: 0.00057\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 205\n",
      "batch training loss: 0.02461, ssr: 0.96325, lr: 0.00057\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 206\n",
      "batch training loss: 0.02319, ssr: 0.96307, lr: 0.00057\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 207\n",
      "batch training loss: 0.02327, ssr: 0.96289, lr: 0.00058\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 208\n",
      "batch training loss: 0.02460, ssr: 0.96271, lr: 0.00058\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 209\n",
      "batch training loss: 0.02279, ssr: 0.96253, lr: 0.00058\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 210\n",
      "batch training loss: 0.02258, ssr: 0.96235, lr: 0.00059\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 211\n",
      "batch training loss: 0.02187, ssr: 0.96217, lr: 0.00059\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 212\n",
      "batch training loss: 0.02400, ssr: 0.96199, lr: 0.00059\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 213\n",
      "batch training loss: 0.02466, ssr: 0.96181, lr: 0.00059\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 214\n",
      "batch training loss: 0.02453, ssr: 0.96163, lr: 0.00060\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 215\n",
      "batch training loss: 0.02427, ssr: 0.96145, lr: 0.00060\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 216\n",
      "batch training loss: 0.02087, ssr: 0.96127, lr: 0.00060\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is improved from 0.02886 to 0.02542, saving model\n",
      "\n",
      "epoch: 217\n",
      "batch training loss: 0.02796, ssr: 0.96109, lr: 0.00061\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 218\n",
      "batch training loss: 0.03218, ssr: 0.96091, lr: 0.00061\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 219\n",
      "batch training loss: 0.02377, ssr: 0.96073, lr: 0.00061\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 220\n",
      "batch training loss: 0.02167, ssr: 0.96055, lr: 0.00061\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 221\n",
      "batch training loss: 0.02127, ssr: 0.96037, lr: 0.00062\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 222\n",
      "batch training loss: 0.02380, ssr: 0.96019, lr: 0.00062\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 223\n",
      "batch training loss: 0.02237, ssr: 0.96001, lr: 0.00062\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 224\n",
      "batch training loss: 0.02066, ssr: 0.95983, lr: 0.00062\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 225\n",
      "batch training loss: 0.02086, ssr: 0.95965, lr: 0.00063\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 226\n",
      "batch training loss: 0.02724, ssr: 0.95947, lr: 0.00063\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 227\n",
      "batch training loss: 0.02255, ssr: 0.95929, lr: 0.00063\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 228\n",
      "batch training loss: 0.02418, ssr: 0.95911, lr: 0.00064\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 229\n",
      "batch training loss: 0.02371, ssr: 0.95893, lr: 0.00064\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 230\n",
      "batch training loss: 0.02273, ssr: 0.95875, lr: 0.00064\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 231\n",
      "batch training loss: 0.02099, ssr: 0.95857, lr: 0.00064\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 232\n",
      "batch training loss: 0.01953, ssr: 0.95839, lr: 0.00065\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.02542 to 0.02420, saving model\n",
      "\n",
      "epoch: 233\n",
      "batch training loss: 0.02189, ssr: 0.95821, lr: 0.00065\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 234\n",
      "batch training loss: 0.02049, ssr: 0.95803, lr: 0.00065\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 235\n",
      "batch training loss: 0.01930, ssr: 0.95785, lr: 0.00066\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 236\n",
      "batch training loss: 0.01894, ssr: 0.95767, lr: 0.00066\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 237\n",
      "batch training loss: 0.01888, ssr: 0.95749, lr: 0.00066\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 238\n",
      "batch training loss: 0.01860, ssr: 0.95731, lr: 0.00066\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 239\n",
      "batch training loss: 0.01858, ssr: 0.95713, lr: 0.00067\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 240\n",
      "batch training loss: 0.01779, ssr: 0.95695, lr: 0.00067\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 241\n",
      "batch training loss: 0.01789, ssr: 0.95677, lr: 0.00067\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 242\n",
      "batch training loss: 0.01884, ssr: 0.95659, lr: 0.00068\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 243\n",
      "batch training loss: 0.02015, ssr: 0.95641, lr: 0.00068\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 244\n",
      "batch training loss: 0.01765, ssr: 0.95623, lr: 0.00068\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 245\n",
      "batch training loss: 0.01721, ssr: 0.95605, lr: 0.00068\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 246\n",
      "batch training loss: 0.01838, ssr: 0.95587, lr: 0.00069\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 247\n",
      "batch training loss: 0.01751, ssr: 0.95569, lr: 0.00069\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 248\n",
      "batch training loss: 0.01783, ssr: 0.95551, lr: 0.00069\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 249\n",
      "batch training loss: 0.01799, ssr: 0.95533, lr: 0.00069\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 250\n",
      "batch training loss: 0.01940, ssr: 0.95515, lr: 0.00070\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 251\n",
      "batch training loss: 0.01820, ssr: 0.95497, lr: 0.00070\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 252\n",
      "batch training loss: 0.01785, ssr: 0.95479, lr: 0.00070\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 253\n",
      "batch training loss: 0.01723, ssr: 0.95461, lr: 0.00071\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 254\n",
      "batch training loss: 0.01728, ssr: 0.95443, lr: 0.00071\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 255\n",
      "batch training loss: 0.01729, ssr: 0.95425, lr: 0.00071\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 256\n",
      "batch training loss: 0.01848, ssr: 0.95407, lr: 0.00071\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.02420 to 0.02150, saving model\n",
      "\n",
      "epoch: 257\n",
      "batch training loss: 0.02088, ssr: 0.95389, lr: 0.00072\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 258\n",
      "batch training loss: 0.01802, ssr: 0.95371, lr: 0.00072\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 259\n",
      "batch training loss: 0.01682, ssr: 0.95353, lr: 0.00072\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 260\n",
      "batch training loss: 0.01670, ssr: 0.95335, lr: 0.00073\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 261\n",
      "batch training loss: 0.01765, ssr: 0.95317, lr: 0.00073\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 262\n",
      "batch training loss: 0.01720, ssr: 0.95299, lr: 0.00073\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.02150 to 0.02129, saving model\n",
      "\n",
      "epoch: 263\n",
      "batch training loss: 0.01910, ssr: 0.95281, lr: 0.00073\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 264\n",
      "batch training loss: 0.01798, ssr: 0.95263, lr: 0.00074\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 265\n",
      "batch training loss: 0.02153, ssr: 0.95245, lr: 0.00074\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 266\n",
      "batch training loss: 0.01690, ssr: 0.95227, lr: 0.00074\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 267\n",
      "batch training loss: 0.01844, ssr: 0.95209, lr: 0.00075\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 268\n",
      "batch training loss: 0.02204, ssr: 0.95191, lr: 0.00075\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 269\n",
      "batch training loss: 0.01842, ssr: 0.95173, lr: 0.00075\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 270\n",
      "batch training loss: 0.01984, ssr: 0.95155, lr: 0.00075\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 271\n",
      "batch training loss: 0.01781, ssr: 0.95137, lr: 0.00076\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 272\n",
      "batch training loss: 0.02189, ssr: 0.95119, lr: 0.00076\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 273\n",
      "batch training loss: 0.01598, ssr: 0.95101, lr: 0.00076\n",
      "epoch eval loss:\n",
      "sst: 0.09\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 274\n",
      "batch training loss: 0.02244, ssr: 0.95083, lr: 0.00076\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 275\n",
      "batch training loss: 0.02508, ssr: 0.95065, lr: 0.00077\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 276\n",
      "batch training loss: 0.01676, ssr: 0.95047, lr: 0.00077\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 277\n",
      "batch training loss: 0.01858, ssr: 0.95029, lr: 0.00077\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 278\n",
      "batch training loss: 0.01747, ssr: 0.95011, lr: 0.00078\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 279\n",
      "batch training loss: 0.01642, ssr: 0.94993, lr: 0.00078\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 280\n",
      "batch training loss: 0.01701, ssr: 0.94975, lr: 0.00078\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 281\n",
      "batch training loss: 0.01632, ssr: 0.94957, lr: 0.00078\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 282\n",
      "batch training loss: 0.01978, ssr: 0.94939, lr: 0.00079\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 283\n",
      "batch training loss: 0.01815, ssr: 0.94921, lr: 0.00079\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 284\n",
      "batch training loss: 0.02039, ssr: 0.94903, lr: 0.00079\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 285\n",
      "batch training loss: 0.01538, ssr: 0.94885, lr: 0.00080\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 286\n",
      "batch training loss: 0.02026, ssr: 0.94867, lr: 0.00080\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 287\n",
      "batch training loss: 0.01808, ssr: 0.94849, lr: 0.00080\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 288\n",
      "batch training loss: 0.01736, ssr: 0.94831, lr: 0.00080\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 289\n",
      "batch training loss: 0.01569, ssr: 0.94813, lr: 0.00081\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 290\n",
      "batch training loss: 0.01617, ssr: 0.94795, lr: 0.00081\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 291\n",
      "batch training loss: 0.01662, ssr: 0.94777, lr: 0.00081\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 292\n",
      "batch training loss: 0.01841, ssr: 0.94759, lr: 0.00082\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 30 epoch\n",
      "\n",
      "epoch: 293\n",
      "batch training loss: 0.01855, ssr: 0.94741, lr: 0.00082\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 31 epoch\n",
      "\n",
      "epoch: 294\n",
      "batch training loss: 0.01514, ssr: 0.94723, lr: 0.00082\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 32 epoch\n",
      "\n",
      "epoch: 295\n",
      "batch training loss: 0.01630, ssr: 0.94705, lr: 0.00082\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 33 epoch\n",
      "\n",
      "epoch: 296\n",
      "batch training loss: 0.01550, ssr: 0.94687, lr: 0.00083\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 34 epoch\n",
      "\n",
      "epoch: 297\n",
      "batch training loss: 0.01740, ssr: 0.94669, lr: 0.00083\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 35 epoch\n",
      "\n",
      "epoch: 298\n",
      "batch training loss: 0.01509, ssr: 0.94651, lr: 0.00083\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 36 epoch\n",
      "\n",
      "epoch: 299\n",
      "batch training loss: 0.01744, ssr: 0.94633, lr: 0.00083\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 37 epoch\n",
      "\n",
      "epoch: 300\n",
      "batch training loss: 0.01883, ssr: 0.94615, lr: 0.00084\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 38 epoch\n",
      "\n",
      "epoch: 301\n",
      "batch training loss: 0.01981, ssr: 0.94597, lr: 0.00084\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 39 epoch\n",
      "\n",
      "epoch: 302\n",
      "batch training loss: 0.01731, ssr: 0.94579, lr: 0.00084\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 40 epoch\n",
      "\n",
      "epoch: 303\n",
      "batch training loss: 0.01496, ssr: 0.94561, lr: 0.00085\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 41 epoch\n",
      "\n",
      "epoch: 304\n",
      "batch training loss: 0.01999, ssr: 0.94543, lr: 0.00085\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 42 epoch\n",
      "\n",
      "epoch: 305\n",
      "batch training loss: 0.01847, ssr: 0.94525, lr: 0.00085\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 43 epoch\n",
      "\n",
      "epoch: 306\n",
      "batch training loss: 0.02032, ssr: 0.94507, lr: 0.00085\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 44 epoch\n",
      "\n",
      "epoch: 307\n",
      "batch training loss: 0.01671, ssr: 0.94489, lr: 0.00086\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 45 epoch\n",
      "\n",
      "epoch: 308\n",
      "batch training loss: 0.01827, ssr: 0.94471, lr: 0.00086\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 46 epoch\n",
      "\n",
      "epoch: 309\n",
      "batch training loss: 0.01606, ssr: 0.94453, lr: 0.00086\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 47 epoch\n",
      "\n",
      "epoch: 310\n",
      "batch training loss: 0.01671, ssr: 0.94435, lr: 0.00087\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 48 epoch\n",
      "\n",
      "epoch: 311\n",
      "batch training loss: 0.01979, ssr: 0.94417, lr: 0.00087\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 49 epoch\n",
      "\n",
      "epoch: 312\n",
      "batch training loss: 0.01856, ssr: 0.94399, lr: 0.00087\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 50 epoch\n",
      "\n",
      "epoch: 313\n",
      "batch training loss: 0.01652, ssr: 0.94381, lr: 0.00087\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 51 epoch\n",
      "\n",
      "epoch: 314\n",
      "batch training loss: 0.01491, ssr: 0.94363, lr: 0.00088\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 52 epoch\n",
      "\n",
      "epoch: 315\n",
      "batch training loss: 0.01659, ssr: 0.94345, lr: 0.00088\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 53 epoch\n",
      "\n",
      "epoch: 316\n",
      "batch training loss: 0.01413, ssr: 0.94327, lr: 0.00088\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 54 epoch\n",
      "\n",
      "epoch: 317\n",
      "batch training loss: 0.01547, ssr: 0.94309, lr: 0.00089\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 55 epoch\n",
      "\n",
      "epoch: 318\n",
      "batch training loss: 0.01437, ssr: 0.94291, lr: 0.00089\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 56 epoch\n",
      "\n",
      "epoch: 319\n",
      "batch training loss: 0.01713, ssr: 0.94273, lr: 0.00089\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 57 epoch\n",
      "\n",
      "epoch: 320\n",
      "batch training loss: 0.01463, ssr: 0.94255, lr: 0.00089\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 58 epoch\n",
      "\n",
      "epoch: 321\n",
      "batch training loss: 0.01416, ssr: 0.94237, lr: 0.00090\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 59 epoch\n",
      "\n",
      "epoch: 322\n",
      "batch training loss: 0.01705, ssr: 0.94219, lr: 0.00090\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 60 epoch\n",
      "\n",
      "epoch: 323\n",
      "batch training loss: 0.02162, ssr: 0.94201, lr: 0.00090\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 61 epoch\n",
      "\n",
      "epoch: 324\n",
      "batch training loss: 0.01753, ssr: 0.94183, lr: 0.00090\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 62 epoch\n",
      "\n",
      "epoch: 325\n",
      "batch training loss: 0.01349, ssr: 0.94165, lr: 0.00091\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 63 epoch\n",
      "\n",
      "epoch: 326\n",
      "batch training loss: 0.01514, ssr: 0.94147, lr: 0.00091\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 64 epoch\n",
      "\n",
      "epoch: 327\n",
      "batch training loss: 0.01476, ssr: 0.94129, lr: 0.00091\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 65 epoch\n",
      "\n",
      "epoch: 328\n",
      "batch training loss: 0.01729, ssr: 0.94111, lr: 0.00092\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 66 epoch\n",
      "\n",
      "epoch: 329\n",
      "batch training loss: 0.01991, ssr: 0.94093, lr: 0.00092\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 67 epoch\n",
      "\n",
      "epoch: 330\n",
      "batch training loss: 0.01642, ssr: 0.94075, lr: 0.00092\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 68 epoch\n",
      "\n",
      "epoch: 331\n",
      "batch training loss: 0.02060, ssr: 0.94057, lr: 0.00092\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 69 epoch\n",
      "\n",
      "epoch: 332\n",
      "batch training loss: 0.01445, ssr: 0.94039, lr: 0.00093\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 70 epoch\n",
      "\n",
      "epoch: 333\n",
      "batch training loss: 0.01752, ssr: 0.94021, lr: 0.00093\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 71 epoch\n",
      "\n",
      "epoch: 334\n",
      "batch training loss: 0.01795, ssr: 0.94003, lr: 0.00093\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 72 epoch\n",
      "\n",
      "epoch: 335\n",
      "batch training loss: 0.01766, ssr: 0.93985, lr: 0.00094\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 73 epoch\n",
      "\n",
      "epoch: 336\n",
      "batch training loss: 0.01785, ssr: 0.93967, lr: 0.00094\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 74 epoch\n",
      "\n",
      "epoch: 337\n",
      "batch training loss: 0.01565, ssr: 0.93949, lr: 0.00094\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 75 epoch\n",
      "\n",
      "epoch: 338\n",
      "batch training loss: 0.01524, ssr: 0.93931, lr: 0.00094\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 76 epoch\n",
      "\n",
      "epoch: 339\n",
      "batch training loss: 0.01373, ssr: 0.93913, lr: 0.00095\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 77 epoch\n",
      "\n",
      "epoch: 340\n",
      "batch training loss: 0.01573, ssr: 0.93895, lr: 0.00095\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 78 epoch\n",
      "\n",
      "epoch: 341\n",
      "batch training loss: 0.01492, ssr: 0.93877, lr: 0.00095\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 79 epoch\n",
      "\n",
      "epoch: 342\n",
      "batch training loss: 0.01401, ssr: 0.93859, lr: 0.00096\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 80 epoch\n",
      "\n",
      "epoch: 343\n",
      "batch training loss: 0.01585, ssr: 0.93841, lr: 0.00096\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 81 epoch\n",
      "\n",
      "epoch: 344\n",
      "batch training loss: 0.01475, ssr: 0.93823, lr: 0.00096\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 82 epoch\n",
      "\n",
      "epoch: 345\n",
      "batch training loss: 0.01642, ssr: 0.93805, lr: 0.00096\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 83 epoch\n",
      "\n",
      "epoch: 346\n",
      "batch training loss: 0.01384, ssr: 0.93787, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 84 epoch\n",
      "\n",
      "epoch: 347\n",
      "batch training loss: 0.01464, ssr: 0.93769, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 85 epoch\n",
      "\n",
      "epoch: 348\n",
      "batch training loss: 0.01360, ssr: 0.93751, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 86 epoch\n",
      "\n",
      "epoch: 349\n",
      "batch training loss: 0.01363, ssr: 0.93733, lr: 0.00097\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 87 epoch\n",
      "\n",
      "epoch: 350\n",
      "batch training loss: 0.01629, ssr: 0.93715, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 88 epoch\n",
      "\n",
      "epoch: 351\n",
      "batch training loss: 0.01425, ssr: 0.93697, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 89 epoch\n",
      "\n",
      "epoch: 352\n",
      "batch training loss: 0.01836, ssr: 0.93679, lr: 0.00098\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 90 epoch\n",
      "\n",
      "epoch: 353\n",
      "batch training loss: 0.01890, ssr: 0.93661, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 91 epoch\n",
      "\n",
      "epoch: 354\n",
      "batch training loss: 0.01358, ssr: 0.93643, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 92 epoch\n",
      "\n",
      "epoch: 355\n",
      "batch training loss: 0.01352, ssr: 0.93625, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 93 epoch\n",
      "\n",
      "epoch: 356\n",
      "batch training loss: 0.01466, ssr: 0.93607, lr: 0.00099\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 94 epoch\n",
      "\n",
      "epoch: 357\n",
      "batch training loss: 0.01393, ssr: 0.93589, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 95 epoch\n",
      "\n",
      "epoch: 358\n",
      "batch training loss: 0.01443, ssr: 0.93571, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 96 epoch\n",
      "\n",
      "epoch: 359\n",
      "batch training loss: 0.01327, ssr: 0.93553, lr: 0.00100\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 97 epoch\n",
      "\n",
      "epoch: 360\n",
      "batch training loss: 0.01476, ssr: 0.93535, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 98 epoch\n",
      "\n",
      "epoch: 361\n",
      "batch training loss: 0.02113, ssr: 0.93517, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 99 epoch\n",
      "\n",
      "epoch: 362\n",
      "batch training loss: 0.01776, ssr: 0.93499, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.02129 to 0.01781, saving model\n",
      "\n",
      "epoch: 363\n",
      "batch training loss: 0.01608, ssr: 0.93481, lr: 0.00101\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 364\n",
      "batch training loss: 0.01501, ssr: 0.93463, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 365\n",
      "batch training loss: 0.01409, ssr: 0.93445, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 366\n",
      "batch training loss: 0.01783, ssr: 0.93427, lr: 0.00102\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 367\n",
      "batch training loss: 0.01766, ssr: 0.93409, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 368\n",
      "batch training loss: 0.01709, ssr: 0.93391, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 369\n",
      "batch training loss: 0.01851, ssr: 0.93373, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 370\n",
      "batch training loss: 0.01604, ssr: 0.93355, lr: 0.00103\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 371\n",
      "batch training loss: 0.01504, ssr: 0.93337, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 372\n",
      "batch training loss: 0.01269, ssr: 0.93319, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 373\n",
      "batch training loss: 0.01398, ssr: 0.93301, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 374\n",
      "batch training loss: 0.01648, ssr: 0.93283, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 375\n",
      "batch training loss: 0.01280, ssr: 0.93265, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 376\n",
      "batch training loss: 0.01912, ssr: 0.93247, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 377\n",
      "batch training loss: 0.01895, ssr: 0.93229, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 378\n",
      "batch training loss: 0.01587, ssr: 0.93211, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 379\n",
      "batch training loss: 0.01430, ssr: 0.93193, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 380\n",
      "batch training loss: 0.01743, ssr: 0.93175, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 381\n",
      "batch training loss: 0.01733, ssr: 0.93157, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 382\n",
      "batch training loss: 0.01929, ssr: 0.93139, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 383\n",
      "batch training loss: 0.01866, ssr: 0.93121, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 384\n",
      "batch training loss: 0.02021, ssr: 0.93103, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 385\n",
      "batch training loss: 0.01717, ssr: 0.93085, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 386\n",
      "batch training loss: 0.01820, ssr: 0.93067, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 387\n",
      "batch training loss: 0.01655, ssr: 0.93049, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 388\n",
      "batch training loss: 0.01353, ssr: 0.93031, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 389\n",
      "batch training loss: 0.01383, ssr: 0.93013, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 390\n",
      "batch training loss: 0.01514, ssr: 0.92995, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 391\n",
      "batch training loss: 0.01675, ssr: 0.92977, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 392\n",
      "batch training loss: 0.01721, ssr: 0.92959, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 30 epoch\n",
      "\n",
      "epoch: 393\n",
      "batch training loss: 0.01464, ssr: 0.92941, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 31 epoch\n",
      "\n",
      "epoch: 394\n",
      "batch training loss: 0.01612, ssr: 0.92923, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 32 epoch\n",
      "\n",
      "epoch: 395\n",
      "batch training loss: 0.01353, ssr: 0.92905, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 33 epoch\n",
      "\n",
      "epoch: 396\n",
      "batch training loss: 0.01488, ssr: 0.92887, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 34 epoch\n",
      "\n",
      "epoch: 397\n",
      "batch training loss: 0.01745, ssr: 0.92869, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 35 epoch\n",
      "\n",
      "epoch: 398\n",
      "batch training loss: 0.01447, ssr: 0.92851, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 36 epoch\n",
      "\n",
      "epoch: 399\n",
      "batch training loss: 0.01524, ssr: 0.92833, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 37 epoch\n",
      "\n",
      "epoch: 400\n",
      "batch training loss: 0.01525, ssr: 0.92815, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 38 epoch\n",
      "\n",
      "epoch: 401\n",
      "batch training loss: 0.01534, ssr: 0.92797, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 39 epoch\n",
      "\n",
      "epoch: 402\n",
      "batch training loss: 0.01609, ssr: 0.92779, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 40 epoch\n",
      "\n",
      "epoch: 403\n",
      "batch training loss: 0.01392, ssr: 0.92761, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 41 epoch\n",
      "\n",
      "epoch: 404\n",
      "batch training loss: 0.01194, ssr: 0.92743, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 42 epoch\n",
      "\n",
      "epoch: 405\n",
      "batch training loss: 0.01614, ssr: 0.92725, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 43 epoch\n",
      "\n",
      "epoch: 406\n",
      "batch training loss: 0.01755, ssr: 0.92707, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 44 epoch\n",
      "\n",
      "epoch: 407\n",
      "batch training loss: 0.01748, ssr: 0.92689, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 45 epoch\n",
      "\n",
      "epoch: 408\n",
      "batch training loss: 0.01342, ssr: 0.92671, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 46 epoch\n",
      "\n",
      "epoch: 409\n",
      "batch training loss: 0.01412, ssr: 0.92653, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 47 epoch\n",
      "\n",
      "epoch: 410\n",
      "batch training loss: 0.01606, ssr: 0.92635, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 48 epoch\n",
      "\n",
      "epoch: 411\n",
      "batch training loss: 0.01402, ssr: 0.92617, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 49 epoch\n",
      "\n",
      "epoch: 412\n",
      "batch training loss: 0.01179, ssr: 0.92599, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 50 epoch\n",
      "\n",
      "epoch: 413\n",
      "batch training loss: 0.01154, ssr: 0.92581, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 51 epoch\n",
      "\n",
      "epoch: 414\n",
      "batch training loss: 0.01483, ssr: 0.92563, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 52 epoch\n",
      "\n",
      "epoch: 415\n",
      "batch training loss: 0.01409, ssr: 0.92545, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 53 epoch\n",
      "\n",
      "epoch: 416\n",
      "batch training loss: 0.01368, ssr: 0.92527, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 54 epoch\n",
      "\n",
      "epoch: 417\n",
      "batch training loss: 0.01212, ssr: 0.92509, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 55 epoch\n",
      "\n",
      "epoch: 418\n",
      "batch training loss: 0.01400, ssr: 0.92491, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 56 epoch\n",
      "\n",
      "epoch: 419\n",
      "batch training loss: 0.02147, ssr: 0.92473, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 57 epoch\n",
      "\n",
      "epoch: 420\n",
      "batch training loss: 0.01791, ssr: 0.92455, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 58 epoch\n",
      "\n",
      "epoch: 421\n",
      "batch training loss: 0.01277, ssr: 0.92437, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.01781 to 0.01762, saving model\n",
      "\n",
      "epoch: 422\n",
      "batch training loss: 0.01704, ssr: 0.92419, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 423\n",
      "batch training loss: 0.01568, ssr: 0.92401, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 424\n",
      "batch training loss: 0.01236, ssr: 0.92383, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 425\n",
      "batch training loss: 0.01394, ssr: 0.92365, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 426\n",
      "batch training loss: 0.01184, ssr: 0.92347, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 427\n",
      "batch training loss: 0.01184, ssr: 0.92329, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 428\n",
      "batch training loss: 0.01755, ssr: 0.92311, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 429\n",
      "batch training loss: 0.01400, ssr: 0.92293, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 430\n",
      "batch training loss: 0.01298, ssr: 0.92275, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 431\n",
      "batch training loss: 0.01179, ssr: 0.92257, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 432\n",
      "batch training loss: 0.01256, ssr: 0.92239, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 433\n",
      "batch training loss: 0.01188, ssr: 0.92221, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 434\n",
      "batch training loss: 0.01203, ssr: 0.92203, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 435\n",
      "batch training loss: 0.01649, ssr: 0.92185, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 436\n",
      "batch training loss: 0.01214, ssr: 0.92167, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 437\n",
      "batch training loss: 0.01124, ssr: 0.92149, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 438\n",
      "batch training loss: 0.01434, ssr: 0.92131, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 439\n",
      "batch training loss: 0.01548, ssr: 0.92113, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 440\n",
      "batch training loss: 0.01363, ssr: 0.92095, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 441\n",
      "batch training loss: 0.01380, ssr: 0.92077, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 442\n",
      "batch training loss: 0.01611, ssr: 0.92059, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 443\n",
      "batch training loss: 0.01264, ssr: 0.92041, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 444\n",
      "batch training loss: 0.01114, ssr: 0.92023, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 445\n",
      "batch training loss: 0.01236, ssr: 0.92005, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 446\n",
      "batch training loss: 0.01296, ssr: 0.91987, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 447\n",
      "batch training loss: 0.01278, ssr: 0.91969, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 448\n",
      "batch training loss: 0.01407, ssr: 0.91951, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 449\n",
      "batch training loss: 0.01789, ssr: 0.91933, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 450\n",
      "batch training loss: 0.01152, ssr: 0.91915, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 451\n",
      "batch training loss: 0.01084, ssr: 0.91897, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 30 epoch\n",
      "\n",
      "epoch: 452\n",
      "batch training loss: 0.01052, ssr: 0.91879, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 31 epoch\n",
      "\n",
      "epoch: 453\n",
      "batch training loss: 0.01217, ssr: 0.91861, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 32 epoch\n",
      "\n",
      "epoch: 454\n",
      "batch training loss: 0.01081, ssr: 0.91843, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 33 epoch\n",
      "\n",
      "epoch: 455\n",
      "batch training loss: 0.01662, ssr: 0.91825, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.01762 to 0.01614, saving model\n",
      "\n",
      "epoch: 456\n",
      "batch training loss: 0.01243, ssr: 0.91807, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 457\n",
      "batch training loss: 0.01728, ssr: 0.91789, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 458\n",
      "batch training loss: 0.01551, ssr: 0.91771, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 459\n",
      "batch training loss: 0.02252, ssr: 0.91753, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 460\n",
      "batch training loss: 0.01290, ssr: 0.91735, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 461\n",
      "batch training loss: 0.01148, ssr: 0.91717, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 462\n",
      "batch training loss: 0.01532, ssr: 0.91699, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 463\n",
      "batch training loss: 0.01866, ssr: 0.91681, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 464\n",
      "batch training loss: 0.01395, ssr: 0.91663, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 465\n",
      "batch training loss: 0.01281, ssr: 0.91645, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 466\n",
      "batch training loss: 0.01513, ssr: 0.91627, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 467\n",
      "batch training loss: 0.01257, ssr: 0.91609, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 468\n",
      "batch training loss: 0.01242, ssr: 0.91591, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 469\n",
      "batch training loss: 0.01361, ssr: 0.91573, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 470\n",
      "batch training loss: 0.01165, ssr: 0.91555, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 471\n",
      "batch training loss: 0.01332, ssr: 0.91537, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 472\n",
      "batch training loss: 0.01446, ssr: 0.91519, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 473\n",
      "batch training loss: 0.01243, ssr: 0.91501, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 474\n",
      "batch training loss: 0.01363, ssr: 0.91483, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 475\n",
      "batch training loss: 0.01253, ssr: 0.91465, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 476\n",
      "batch training loss: 0.01138, ssr: 0.91447, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 477\n",
      "batch training loss: 0.01281, ssr: 0.91429, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 478\n",
      "batch training loss: 0.01072, ssr: 0.91411, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 479\n",
      "batch training loss: 0.01038, ssr: 0.91393, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 480\n",
      "batch training loss: 0.01224, ssr: 0.91375, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 481\n",
      "batch training loss: 0.01701, ssr: 0.91357, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 482\n",
      "batch training loss: 0.01099, ssr: 0.91339, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 483\n",
      "batch training loss: 0.01054, ssr: 0.91321, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 484\n",
      "batch training loss: 0.01381, ssr: 0.91303, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 485\n",
      "batch training loss: 0.01071, ssr: 0.91285, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 30 epoch\n",
      "\n",
      "epoch: 486\n",
      "batch training loss: 0.01315, ssr: 0.91267, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 31 epoch\n",
      "\n",
      "epoch: 487\n",
      "batch training loss: 0.01199, ssr: 0.91249, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 32 epoch\n",
      "\n",
      "epoch: 488\n",
      "batch training loss: 0.01154, ssr: 0.91231, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 33 epoch\n",
      "\n",
      "epoch: 489\n",
      "batch training loss: 0.01640, ssr: 0.91213, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 34 epoch\n",
      "\n",
      "epoch: 490\n",
      "batch training loss: 0.01633, ssr: 0.91195, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 35 epoch\n",
      "\n",
      "epoch: 491\n",
      "batch training loss: 0.01089, ssr: 0.91177, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 36 epoch\n",
      "\n",
      "epoch: 492\n",
      "batch training loss: 0.01116, ssr: 0.91159, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 37 epoch\n",
      "\n",
      "epoch: 493\n",
      "batch training loss: 0.01294, ssr: 0.91141, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 38 epoch\n",
      "\n",
      "epoch: 494\n",
      "batch training loss: 0.01303, ssr: 0.91123, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 39 epoch\n",
      "\n",
      "epoch: 495\n",
      "batch training loss: 0.01064, ssr: 0.91105, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 40 epoch\n",
      "\n",
      "epoch: 496\n",
      "batch training loss: 0.01244, ssr: 0.91087, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 41 epoch\n",
      "\n",
      "epoch: 497\n",
      "batch training loss: 0.01052, ssr: 0.91069, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 42 epoch\n",
      "\n",
      "epoch: 498\n",
      "batch training loss: 0.01056, ssr: 0.91051, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 43 epoch\n",
      "\n",
      "epoch: 499\n",
      "batch training loss: 0.01539, ssr: 0.91033, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 44 epoch\n",
      "\n",
      "epoch: 500\n",
      "batch training loss: 0.01126, ssr: 0.91015, lr: 0.00140\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 45 epoch\n",
      "\n",
      "epoch: 501\n",
      "batch training loss: 0.01477, ssr: 0.90997, lr: 0.00140\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 46 epoch\n",
      "\n",
      "epoch: 502\n",
      "batch training loss: 0.01416, ssr: 0.90979, lr: 0.00140\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 47 epoch\n",
      "\n",
      "epoch: 503\n",
      "batch training loss: 0.01319, ssr: 0.90961, lr: 0.00140\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 48 epoch\n",
      "\n",
      "epoch: 504\n",
      "batch training loss: 0.01368, ssr: 0.90943, lr: 0.00140\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 49 epoch\n",
      "\n",
      "epoch: 505\n",
      "batch training loss: 0.01371, ssr: 0.90925, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 50 epoch\n",
      "\n",
      "epoch: 506\n",
      "batch training loss: 0.01220, ssr: 0.90907, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 51 epoch\n",
      "\n",
      "epoch: 507\n",
      "batch training loss: 0.01089, ssr: 0.90889, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 52 epoch\n",
      "\n",
      "epoch: 508\n",
      "batch training loss: 0.00994, ssr: 0.90871, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 53 epoch\n",
      "\n",
      "epoch: 509\n",
      "batch training loss: 0.01676, ssr: 0.90853, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 54 epoch\n",
      "\n",
      "epoch: 510\n",
      "batch training loss: 0.01187, ssr: 0.90835, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 55 epoch\n",
      "\n",
      "epoch: 511\n",
      "batch training loss: 0.01265, ssr: 0.90817, lr: 0.00139\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 56 epoch\n",
      "\n",
      "epoch: 512\n",
      "batch training loss: 0.01403, ssr: 0.90799, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 57 epoch\n",
      "\n",
      "epoch: 513\n",
      "batch training loss: 0.01149, ssr: 0.90781, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 58 epoch\n",
      "\n",
      "epoch: 514\n",
      "batch training loss: 0.01148, ssr: 0.90763, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 59 epoch\n",
      "\n",
      "epoch: 515\n",
      "batch training loss: 0.01478, ssr: 0.90745, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 60 epoch\n",
      "\n",
      "epoch: 516\n",
      "batch training loss: 0.00980, ssr: 0.90727, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 61 epoch\n",
      "\n",
      "epoch: 517\n",
      "batch training loss: 0.00981, ssr: 0.90709, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 62 epoch\n",
      "\n",
      "epoch: 518\n",
      "batch training loss: 0.01208, ssr: 0.90691, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 63 epoch\n",
      "\n",
      "epoch: 519\n",
      "batch training loss: 0.01421, ssr: 0.90673, lr: 0.00138\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 64 epoch\n",
      "\n",
      "epoch: 520\n",
      "batch training loss: 0.01508, ssr: 0.90655, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 65 epoch\n",
      "\n",
      "epoch: 521\n",
      "batch training loss: 0.01111, ssr: 0.90637, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 66 epoch\n",
      "\n",
      "epoch: 522\n",
      "batch training loss: 0.01451, ssr: 0.90619, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.08\n",
      "eval score is not improved for 67 epoch\n",
      "\n",
      "epoch: 523\n",
      "batch training loss: 0.01245, ssr: 0.90601, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is improved from 0.01614 to 0.01562, saving model\n",
      "\n",
      "epoch: 524\n",
      "batch training loss: 0.01166, ssr: 0.90583, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 525\n",
      "batch training loss: 0.01688, ssr: 0.90565, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 526\n",
      "batch training loss: 0.01388, ssr: 0.90547, lr: 0.00137\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 527\n",
      "batch training loss: 0.01261, ssr: 0.90529, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 528\n",
      "batch training loss: 0.01399, ssr: 0.90511, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.01562 to 0.01486, saving model\n",
      "\n",
      "epoch: 529\n",
      "batch training loss: 0.01145, ssr: 0.90493, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 530\n",
      "batch training loss: 0.01370, ssr: 0.90475, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 531\n",
      "batch training loss: 0.01051, ssr: 0.90457, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.01486 to 0.01481, saving model\n",
      "\n",
      "epoch: 532\n",
      "batch training loss: 0.01042, ssr: 0.90439, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 533\n",
      "batch training loss: 0.01292, ssr: 0.90421, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 534\n",
      "batch training loss: 0.01159, ssr: 0.90403, lr: 0.00136\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 535\n",
      "batch training loss: 0.01519, ssr: 0.90385, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 536\n",
      "batch training loss: 0.02183, ssr: 0.90367, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 537\n",
      "batch training loss: 0.01922, ssr: 0.90349, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 538\n",
      "batch training loss: 0.01285, ssr: 0.90331, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 539\n",
      "batch training loss: 0.01343, ssr: 0.90313, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 540\n",
      "batch training loss: 0.01404, ssr: 0.90295, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 541\n",
      "batch training loss: 0.01182, ssr: 0.90277, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 542\n",
      "batch training loss: 0.01238, ssr: 0.90259, lr: 0.00135\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 543\n",
      "batch training loss: 0.01332, ssr: 0.90241, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 544\n",
      "batch training loss: 0.01196, ssr: 0.90223, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 545\n",
      "batch training loss: 0.01190, ssr: 0.90205, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 546\n",
      "batch training loss: 0.01078, ssr: 0.90187, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 547\n",
      "batch training loss: 0.01212, ssr: 0.90169, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 548\n",
      "batch training loss: 0.01316, ssr: 0.90151, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 549\n",
      "batch training loss: 0.02066, ssr: 0.90133, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 550\n",
      "batch training loss: 0.01072, ssr: 0.90115, lr: 0.00134\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 551\n",
      "batch training loss: 0.01573, ssr: 0.90097, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 552\n",
      "batch training loss: 0.01751, ssr: 0.90079, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 553\n",
      "batch training loss: 0.01691, ssr: 0.90061, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 554\n",
      "batch training loss: 0.01385, ssr: 0.90043, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 555\n",
      "batch training loss: 0.01024, ssr: 0.90025, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 556\n",
      "batch training loss: 0.01370, ssr: 0.90007, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 557\n",
      "batch training loss: 0.01349, ssr: 0.89989, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 558\n",
      "batch training loss: 0.01242, ssr: 0.89971, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 559\n",
      "batch training loss: 0.01191, ssr: 0.89953, lr: 0.00133\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 560\n",
      "batch training loss: 0.01355, ssr: 0.89935, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 561\n",
      "batch training loss: 0.01230, ssr: 0.89917, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 30 epoch\n",
      "\n",
      "epoch: 562\n",
      "batch training loss: 0.01053, ssr: 0.89899, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 31 epoch\n",
      "\n",
      "epoch: 563\n",
      "batch training loss: 0.01122, ssr: 0.89881, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 32 epoch\n",
      "\n",
      "epoch: 564\n",
      "batch training loss: 0.01275, ssr: 0.89863, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 33 epoch\n",
      "\n",
      "epoch: 565\n",
      "batch training loss: 0.01193, ssr: 0.89845, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 34 epoch\n",
      "\n",
      "epoch: 566\n",
      "batch training loss: 0.01206, ssr: 0.89827, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 35 epoch\n",
      "\n",
      "epoch: 567\n",
      "batch training loss: 0.01117, ssr: 0.89809, lr: 0.00132\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 36 epoch\n",
      "\n",
      "epoch: 568\n",
      "batch training loss: 0.01173, ssr: 0.89791, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 37 epoch\n",
      "\n",
      "epoch: 569\n",
      "batch training loss: 0.01112, ssr: 0.89773, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 38 epoch\n",
      "\n",
      "epoch: 570\n",
      "batch training loss: 0.01064, ssr: 0.89755, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 39 epoch\n",
      "\n",
      "epoch: 571\n",
      "batch training loss: 0.01116, ssr: 0.89737, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 40 epoch\n",
      "\n",
      "epoch: 572\n",
      "batch training loss: 0.01007, ssr: 0.89719, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 41 epoch\n",
      "\n",
      "epoch: 573\n",
      "batch training loss: 0.01260, ssr: 0.89701, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 42 epoch\n",
      "\n",
      "epoch: 574\n",
      "batch training loss: 0.01035, ssr: 0.89683, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 43 epoch\n",
      "\n",
      "epoch: 575\n",
      "batch training loss: 0.01522, ssr: 0.89665, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 44 epoch\n",
      "\n",
      "epoch: 576\n",
      "batch training loss: 0.00938, ssr: 0.89647, lr: 0.00131\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 45 epoch\n",
      "\n",
      "epoch: 577\n",
      "batch training loss: 0.01205, ssr: 0.89629, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 46 epoch\n",
      "\n",
      "epoch: 578\n",
      "batch training loss: 0.01004, ssr: 0.89611, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 47 epoch\n",
      "\n",
      "epoch: 579\n",
      "batch training loss: 0.01296, ssr: 0.89593, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 48 epoch\n",
      "\n",
      "epoch: 580\n",
      "batch training loss: 0.01488, ssr: 0.89575, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 49 epoch\n",
      "\n",
      "epoch: 581\n",
      "batch training loss: 0.01255, ssr: 0.89557, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 50 epoch\n",
      "\n",
      "epoch: 582\n",
      "batch training loss: 0.01172, ssr: 0.89539, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 51 epoch\n",
      "\n",
      "epoch: 583\n",
      "batch training loss: 0.01238, ssr: 0.89521, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 52 epoch\n",
      "\n",
      "epoch: 584\n",
      "batch training loss: 0.01097, ssr: 0.89503, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 53 epoch\n",
      "\n",
      "epoch: 585\n",
      "batch training loss: 0.01009, ssr: 0.89485, lr: 0.00130\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 54 epoch\n",
      "\n",
      "epoch: 586\n",
      "batch training loss: 0.00961, ssr: 0.89467, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 55 epoch\n",
      "\n",
      "epoch: 587\n",
      "batch training loss: 0.01056, ssr: 0.89449, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 56 epoch\n",
      "\n",
      "epoch: 588\n",
      "batch training loss: 0.01270, ssr: 0.89431, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 57 epoch\n",
      "\n",
      "epoch: 589\n",
      "batch training loss: 0.01142, ssr: 0.89413, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 58 epoch\n",
      "\n",
      "epoch: 590\n",
      "batch training loss: 0.00965, ssr: 0.89395, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 59 epoch\n",
      "\n",
      "epoch: 591\n",
      "batch training loss: 0.01155, ssr: 0.89377, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 60 epoch\n",
      "\n",
      "epoch: 592\n",
      "batch training loss: 0.01050, ssr: 0.89359, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 61 epoch\n",
      "\n",
      "epoch: 593\n",
      "batch training loss: 0.01296, ssr: 0.89341, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 62 epoch\n",
      "\n",
      "epoch: 594\n",
      "batch training loss: 0.01167, ssr: 0.89323, lr: 0.00129\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 63 epoch\n",
      "\n",
      "epoch: 595\n",
      "batch training loss: 0.01169, ssr: 0.89305, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 64 epoch\n",
      "\n",
      "epoch: 596\n",
      "batch training loss: 0.00957, ssr: 0.89287, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 65 epoch\n",
      "\n",
      "epoch: 597\n",
      "batch training loss: 0.00920, ssr: 0.89269, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 66 epoch\n",
      "\n",
      "epoch: 598\n",
      "batch training loss: 0.00971, ssr: 0.89251, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 67 epoch\n",
      "\n",
      "epoch: 599\n",
      "batch training loss: 0.01391, ssr: 0.89233, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 68 epoch\n",
      "\n",
      "epoch: 600\n",
      "batch training loss: 0.01027, ssr: 0.89215, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 69 epoch\n",
      "\n",
      "epoch: 601\n",
      "batch training loss: 0.00982, ssr: 0.89197, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 70 epoch\n",
      "\n",
      "epoch: 602\n",
      "batch training loss: 0.01086, ssr: 0.89179, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 71 epoch\n",
      "\n",
      "epoch: 603\n",
      "batch training loss: 0.00970, ssr: 0.89161, lr: 0.00128\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 72 epoch\n",
      "\n",
      "epoch: 604\n",
      "batch training loss: 0.01030, ssr: 0.89143, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 73 epoch\n",
      "\n",
      "epoch: 605\n",
      "batch training loss: 0.01200, ssr: 0.89125, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 74 epoch\n",
      "\n",
      "epoch: 606\n",
      "batch training loss: 0.00921, ssr: 0.89107, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 75 epoch\n",
      "\n",
      "epoch: 607\n",
      "batch training loss: 0.00914, ssr: 0.89089, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 76 epoch\n",
      "\n",
      "epoch: 608\n",
      "batch training loss: 0.01288, ssr: 0.89071, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 77 epoch\n",
      "\n",
      "epoch: 609\n",
      "batch training loss: 0.01006, ssr: 0.89053, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 78 epoch\n",
      "\n",
      "epoch: 610\n",
      "batch training loss: 0.00962, ssr: 0.89035, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 79 epoch\n",
      "\n",
      "epoch: 611\n",
      "batch training loss: 0.01461, ssr: 0.89017, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 80 epoch\n",
      "\n",
      "epoch: 612\n",
      "batch training loss: 0.01398, ssr: 0.88999, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 81 epoch\n",
      "\n",
      "epoch: 613\n",
      "batch training loss: 0.01818, ssr: 0.88981, lr: 0.00127\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 82 epoch\n",
      "\n",
      "epoch: 614\n",
      "batch training loss: 0.01386, ssr: 0.88963, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 83 epoch\n",
      "\n",
      "epoch: 615\n",
      "batch training loss: 0.00998, ssr: 0.88945, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 84 epoch\n",
      "\n",
      "epoch: 616\n",
      "batch training loss: 0.01342, ssr: 0.88927, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 85 epoch\n",
      "\n",
      "epoch: 617\n",
      "batch training loss: 0.01038, ssr: 0.88909, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 86 epoch\n",
      "\n",
      "epoch: 618\n",
      "batch training loss: 0.01049, ssr: 0.88891, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 87 epoch\n",
      "\n",
      "epoch: 619\n",
      "batch training loss: 0.01148, ssr: 0.88873, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 88 epoch\n",
      "\n",
      "epoch: 620\n",
      "batch training loss: 0.01047, ssr: 0.88855, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 89 epoch\n",
      "\n",
      "epoch: 621\n",
      "batch training loss: 0.01134, ssr: 0.88837, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 90 epoch\n",
      "\n",
      "epoch: 622\n",
      "batch training loss: 0.01232, ssr: 0.88819, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 91 epoch\n",
      "\n",
      "epoch: 623\n",
      "batch training loss: 0.01184, ssr: 0.88801, lr: 0.00126\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 92 epoch\n",
      "\n",
      "epoch: 624\n",
      "batch training loss: 0.01053, ssr: 0.88783, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 93 epoch\n",
      "\n",
      "epoch: 625\n",
      "batch training loss: 0.01086, ssr: 0.88765, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 94 epoch\n",
      "\n",
      "epoch: 626\n",
      "batch training loss: 0.01092, ssr: 0.88747, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 95 epoch\n",
      "\n",
      "epoch: 627\n",
      "batch training loss: 0.01064, ssr: 0.88729, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 96 epoch\n",
      "\n",
      "epoch: 628\n",
      "batch training loss: 0.01188, ssr: 0.88711, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 97 epoch\n",
      "\n",
      "epoch: 629\n",
      "batch training loss: 0.01437, ssr: 0.88693, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 98 epoch\n",
      "\n",
      "epoch: 630\n",
      "batch training loss: 0.00963, ssr: 0.88675, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 99 epoch\n",
      "\n",
      "epoch: 631\n",
      "batch training loss: 0.01312, ssr: 0.88657, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 100 epoch\n",
      "\n",
      "epoch: 632\n",
      "batch training loss: 0.01207, ssr: 0.88639, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 101 epoch\n",
      "\n",
      "epoch: 633\n",
      "batch training loss: 0.00962, ssr: 0.88621, lr: 0.00125\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 102 epoch\n",
      "\n",
      "epoch: 634\n",
      "batch training loss: 0.01072, ssr: 0.88603, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 103 epoch\n",
      "\n",
      "epoch: 635\n",
      "batch training loss: 0.01249, ssr: 0.88585, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 104 epoch\n",
      "\n",
      "epoch: 636\n",
      "batch training loss: 0.01028, ssr: 0.88567, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 105 epoch\n",
      "\n",
      "epoch: 637\n",
      "batch training loss: 0.00970, ssr: 0.88549, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 106 epoch\n",
      "\n",
      "epoch: 638\n",
      "batch training loss: 0.00937, ssr: 0.88531, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 107 epoch\n",
      "\n",
      "epoch: 639\n",
      "batch training loss: 0.01014, ssr: 0.88513, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 108 epoch\n",
      "\n",
      "epoch: 640\n",
      "batch training loss: 0.01044, ssr: 0.88495, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 109 epoch\n",
      "\n",
      "epoch: 641\n",
      "batch training loss: 0.01451, ssr: 0.88477, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 110 epoch\n",
      "\n",
      "epoch: 642\n",
      "batch training loss: 0.01150, ssr: 0.88459, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 111 epoch\n",
      "\n",
      "epoch: 643\n",
      "batch training loss: 0.01469, ssr: 0.88441, lr: 0.00124\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 112 epoch\n",
      "\n",
      "epoch: 644\n",
      "batch training loss: 0.01172, ssr: 0.88423, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 113 epoch\n",
      "\n",
      "epoch: 645\n",
      "batch training loss: 0.01220, ssr: 0.88405, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 114 epoch\n",
      "\n",
      "epoch: 646\n",
      "batch training loss: 0.01107, ssr: 0.88387, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 115 epoch\n",
      "\n",
      "epoch: 647\n",
      "batch training loss: 0.00963, ssr: 0.88369, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 116 epoch\n",
      "\n",
      "epoch: 648\n",
      "batch training loss: 0.01043, ssr: 0.88351, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 117 epoch\n",
      "\n",
      "epoch: 649\n",
      "batch training loss: 0.01061, ssr: 0.88333, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 118 epoch\n",
      "\n",
      "epoch: 650\n",
      "batch training loss: 0.00949, ssr: 0.88315, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 119 epoch\n",
      "\n",
      "epoch: 651\n",
      "batch training loss: 0.00881, ssr: 0.88297, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 120 epoch\n",
      "\n",
      "epoch: 652\n",
      "batch training loss: 0.00949, ssr: 0.88279, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is improved from 0.01481 to 0.01326, saving model\n",
      "\n",
      "epoch: 653\n",
      "batch training loss: 0.01022, ssr: 0.88261, lr: 0.00123\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 1 epoch\n",
      "\n",
      "epoch: 654\n",
      "batch training loss: 0.01354, ssr: 0.88243, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 2 epoch\n",
      "\n",
      "epoch: 655\n",
      "batch training loss: 0.01615, ssr: 0.88225, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 3 epoch\n",
      "\n",
      "epoch: 656\n",
      "batch training loss: 0.01285, ssr: 0.88207, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is not improved for 4 epoch\n",
      "\n",
      "epoch: 657\n",
      "batch training loss: 0.01163, ssr: 0.88189, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 5 epoch\n",
      "\n",
      "epoch: 658\n",
      "batch training loss: 0.01272, ssr: 0.88171, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 6 epoch\n",
      "\n",
      "epoch: 659\n",
      "batch training loss: 0.01275, ssr: 0.88153, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 7 epoch\n",
      "\n",
      "epoch: 660\n",
      "batch training loss: 0.01194, ssr: 0.88135, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 8 epoch\n",
      "\n",
      "epoch: 661\n",
      "batch training loss: 0.00943, ssr: 0.88117, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 9 epoch\n",
      "\n",
      "epoch: 662\n",
      "batch training loss: 0.01054, ssr: 0.88099, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 10 epoch\n",
      "\n",
      "epoch: 663\n",
      "batch training loss: 0.01291, ssr: 0.88081, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 11 epoch\n",
      "\n",
      "epoch: 664\n",
      "batch training loss: 0.01125, ssr: 0.88063, lr: 0.00122\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 12 epoch\n",
      "\n",
      "epoch: 665\n",
      "batch training loss: 0.01265, ssr: 0.88045, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 13 epoch\n",
      "\n",
      "epoch: 666\n",
      "batch training loss: 0.01237, ssr: 0.88027, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 14 epoch\n",
      "\n",
      "epoch: 667\n",
      "batch training loss: 0.01292, ssr: 0.88009, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 15 epoch\n",
      "\n",
      "epoch: 668\n",
      "batch training loss: 0.01060, ssr: 0.87991, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 16 epoch\n",
      "\n",
      "epoch: 669\n",
      "batch training loss: 0.00907, ssr: 0.87973, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 17 epoch\n",
      "\n",
      "epoch: 670\n",
      "batch training loss: 0.01186, ssr: 0.87955, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 18 epoch\n",
      "\n",
      "epoch: 671\n",
      "batch training loss: 0.01160, ssr: 0.87937, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 19 epoch\n",
      "\n",
      "epoch: 672\n",
      "batch training loss: 0.01100, ssr: 0.87919, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 20 epoch\n",
      "\n",
      "epoch: 673\n",
      "batch training loss: 0.01156, ssr: 0.87901, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 21 epoch\n",
      "\n",
      "epoch: 674\n",
      "batch training loss: 0.01462, ssr: 0.87883, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 22 epoch\n",
      "\n",
      "epoch: 675\n",
      "batch training loss: 0.01148, ssr: 0.87865, lr: 0.00121\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 23 epoch\n",
      "\n",
      "epoch: 676\n",
      "batch training loss: 0.00997, ssr: 0.87847, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 24 epoch\n",
      "\n",
      "epoch: 677\n",
      "batch training loss: 0.00922, ssr: 0.87829, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 25 epoch\n",
      "\n",
      "epoch: 678\n",
      "batch training loss: 0.01112, ssr: 0.87811, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 26 epoch\n",
      "\n",
      "epoch: 679\n",
      "batch training loss: 0.01223, ssr: 0.87793, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 27 epoch\n",
      "\n",
      "epoch: 680\n",
      "batch training loss: 0.00901, ssr: 0.87775, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 28 epoch\n",
      "\n",
      "epoch: 681\n",
      "batch training loss: 0.01301, ssr: 0.87757, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 29 epoch\n",
      "\n",
      "epoch: 682\n",
      "batch training loss: 0.01124, ssr: 0.87739, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is not improved for 30 epoch\n",
      "\n",
      "epoch: 683\n",
      "batch training loss: 0.00997, ssr: 0.87721, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 31 epoch\n",
      "\n",
      "epoch: 684\n",
      "batch training loss: 0.01393, ssr: 0.87703, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 32 epoch\n",
      "\n",
      "epoch: 685\n",
      "batch training loss: 0.01300, ssr: 0.87685, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 33 epoch\n",
      "\n",
      "epoch: 686\n",
      "batch training loss: 0.01022, ssr: 0.87667, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 34 epoch\n",
      "\n",
      "epoch: 687\n",
      "batch training loss: 0.01495, ssr: 0.87649, lr: 0.00120\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 35 epoch\n",
      "\n",
      "epoch: 688\n",
      "batch training loss: 0.01048, ssr: 0.87631, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 36 epoch\n",
      "\n",
      "epoch: 689\n",
      "batch training loss: 0.01175, ssr: 0.87613, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 37 epoch\n",
      "\n",
      "epoch: 690\n",
      "batch training loss: 0.01062, ssr: 0.87595, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 38 epoch\n",
      "\n",
      "epoch: 691\n",
      "batch training loss: 0.01255, ssr: 0.87577, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 39 epoch\n",
      "\n",
      "epoch: 692\n",
      "batch training loss: 0.01366, ssr: 0.87559, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 40 epoch\n",
      "\n",
      "epoch: 693\n",
      "batch training loss: 0.01093, ssr: 0.87541, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 41 epoch\n",
      "\n",
      "epoch: 694\n",
      "batch training loss: 0.01142, ssr: 0.87523, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 42 epoch\n",
      "\n",
      "epoch: 695\n",
      "batch training loss: 0.01038, ssr: 0.87505, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 43 epoch\n",
      "\n",
      "epoch: 696\n",
      "batch training loss: 0.00857, ssr: 0.87487, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 44 epoch\n",
      "\n",
      "epoch: 697\n",
      "batch training loss: 0.00961, ssr: 0.87469, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 45 epoch\n",
      "\n",
      "epoch: 698\n",
      "batch training loss: 0.01046, ssr: 0.87451, lr: 0.00119\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 46 epoch\n",
      "\n",
      "epoch: 699\n",
      "batch training loss: 0.01172, ssr: 0.87433, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 47 epoch\n",
      "\n",
      "epoch: 700\n",
      "batch training loss: 0.01142, ssr: 0.87415, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.07\n",
      "eval score is not improved for 48 epoch\n",
      "\n",
      "epoch: 701\n",
      "batch training loss: 0.01012, ssr: 0.87397, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 49 epoch\n",
      "\n",
      "epoch: 702\n",
      "batch training loss: 0.01112, ssr: 0.87379, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 50 epoch\n",
      "\n",
      "epoch: 703\n",
      "batch training loss: 0.01096, ssr: 0.87361, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 51 epoch\n",
      "\n",
      "epoch: 704\n",
      "batch training loss: 0.01250, ssr: 0.87343, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 52 epoch\n",
      "\n",
      "epoch: 705\n",
      "batch training loss: 0.01334, ssr: 0.87325, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 53 epoch\n",
      "\n",
      "epoch: 706\n",
      "batch training loss: 0.01011, ssr: 0.87307, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 54 epoch\n",
      "\n",
      "epoch: 707\n",
      "batch training loss: 0.01008, ssr: 0.87289, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 55 epoch\n",
      "\n",
      "epoch: 708\n",
      "batch training loss: 0.00890, ssr: 0.87271, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 56 epoch\n",
      "\n",
      "epoch: 709\n",
      "batch training loss: 0.01253, ssr: 0.87253, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 57 epoch\n",
      "\n",
      "epoch: 710\n",
      "batch training loss: 0.01008, ssr: 0.87235, lr: 0.00118\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 58 epoch\n",
      "\n",
      "epoch: 711\n",
      "batch training loss: 0.00982, ssr: 0.87217, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 59 epoch\n",
      "\n",
      "epoch: 712\n",
      "batch training loss: 0.01032, ssr: 0.87199, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 60 epoch\n",
      "\n",
      "epoch: 713\n",
      "batch training loss: 0.01165, ssr: 0.87181, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 61 epoch\n",
      "\n",
      "epoch: 714\n",
      "batch training loss: 0.00957, ssr: 0.87163, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 62 epoch\n",
      "\n",
      "epoch: 715\n",
      "batch training loss: 0.01067, ssr: 0.87145, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 63 epoch\n",
      "\n",
      "epoch: 716\n",
      "batch training loss: 0.01002, ssr: 0.87127, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 64 epoch\n",
      "\n",
      "epoch: 717\n",
      "batch training loss: 0.00992, ssr: 0.87109, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 65 epoch\n",
      "\n",
      "epoch: 718\n",
      "batch training loss: 0.00943, ssr: 0.87091, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 66 epoch\n",
      "\n",
      "epoch: 719\n",
      "batch training loss: 0.01010, ssr: 0.87073, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 67 epoch\n",
      "\n",
      "epoch: 720\n",
      "batch training loss: 0.01140, ssr: 0.87055, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 68 epoch\n",
      "\n",
      "epoch: 721\n",
      "batch training loss: 0.00903, ssr: 0.87037, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 69 epoch\n",
      "\n",
      "epoch: 722\n",
      "batch training loss: 0.00960, ssr: 0.87019, lr: 0.00117\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 70 epoch\n",
      "\n",
      "epoch: 723\n",
      "batch training loss: 0.01057, ssr: 0.87001, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 71 epoch\n",
      "\n",
      "epoch: 724\n",
      "batch training loss: 0.01122, ssr: 0.86983, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 72 epoch\n",
      "\n",
      "epoch: 725\n",
      "batch training loss: 0.01050, ssr: 0.86965, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 73 epoch\n",
      "\n",
      "epoch: 726\n",
      "batch training loss: 0.01154, ssr: 0.86947, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 74 epoch\n",
      "\n",
      "epoch: 727\n",
      "batch training loss: 0.01077, ssr: 0.86929, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 75 epoch\n",
      "\n",
      "epoch: 728\n",
      "batch training loss: 0.01234, ssr: 0.86911, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 76 epoch\n",
      "\n",
      "epoch: 729\n",
      "batch training loss: 0.00963, ssr: 0.86893, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 77 epoch\n",
      "\n",
      "epoch: 730\n",
      "batch training loss: 0.00894, ssr: 0.86875, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 78 epoch\n",
      "\n",
      "epoch: 731\n",
      "batch training loss: 0.00967, ssr: 0.86857, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 79 epoch\n",
      "\n",
      "epoch: 732\n",
      "batch training loss: 0.01053, ssr: 0.86839, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 80 epoch\n",
      "\n",
      "epoch: 733\n",
      "batch training loss: 0.00909, ssr: 0.86821, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 81 epoch\n",
      "\n",
      "epoch: 734\n",
      "batch training loss: 0.01016, ssr: 0.86803, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 82 epoch\n",
      "\n",
      "epoch: 735\n",
      "batch training loss: 0.01338, ssr: 0.86785, lr: 0.00116\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 83 epoch\n",
      "\n",
      "epoch: 736\n",
      "batch training loss: 0.00991, ssr: 0.86767, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 84 epoch\n",
      "\n",
      "epoch: 737\n",
      "batch training loss: 0.01279, ssr: 0.86749, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 85 epoch\n",
      "\n",
      "epoch: 738\n",
      "batch training loss: 0.01047, ssr: 0.86731, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 86 epoch\n",
      "\n",
      "epoch: 739\n",
      "batch training loss: 0.01003, ssr: 0.86713, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 87 epoch\n",
      "\n",
      "epoch: 740\n",
      "batch training loss: 0.01007, ssr: 0.86695, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 88 epoch\n",
      "\n",
      "epoch: 741\n",
      "batch training loss: 0.01311, ssr: 0.86677, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 89 epoch\n",
      "\n",
      "epoch: 742\n",
      "batch training loss: 0.01169, ssr: 0.86659, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 90 epoch\n",
      "\n",
      "epoch: 743\n",
      "batch training loss: 0.00927, ssr: 0.86641, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 91 epoch\n",
      "\n",
      "epoch: 744\n",
      "batch training loss: 0.01049, ssr: 0.86623, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 92 epoch\n",
      "\n",
      "epoch: 745\n",
      "batch training loss: 0.01139, ssr: 0.86605, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 93 epoch\n",
      "\n",
      "epoch: 746\n",
      "batch training loss: 0.00874, ssr: 0.86587, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 94 epoch\n",
      "\n",
      "epoch: 747\n",
      "batch training loss: 0.01108, ssr: 0.86569, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 95 epoch\n",
      "\n",
      "epoch: 748\n",
      "batch training loss: 0.01041, ssr: 0.86551, lr: 0.00115\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 96 epoch\n",
      "\n",
      "epoch: 749\n",
      "batch training loss: 0.01008, ssr: 0.86533, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 97 epoch\n",
      "\n",
      "epoch: 750\n",
      "batch training loss: 0.01031, ssr: 0.86515, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 98 epoch\n",
      "\n",
      "epoch: 751\n",
      "batch training loss: 0.00870, ssr: 0.86497, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 99 epoch\n",
      "\n",
      "epoch: 752\n",
      "batch training loss: 0.01256, ssr: 0.86479, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 100 epoch\n",
      "\n",
      "epoch: 753\n",
      "batch training loss: 0.00867, ssr: 0.86461, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 101 epoch\n",
      "\n",
      "epoch: 754\n",
      "batch training loss: 0.01077, ssr: 0.86443, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 102 epoch\n",
      "\n",
      "epoch: 755\n",
      "batch training loss: 0.00948, ssr: 0.86425, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 103 epoch\n",
      "\n",
      "epoch: 756\n",
      "batch training loss: 0.01066, ssr: 0.86407, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 104 epoch\n",
      "\n",
      "epoch: 757\n",
      "batch training loss: 0.01189, ssr: 0.86389, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 105 epoch\n",
      "\n",
      "epoch: 758\n",
      "batch training loss: 0.01215, ssr: 0.86371, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 106 epoch\n",
      "\n",
      "epoch: 759\n",
      "batch training loss: 0.01068, ssr: 0.86353, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 107 epoch\n",
      "\n",
      "epoch: 760\n",
      "batch training loss: 0.01034, ssr: 0.86335, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 108 epoch\n",
      "\n",
      "epoch: 761\n",
      "batch training loss: 0.01171, ssr: 0.86317, lr: 0.00114\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 109 epoch\n",
      "\n",
      "epoch: 762\n",
      "batch training loss: 0.01138, ssr: 0.86299, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 110 epoch\n",
      "\n",
      "epoch: 763\n",
      "batch training loss: 0.01397, ssr: 0.86281, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 111 epoch\n",
      "\n",
      "epoch: 764\n",
      "batch training loss: 0.01248, ssr: 0.86263, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 112 epoch\n",
      "\n",
      "epoch: 765\n",
      "batch training loss: 0.00934, ssr: 0.86245, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 113 epoch\n",
      "\n",
      "epoch: 766\n",
      "batch training loss: 0.01032, ssr: 0.86227, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 114 epoch\n",
      "\n",
      "epoch: 767\n",
      "batch training loss: 0.00854, ssr: 0.86209, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 115 epoch\n",
      "\n",
      "epoch: 768\n",
      "batch training loss: 0.00954, ssr: 0.86191, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 116 epoch\n",
      "\n",
      "epoch: 769\n",
      "batch training loss: 0.00950, ssr: 0.86173, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 117 epoch\n",
      "\n",
      "epoch: 770\n",
      "batch training loss: 0.01247, ssr: 0.86155, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 118 epoch\n",
      "\n",
      "epoch: 771\n",
      "batch training loss: 0.00881, ssr: 0.86137, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 119 epoch\n",
      "\n",
      "epoch: 772\n",
      "batch training loss: 0.00907, ssr: 0.86119, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 120 epoch\n",
      "\n",
      "epoch: 773\n",
      "batch training loss: 0.00891, ssr: 0.86101, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 121 epoch\n",
      "\n",
      "epoch: 774\n",
      "batch training loss: 0.00984, ssr: 0.86083, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 122 epoch\n",
      "\n",
      "epoch: 775\n",
      "batch training loss: 0.00893, ssr: 0.86065, lr: 0.00113\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 123 epoch\n",
      "\n",
      "epoch: 776\n",
      "batch training loss: 0.01263, ssr: 0.86047, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 124 epoch\n",
      "\n",
      "epoch: 777\n",
      "batch training loss: 0.00981, ssr: 0.86029, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 125 epoch\n",
      "\n",
      "epoch: 778\n",
      "batch training loss: 0.00980, ssr: 0.86011, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 126 epoch\n",
      "\n",
      "epoch: 779\n",
      "batch training loss: 0.01048, ssr: 0.85993, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 127 epoch\n",
      "\n",
      "epoch: 780\n",
      "batch training loss: 0.00850, ssr: 0.85975, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 128 epoch\n",
      "\n",
      "epoch: 781\n",
      "batch training loss: 0.01031, ssr: 0.85957, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 129 epoch\n",
      "\n",
      "epoch: 782\n",
      "batch training loss: 0.00927, ssr: 0.85939, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 130 epoch\n",
      "\n",
      "epoch: 783\n",
      "batch training loss: 0.00836, ssr: 0.85921, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 131 epoch\n",
      "\n",
      "epoch: 784\n",
      "batch training loss: 0.00837, ssr: 0.85903, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 132 epoch\n",
      "\n",
      "epoch: 785\n",
      "batch training loss: 0.00931, ssr: 0.85885, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 133 epoch\n",
      "\n",
      "epoch: 786\n",
      "batch training loss: 0.01300, ssr: 0.85867, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 134 epoch\n",
      "\n",
      "epoch: 787\n",
      "batch training loss: 0.00849, ssr: 0.85849, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 135 epoch\n",
      "\n",
      "epoch: 788\n",
      "batch training loss: 0.01099, ssr: 0.85831, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 136 epoch\n",
      "\n",
      "epoch: 789\n",
      "batch training loss: 0.01304, ssr: 0.85813, lr: 0.00112\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 137 epoch\n",
      "\n",
      "epoch: 790\n",
      "batch training loss: 0.01092, ssr: 0.85795, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 138 epoch\n",
      "\n",
      "epoch: 791\n",
      "batch training loss: 0.01164, ssr: 0.85777, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 139 epoch\n",
      "\n",
      "epoch: 792\n",
      "batch training loss: 0.01242, ssr: 0.85759, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 140 epoch\n",
      "\n",
      "epoch: 793\n",
      "batch training loss: 0.01165, ssr: 0.85741, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 141 epoch\n",
      "\n",
      "epoch: 794\n",
      "batch training loss: 0.00987, ssr: 0.85723, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 142 epoch\n",
      "\n",
      "epoch: 795\n",
      "batch training loss: 0.00905, ssr: 0.85705, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 143 epoch\n",
      "\n",
      "epoch: 796\n",
      "batch training loss: 0.01323, ssr: 0.85687, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 144 epoch\n",
      "\n",
      "epoch: 797\n",
      "batch training loss: 0.00987, ssr: 0.85669, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 145 epoch\n",
      "\n",
      "epoch: 798\n",
      "batch training loss: 0.01100, ssr: 0.85651, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 146 epoch\n",
      "\n",
      "epoch: 799\n",
      "batch training loss: 0.00992, ssr: 0.85633, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 147 epoch\n",
      "\n",
      "epoch: 800\n",
      "batch training loss: 0.00992, ssr: 0.85615, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 148 epoch\n",
      "\n",
      "epoch: 801\n",
      "batch training loss: 0.01008, ssr: 0.85597, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 149 epoch\n",
      "\n",
      "epoch: 802\n",
      "batch training loss: 0.01069, ssr: 0.85579, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 150 epoch\n",
      "\n",
      "epoch: 803\n",
      "batch training loss: 0.00942, ssr: 0.85561, lr: 0.00111\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 151 epoch\n",
      "\n",
      "epoch: 804\n",
      "batch training loss: 0.01270, ssr: 0.85543, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 152 epoch\n",
      "\n",
      "epoch: 805\n",
      "batch training loss: 0.01116, ssr: 0.85525, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 153 epoch\n",
      "\n",
      "epoch: 806\n",
      "batch training loss: 0.01073, ssr: 0.85507, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 154 epoch\n",
      "\n",
      "epoch: 807\n",
      "batch training loss: 0.00927, ssr: 0.85489, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 155 epoch\n",
      "\n",
      "epoch: 808\n",
      "batch training loss: 0.01138, ssr: 0.85471, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 156 epoch\n",
      "\n",
      "epoch: 809\n",
      "batch training loss: 0.00844, ssr: 0.85453, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 157 epoch\n",
      "\n",
      "epoch: 810\n",
      "batch training loss: 0.01103, ssr: 0.85435, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 158 epoch\n",
      "\n",
      "epoch: 811\n",
      "batch training loss: 0.00903, ssr: 0.85417, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 159 epoch\n",
      "\n",
      "epoch: 812\n",
      "batch training loss: 0.01116, ssr: 0.85399, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 160 epoch\n",
      "\n",
      "epoch: 813\n",
      "batch training loss: 0.01194, ssr: 0.85381, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 161 epoch\n",
      "\n",
      "epoch: 814\n",
      "batch training loss: 0.01306, ssr: 0.85363, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 162 epoch\n",
      "\n",
      "epoch: 815\n",
      "batch training loss: 0.01205, ssr: 0.85345, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 163 epoch\n",
      "\n",
      "epoch: 816\n",
      "batch training loss: 0.00916, ssr: 0.85327, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 164 epoch\n",
      "\n",
      "epoch: 817\n",
      "batch training loss: 0.00893, ssr: 0.85309, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 165 epoch\n",
      "\n",
      "epoch: 818\n",
      "batch training loss: 0.01129, ssr: 0.85291, lr: 0.00110\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 166 epoch\n",
      "\n",
      "epoch: 819\n",
      "batch training loss: 0.01290, ssr: 0.85273, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 167 epoch\n",
      "\n",
      "epoch: 820\n",
      "batch training loss: 0.01110, ssr: 0.85255, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 168 epoch\n",
      "\n",
      "epoch: 821\n",
      "batch training loss: 0.01045, ssr: 0.85237, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 169 epoch\n",
      "\n",
      "epoch: 822\n",
      "batch training loss: 0.00891, ssr: 0.85219, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.01\n",
      "eval score is not improved for 170 epoch\n",
      "\n",
      "epoch: 823\n",
      "batch training loss: 0.01632, ssr: 0.85201, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 171 epoch\n",
      "\n",
      "epoch: 824\n",
      "batch training loss: 0.01299, ssr: 0.85183, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 172 epoch\n",
      "\n",
      "epoch: 825\n",
      "batch training loss: 0.01016, ssr: 0.85165, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 173 epoch\n",
      "\n",
      "epoch: 826\n",
      "batch training loss: 0.01037, ssr: 0.85147, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 174 epoch\n",
      "\n",
      "epoch: 827\n",
      "batch training loss: 0.00922, ssr: 0.85129, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 175 epoch\n",
      "\n",
      "epoch: 828\n",
      "batch training loss: 0.01166, ssr: 0.85111, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 176 epoch\n",
      "\n",
      "epoch: 829\n",
      "batch training loss: 0.01382, ssr: 0.85093, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 177 epoch\n",
      "\n",
      "epoch: 830\n",
      "batch training loss: 0.01090, ssr: 0.85075, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 178 epoch\n",
      "\n",
      "epoch: 831\n",
      "batch training loss: 0.00969, ssr: 0.85057, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 179 epoch\n",
      "\n",
      "epoch: 832\n",
      "batch training loss: 0.00952, ssr: 0.85039, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 180 epoch\n",
      "\n",
      "epoch: 833\n",
      "batch training loss: 0.01095, ssr: 0.85021, lr: 0.00109\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 181 epoch\n",
      "\n",
      "epoch: 834\n",
      "batch training loss: 0.01063, ssr: 0.85003, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 182 epoch\n",
      "\n",
      "epoch: 835\n",
      "batch training loss: 0.01069, ssr: 0.84985, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 183 epoch\n",
      "\n",
      "epoch: 836\n",
      "batch training loss: 0.00940, ssr: 0.84967, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 184 epoch\n",
      "\n",
      "epoch: 837\n",
      "batch training loss: 0.00924, ssr: 0.84949, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 185 epoch\n",
      "\n",
      "epoch: 838\n",
      "batch training loss: 0.00937, ssr: 0.84931, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.06\n",
      "eval score is not improved for 186 epoch\n",
      "\n",
      "epoch: 839\n",
      "batch training loss: 0.01052, ssr: 0.84913, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 187 epoch\n",
      "\n",
      "epoch: 840\n",
      "batch training loss: 0.01168, ssr: 0.84895, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 188 epoch\n",
      "\n",
      "epoch: 841\n",
      "batch training loss: 0.01155, ssr: 0.84877, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 189 epoch\n",
      "\n",
      "epoch: 842\n",
      "batch training loss: 0.01590, ssr: 0.84859, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 190 epoch\n",
      "\n",
      "epoch: 843\n",
      "batch training loss: 0.01110, ssr: 0.84841, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 191 epoch\n",
      "\n",
      "epoch: 844\n",
      "batch training loss: 0.00915, ssr: 0.84823, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 192 epoch\n",
      "\n",
      "epoch: 845\n",
      "batch training loss: 0.00875, ssr: 0.84805, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 193 epoch\n",
      "\n",
      "epoch: 846\n",
      "batch training loss: 0.00823, ssr: 0.84787, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 194 epoch\n",
      "\n",
      "epoch: 847\n",
      "batch training loss: 0.01146, ssr: 0.84769, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 195 epoch\n",
      "\n",
      "epoch: 848\n",
      "batch training loss: 0.00869, ssr: 0.84751, lr: 0.00108\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 196 epoch\n",
      "\n",
      "epoch: 849\n",
      "batch training loss: 0.00830, ssr: 0.84733, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 197 epoch\n",
      "\n",
      "epoch: 850\n",
      "batch training loss: 0.00892, ssr: 0.84715, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 198 epoch\n",
      "\n",
      "epoch: 851\n",
      "batch training loss: 0.00880, ssr: 0.84697, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 199 epoch\n",
      "\n",
      "epoch: 852\n",
      "batch training loss: 0.01052, ssr: 0.84679, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 200 epoch\n",
      "\n",
      "epoch: 853\n",
      "batch training loss: 0.01284, ssr: 0.84661, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 201 epoch\n",
      "\n",
      "epoch: 854\n",
      "batch training loss: 0.01089, ssr: 0.84643, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 202 epoch\n",
      "\n",
      "epoch: 855\n",
      "batch training loss: 0.00955, ssr: 0.84625, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 203 epoch\n",
      "\n",
      "epoch: 856\n",
      "batch training loss: 0.00924, ssr: 0.84607, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 204 epoch\n",
      "\n",
      "epoch: 857\n",
      "batch training loss: 0.01025, ssr: 0.84589, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 205 epoch\n",
      "\n",
      "epoch: 858\n",
      "batch training loss: 0.01041, ssr: 0.84571, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 206 epoch\n",
      "\n",
      "epoch: 859\n",
      "batch training loss: 0.01006, ssr: 0.84553, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 207 epoch\n",
      "\n",
      "epoch: 860\n",
      "batch training loss: 0.00827, ssr: 0.84535, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 208 epoch\n",
      "\n",
      "epoch: 861\n",
      "batch training loss: 0.01160, ssr: 0.84517, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 209 epoch\n",
      "\n",
      "epoch: 862\n",
      "batch training loss: 0.01014, ssr: 0.84499, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 210 epoch\n",
      "\n",
      "epoch: 863\n",
      "batch training loss: 0.00822, ssr: 0.84481, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 211 epoch\n",
      "\n",
      "epoch: 864\n",
      "batch training loss: 0.00976, ssr: 0.84463, lr: 0.00107\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 212 epoch\n",
      "\n",
      "epoch: 865\n",
      "batch training loss: 0.00995, ssr: 0.84445, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 213 epoch\n",
      "\n",
      "epoch: 866\n",
      "batch training loss: 0.01119, ssr: 0.84427, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 214 epoch\n",
      "\n",
      "epoch: 867\n",
      "batch training loss: 0.00855, ssr: 0.84409, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 215 epoch\n",
      "\n",
      "epoch: 868\n",
      "batch training loss: 0.00850, ssr: 0.84391, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 216 epoch\n",
      "\n",
      "epoch: 869\n",
      "batch training loss: 0.00969, ssr: 0.84373, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 217 epoch\n",
      "\n",
      "epoch: 870\n",
      "batch training loss: 0.00880, ssr: 0.84355, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 218 epoch\n",
      "\n",
      "epoch: 871\n",
      "batch training loss: 0.01304, ssr: 0.84337, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 219 epoch\n",
      "\n",
      "epoch: 872\n",
      "batch training loss: 0.00934, ssr: 0.84319, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 220 epoch\n",
      "\n",
      "epoch: 873\n",
      "batch training loss: 0.00928, ssr: 0.84301, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 221 epoch\n",
      "\n",
      "epoch: 874\n",
      "batch training loss: 0.01195, ssr: 0.84283, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 222 epoch\n",
      "\n",
      "epoch: 875\n",
      "batch training loss: 0.01139, ssr: 0.84265, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 223 epoch\n",
      "\n",
      "epoch: 876\n",
      "batch training loss: 0.01077, ssr: 0.84247, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 224 epoch\n",
      "\n",
      "epoch: 877\n",
      "batch training loss: 0.00943, ssr: 0.84229, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 225 epoch\n",
      "\n",
      "epoch: 878\n",
      "batch training loss: 0.00788, ssr: 0.84211, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 226 epoch\n",
      "\n",
      "epoch: 879\n",
      "batch training loss: 0.00846, ssr: 0.84193, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 227 epoch\n",
      "\n",
      "epoch: 880\n",
      "batch training loss: 0.01093, ssr: 0.84175, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 228 epoch\n",
      "\n",
      "epoch: 881\n",
      "batch training loss: 0.00888, ssr: 0.84157, lr: 0.00106\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 229 epoch\n",
      "\n",
      "epoch: 882\n",
      "batch training loss: 0.01224, ssr: 0.84139, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.05\n",
      "eval score is not improved for 230 epoch\n",
      "\n",
      "epoch: 883\n",
      "batch training loss: 0.00903, ssr: 0.84121, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 231 epoch\n",
      "\n",
      "epoch: 884\n",
      "batch training loss: 0.00862, ssr: 0.84103, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 232 epoch\n",
      "\n",
      "epoch: 885\n",
      "batch training loss: 0.01200, ssr: 0.84085, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 233 epoch\n",
      "\n",
      "epoch: 886\n",
      "batch training loss: 0.01020, ssr: 0.84067, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 234 epoch\n",
      "\n",
      "epoch: 887\n",
      "batch training loss: 0.00773, ssr: 0.84049, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 235 epoch\n",
      "\n",
      "epoch: 888\n",
      "batch training loss: 0.00874, ssr: 0.84031, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 236 epoch\n",
      "\n",
      "epoch: 889\n",
      "batch training loss: 0.01247, ssr: 0.84013, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 237 epoch\n",
      "\n",
      "epoch: 890\n",
      "batch training loss: 0.01074, ssr: 0.83995, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 238 epoch\n",
      "\n",
      "epoch: 891\n",
      "batch training loss: 0.00797, ssr: 0.83977, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 239 epoch\n",
      "\n",
      "epoch: 892\n",
      "batch training loss: 0.00854, ssr: 0.83959, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 240 epoch\n",
      "\n",
      "epoch: 893\n",
      "batch training loss: 0.00838, ssr: 0.83941, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 241 epoch\n",
      "\n",
      "epoch: 894\n",
      "batch training loss: 0.00803, ssr: 0.83923, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 242 epoch\n",
      "\n",
      "epoch: 895\n",
      "batch training loss: 0.00815, ssr: 0.83905, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 243 epoch\n",
      "\n",
      "epoch: 896\n",
      "batch training loss: 0.00942, ssr: 0.83887, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.02\n",
      "eval score is not improved for 244 epoch\n",
      "\n",
      "epoch: 897\n",
      "batch training loss: 0.00953, ssr: 0.83869, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 245 epoch\n",
      "\n",
      "epoch: 898\n",
      "batch training loss: 0.01166, ssr: 0.83851, lr: 0.00105\n",
      "epoch eval loss:\n",
      "sst: 0.03\n",
      "eval score is not improved for 246 epoch\n",
      "\n",
      "epoch: 899\n",
      "batch training loss: 0.00904, ssr: 0.83833, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 247 epoch\n",
      "\n",
      "epoch: 900\n",
      "batch training loss: 0.00979, ssr: 0.83815, lr: 0.00104\n",
      "epoch eval loss:\n",
      "sst: 0.04\n",
      "eval score is not improved for 248 epoch\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataset_train, dataset_eval, 'checkpoint.chk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst input': (12, 3, 5, 60, 80), 'sst target': (12, 1, 60, 80)}\n"
     ]
    }
   ],
   "source": [
    "dataset_test = cmip_dataset(X_test, true_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=configs.batch_size_test, shuffle=False)\n",
    "print(dataset_test.GetDataShape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk = torch.load('/media/aita-ocean/data/XGX_NEW/argo/cf/depth2/checkpoint.chk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.network.load_state_dict(chk['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 3, 5, 60, 80)\n",
      "(12, 3, 5, 60, 80)\n"
     ]
    }
   ],
   "source": [
    "print(sta_train.shape)\n",
    "print(sta_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 3, 5, 60, 80)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = np.concatenate((sta_train, sta_test),0)\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.6819, 0.6906, 0.6900,  ..., 0.5225, 0.5414, 0.6116],\n",
       "           [0.6718, 0.6797, 0.6828,  ..., 0.5408, 0.5661, 0.6369],\n",
       "           [0.6618, 0.6628, 0.6664,  ..., 0.5736, 0.5951, 0.6399],\n",
       "           ...,\n",
       "           [0.1477, 0.1850, 0.2534,  ..., 0.3844, 0.3601, 0.3801],\n",
       "           [0.1368, 0.1756, 0.2520,  ..., 0.4087, 0.3753, 0.3869],\n",
       "           [0.1290, 0.1596, 0.2449,  ..., 0.4524, 0.4132, 0.4045]],\n",
       "\n",
       "          [[0.6058, 0.6024, 0.6012,  ..., 0.4539, 0.4677, 0.5232],\n",
       "           [0.5994, 0.5992, 0.5991,  ..., 0.4687, 0.4890, 0.5480],\n",
       "           [0.5942, 0.5967, 0.5980,  ..., 0.4852, 0.5150, 0.5631],\n",
       "           ...,\n",
       "           [0.6190, 0.6237, 0.6340,  ..., 0.4901, 0.4816, 0.5155],\n",
       "           [0.6178, 0.6209, 0.6377,  ..., 0.5117, 0.4980, 0.5283],\n",
       "           [0.6200, 0.6188, 0.6439,  ..., 0.5536, 0.5336, 0.5505]],\n",
       "\n",
       "          [[0.6307, 0.5170, 0.5376,  ..., 0.4969, 0.4141, 0.3436],\n",
       "           [0.4849, 0.4463, 0.5330,  ..., 0.5214, 0.3832, 0.3959],\n",
       "           [0.3131, 0.3577, 0.4635,  ..., 0.4472, 0.4279, 0.4618],\n",
       "           ...,\n",
       "           [0.5222, 0.5768, 0.5052,  ..., 0.5169, 0.5253, 0.5457],\n",
       "           [0.5023, 0.5354, 0.5864,  ..., 0.4442, 0.4700, 0.4444],\n",
       "           [0.5178, 0.5224, 0.6153,  ..., 0.4717, 0.4440, 0.3639]],\n",
       "\n",
       "          [[0.3389, 0.3289, 0.3207,  ..., 0.2806, 0.2967, 0.3191],\n",
       "           [0.3413, 0.3341, 0.3259,  ..., 0.3194, 0.3426, 0.3658],\n",
       "           [0.3374, 0.3324, 0.3254,  ..., 0.3705, 0.3904, 0.4040],\n",
       "           ...,\n",
       "           [0.7074, 0.7086, 0.7046,  ..., 0.3502, 0.3445, 0.3453],\n",
       "           [0.7109, 0.7118, 0.7154,  ..., 0.3500, 0.3444, 0.3465],\n",
       "           [0.7090, 0.7184, 0.7264,  ..., 0.3513, 0.3448, 0.3425]],\n",
       "\n",
       "          [[0.3896, 0.4012, 0.4179,  ..., 0.5071, 0.4871, 0.4684],\n",
       "           [0.4212, 0.4350, 0.4549,  ..., 0.4767, 0.4524, 0.4309],\n",
       "           [0.4573, 0.4714, 0.4877,  ..., 0.4425, 0.4218, 0.4058],\n",
       "           ...,\n",
       "           [0.4305, 0.4373, 0.4449,  ..., 0.2567, 0.2181, 0.1822],\n",
       "           [0.4075, 0.4131, 0.4191,  ..., 0.2297, 0.1880, 0.1538],\n",
       "           [0.3729, 0.3778, 0.3887,  ..., 0.2101, 0.1638, 0.1316]]],\n",
       "\n",
       "\n",
       "         [[[0.7226, 0.7234, 0.7211,  ..., 0.4207, 0.4180, 0.4442],\n",
       "           [0.7087, 0.7093, 0.7099,  ..., 0.4278, 0.4212, 0.4570],\n",
       "           [0.6922, 0.6891, 0.6916,  ..., 0.5208, 0.5103, 0.5623],\n",
       "           ...,\n",
       "           [0.1759, 0.1927, 0.2293,  ..., 0.3425, 0.3165, 0.3024],\n",
       "           [0.1413, 0.1490, 0.1863,  ..., 0.3542, 0.3249, 0.3095],\n",
       "           [0.1003, 0.1084, 0.1520,  ..., 0.3765, 0.3443, 0.3267]],\n",
       "\n",
       "          [[0.5990, 0.5991, 0.5991,  ..., 0.3595, 0.3551, 0.3710],\n",
       "           [0.5984, 0.6000, 0.6006,  ..., 0.3631, 0.3628, 0.3915],\n",
       "           [0.5987, 0.6014, 0.6024,  ..., 0.4216, 0.4275, 0.4753],\n",
       "           ...,\n",
       "           [0.6263, 0.6191, 0.6116,  ..., 0.4625, 0.4464, 0.4395],\n",
       "           [0.6194, 0.6101, 0.6128,  ..., 0.4775, 0.4601, 0.4524],\n",
       "           [0.6137, 0.6061, 0.6167,  ..., 0.5077, 0.4918, 0.4840]],\n",
       "\n",
       "          [[0.4570, 0.4827, 0.5081,  ..., 0.4863, 0.3999, 0.3626],\n",
       "           [0.5291, 0.5427, 0.5401,  ..., 0.5558, 0.3972, 0.3659],\n",
       "           [0.5767, 0.5170, 0.4354,  ..., 0.4657, 0.4683, 0.5079],\n",
       "           ...,\n",
       "           [0.4813, 0.4542, 0.4652,  ..., 0.4945, 0.5086, 0.4848],\n",
       "           [0.4759, 0.5299, 0.6236,  ..., 0.4423, 0.4392, 0.5015],\n",
       "           [0.5108, 0.5488, 0.5397,  ..., 0.4723, 0.4349, 0.4362]],\n",
       "\n",
       "          [[0.4367, 0.4223, 0.4180,  ..., 0.3229, 0.3239, 0.3233],\n",
       "           [0.4446, 0.4302, 0.4232,  ..., 0.3173, 0.3183, 0.3183],\n",
       "           [0.4412, 0.4379, 0.4282,  ..., 0.3119, 0.3145, 0.3181],\n",
       "           ...,\n",
       "           [0.6302, 0.6402, 0.6453,  ..., 0.4286, 0.4172, 0.4061],\n",
       "           [0.6325, 0.6392, 0.6464,  ..., 0.4329, 0.4204, 0.4095],\n",
       "           [0.6435, 0.6510, 0.6578,  ..., 0.4258, 0.4125, 0.4026]],\n",
       "\n",
       "          [[0.5269, 0.5265, 0.5318,  ..., 0.5885, 0.5900, 0.5856],\n",
       "           [0.5476, 0.5469, 0.5545,  ..., 0.6109, 0.6082, 0.6002],\n",
       "           [0.5690, 0.5740, 0.5844,  ..., 0.6265, 0.6151, 0.6065],\n",
       "           ...,\n",
       "           [0.4113, 0.4124, 0.4156,  ..., 0.2931, 0.2593, 0.2204],\n",
       "           [0.3884, 0.3878, 0.3899,  ..., 0.2676, 0.2391, 0.2056],\n",
       "           [0.3618, 0.3619, 0.3623,  ..., 0.2419, 0.2169, 0.1927]]],\n",
       "\n",
       "\n",
       "         [[[0.6915, 0.6909, 0.6899,  ..., 0.5875, 0.5981, 0.6315],\n",
       "           [0.6789, 0.6799, 0.6818,  ..., 0.6123, 0.6185, 0.6405],\n",
       "           [0.6641, 0.6625, 0.6656,  ..., 0.6905, 0.6886, 0.6834],\n",
       "           ...,\n",
       "           [0.2199, 0.1956, 0.2270,  ..., 0.3778, 0.3611, 0.3508],\n",
       "           [0.1422, 0.1083, 0.1247,  ..., 0.3896, 0.3750, 0.3630],\n",
       "           [0.0801, 0.0701, 0.0851,  ..., 0.4048, 0.3960, 0.3825]],\n",
       "\n",
       "          [[0.5907, 0.5933, 0.5953,  ..., 0.4637, 0.4902, 0.5488],\n",
       "           [0.5861, 0.5866, 0.5884,  ..., 0.4809, 0.5086, 0.5603],\n",
       "           [0.5833, 0.5835, 0.5852,  ..., 0.5387, 0.5632, 0.5898],\n",
       "           ...,\n",
       "           [0.6501, 0.6095, 0.5576,  ..., 0.4944, 0.4792, 0.4755],\n",
       "           [0.6239, 0.5921, 0.5740,  ..., 0.5197, 0.5059, 0.4994],\n",
       "           [0.6066, 0.5893, 0.5861,  ..., 0.5480, 0.5401, 0.5347]],\n",
       "\n",
       "          [[0.3895, 0.5685, 0.5806,  ..., 0.4220, 0.3835, 0.4056],\n",
       "           [0.5717, 0.5638, 0.4827,  ..., 0.4968, 0.3657, 0.3591],\n",
       "           [0.6595, 0.5594, 0.3168,  ..., 0.4799, 0.4284, 0.4374],\n",
       "           ...,\n",
       "           [0.4695, 0.5653, 0.4748,  ..., 0.4704, 0.4632, 0.3982],\n",
       "           [0.5854, 0.6157, 0.5626,  ..., 0.4496, 0.4813, 0.5170],\n",
       "           [0.5032, 0.3942, 0.4014,  ..., 0.4574, 0.4710, 0.4468]],\n",
       "\n",
       "          [[0.1336, 0.1337, 0.1391,  ..., 0.3156, 0.3013, 0.2926],\n",
       "           [0.1298, 0.1293, 0.1374,  ..., 0.3821, 0.3606, 0.3436],\n",
       "           [0.1387, 0.1360, 0.1381,  ..., 0.4297, 0.4131, 0.3968],\n",
       "           ...,\n",
       "           [0.3753, 0.3793, 0.3835,  ..., 0.4222, 0.4281, 0.4299],\n",
       "           [0.3909, 0.3998, 0.4085,  ..., 0.4147, 0.4184, 0.4210],\n",
       "           [0.4031, 0.4160, 0.4335,  ..., 0.4057, 0.4073, 0.4107]],\n",
       "\n",
       "          [[0.5893, 0.6022, 0.6198,  ..., 0.4107, 0.4048, 0.3958],\n",
       "           [0.6279, 0.6404, 0.6567,  ..., 0.4125, 0.4042, 0.3926],\n",
       "           [0.6583, 0.6745, 0.6935,  ..., 0.4225, 0.4113, 0.4031],\n",
       "           ...,\n",
       "           [0.5173, 0.5160, 0.5160,  ..., 0.1766, 0.1541, 0.1403],\n",
       "           [0.4867, 0.4889, 0.4929,  ..., 0.1421, 0.1270, 0.1117],\n",
       "           [0.4547, 0.4648, 0.4722,  ..., 0.1135, 0.1041, 0.0923]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.7226, 0.7234, 0.7211,  ..., 0.4207, 0.4180, 0.4442],\n",
       "           [0.7087, 0.7093, 0.7099,  ..., 0.4278, 0.4212, 0.4570],\n",
       "           [0.6922, 0.6891, 0.6916,  ..., 0.5208, 0.5103, 0.5623],\n",
       "           ...,\n",
       "           [0.1759, 0.1927, 0.2293,  ..., 0.3425, 0.3165, 0.3024],\n",
       "           [0.1413, 0.1490, 0.1863,  ..., 0.3542, 0.3249, 0.3095],\n",
       "           [0.1003, 0.1084, 0.1520,  ..., 0.3765, 0.3443, 0.3267]],\n",
       "\n",
       "          [[0.5990, 0.5991, 0.5991,  ..., 0.3595, 0.3551, 0.3710],\n",
       "           [0.5984, 0.6000, 0.6006,  ..., 0.3631, 0.3628, 0.3915],\n",
       "           [0.5987, 0.6014, 0.6024,  ..., 0.4216, 0.4275, 0.4753],\n",
       "           ...,\n",
       "           [0.6263, 0.6191, 0.6116,  ..., 0.4625, 0.4464, 0.4395],\n",
       "           [0.6194, 0.6101, 0.6128,  ..., 0.4775, 0.4601, 0.4524],\n",
       "           [0.6137, 0.6061, 0.6167,  ..., 0.5077, 0.4918, 0.4840]],\n",
       "\n",
       "          [[0.4570, 0.4827, 0.5081,  ..., 0.4863, 0.3999, 0.3626],\n",
       "           [0.5291, 0.5427, 0.5401,  ..., 0.5558, 0.3972, 0.3659],\n",
       "           [0.5767, 0.5170, 0.4354,  ..., 0.4657, 0.4683, 0.5079],\n",
       "           ...,\n",
       "           [0.4813, 0.4542, 0.4652,  ..., 0.4945, 0.5086, 0.4848],\n",
       "           [0.4759, 0.5299, 0.6236,  ..., 0.4423, 0.4392, 0.5015],\n",
       "           [0.5108, 0.5488, 0.5397,  ..., 0.4723, 0.4349, 0.4362]],\n",
       "\n",
       "          [[0.4367, 0.4223, 0.4180,  ..., 0.3229, 0.3239, 0.3233],\n",
       "           [0.4446, 0.4302, 0.4232,  ..., 0.3173, 0.3183, 0.3183],\n",
       "           [0.4412, 0.4379, 0.4282,  ..., 0.3119, 0.3145, 0.3181],\n",
       "           ...,\n",
       "           [0.6302, 0.6402, 0.6453,  ..., 0.4286, 0.4172, 0.4061],\n",
       "           [0.6325, 0.6392, 0.6464,  ..., 0.4329, 0.4204, 0.4095],\n",
       "           [0.6435, 0.6510, 0.6578,  ..., 0.4258, 0.4125, 0.4026]],\n",
       "\n",
       "          [[0.5269, 0.5265, 0.5318,  ..., 0.5885, 0.5900, 0.5856],\n",
       "           [0.5476, 0.5469, 0.5545,  ..., 0.6109, 0.6082, 0.6002],\n",
       "           [0.5690, 0.5740, 0.5844,  ..., 0.6265, 0.6151, 0.6065],\n",
       "           ...,\n",
       "           [0.4113, 0.4124, 0.4156,  ..., 0.2931, 0.2593, 0.2204],\n",
       "           [0.3884, 0.3878, 0.3899,  ..., 0.2676, 0.2391, 0.2056],\n",
       "           [0.3618, 0.3619, 0.3623,  ..., 0.2419, 0.2169, 0.1927]]],\n",
       "\n",
       "\n",
       "         [[[0.6915, 0.6909, 0.6899,  ..., 0.5875, 0.5981, 0.6315],\n",
       "           [0.6789, 0.6799, 0.6818,  ..., 0.6123, 0.6185, 0.6405],\n",
       "           [0.6641, 0.6625, 0.6656,  ..., 0.6905, 0.6886, 0.6834],\n",
       "           ...,\n",
       "           [0.2199, 0.1956, 0.2270,  ..., 0.3778, 0.3611, 0.3508],\n",
       "           [0.1422, 0.1083, 0.1247,  ..., 0.3896, 0.3750, 0.3630],\n",
       "           [0.0801, 0.0701, 0.0851,  ..., 0.4048, 0.3960, 0.3825]],\n",
       "\n",
       "          [[0.5907, 0.5933, 0.5953,  ..., 0.4637, 0.4902, 0.5488],\n",
       "           [0.5861, 0.5866, 0.5884,  ..., 0.4809, 0.5086, 0.5603],\n",
       "           [0.5833, 0.5835, 0.5852,  ..., 0.5387, 0.5632, 0.5898],\n",
       "           ...,\n",
       "           [0.6501, 0.6095, 0.5576,  ..., 0.4944, 0.4792, 0.4755],\n",
       "           [0.6239, 0.5921, 0.5740,  ..., 0.5197, 0.5059, 0.4994],\n",
       "           [0.6066, 0.5893, 0.5861,  ..., 0.5480, 0.5401, 0.5347]],\n",
       "\n",
       "          [[0.3895, 0.5685, 0.5806,  ..., 0.4220, 0.3835, 0.4056],\n",
       "           [0.5717, 0.5638, 0.4827,  ..., 0.4968, 0.3657, 0.3591],\n",
       "           [0.6595, 0.5594, 0.3168,  ..., 0.4799, 0.4284, 0.4374],\n",
       "           ...,\n",
       "           [0.4695, 0.5653, 0.4748,  ..., 0.4704, 0.4632, 0.3982],\n",
       "           [0.5854, 0.6157, 0.5626,  ..., 0.4496, 0.4813, 0.5170],\n",
       "           [0.5032, 0.3942, 0.4014,  ..., 0.4574, 0.4710, 0.4468]],\n",
       "\n",
       "          [[0.1336, 0.1337, 0.1391,  ..., 0.3156, 0.3013, 0.2926],\n",
       "           [0.1298, 0.1293, 0.1374,  ..., 0.3821, 0.3606, 0.3436],\n",
       "           [0.1387, 0.1360, 0.1381,  ..., 0.4297, 0.4131, 0.3968],\n",
       "           ...,\n",
       "           [0.3753, 0.3793, 0.3835,  ..., 0.4222, 0.4281, 0.4299],\n",
       "           [0.3909, 0.3998, 0.4085,  ..., 0.4147, 0.4184, 0.4210],\n",
       "           [0.4031, 0.4160, 0.4335,  ..., 0.4057, 0.4073, 0.4107]],\n",
       "\n",
       "          [[0.5893, 0.6022, 0.6198,  ..., 0.4107, 0.4048, 0.3958],\n",
       "           [0.6279, 0.6404, 0.6567,  ..., 0.4125, 0.4042, 0.3926],\n",
       "           [0.6583, 0.6745, 0.6935,  ..., 0.4225, 0.4113, 0.4031],\n",
       "           ...,\n",
       "           [0.5173, 0.5160, 0.5160,  ..., 0.1766, 0.1541, 0.1403],\n",
       "           [0.4867, 0.4889, 0.4929,  ..., 0.1421, 0.1270, 0.1117],\n",
       "           [0.4547, 0.4648, 0.4722,  ..., 0.1135, 0.1041, 0.0923]]],\n",
       "\n",
       "\n",
       "         [[[0.6440, 0.6511, 0.6498,  ..., 0.6046, 0.6053, 0.6198],\n",
       "           [0.6309, 0.6381, 0.6395,  ..., 0.5808, 0.5850, 0.6089],\n",
       "           [0.6142, 0.6180, 0.6201,  ..., 0.5635, 0.5669, 0.5882],\n",
       "           ...,\n",
       "           [0.2575, 0.2710, 0.3083,  ..., 0.3420, 0.3269, 0.3225],\n",
       "           [0.2339, 0.2393, 0.2546,  ..., 0.3811, 0.3753, 0.3718],\n",
       "           [0.2122, 0.2122, 0.2185,  ..., 0.4467, 0.4692, 0.4729]],\n",
       "\n",
       "          [[0.5909, 0.5910, 0.5903,  ..., 0.4405, 0.4711, 0.5455],\n",
       "           [0.5917, 0.5903, 0.5882,  ..., 0.4846, 0.5092, 0.5699],\n",
       "           [0.5945, 0.5929, 0.5906,  ..., 0.5213, 0.5352, 0.5713],\n",
       "           ...,\n",
       "           [0.6498, 0.6480, 0.6379,  ..., 0.4426, 0.4398, 0.4495],\n",
       "           [0.6483, 0.6462, 0.6425,  ..., 0.4793, 0.4802, 0.4888],\n",
       "           [0.6526, 0.6493, 0.6468,  ..., 0.5289, 0.5430, 0.5524]],\n",
       "\n",
       "          [[0.5921, 0.7468, 0.6415,  ..., 0.3632, 0.3990, 0.4373],\n",
       "           [0.5498, 0.4062, 0.5195,  ..., 0.3968, 0.3518, 0.4016],\n",
       "           [0.4915, 0.2719, 0.3564,  ..., 0.3916, 0.3637, 0.3962],\n",
       "           ...,\n",
       "           [0.4735, 0.2964, 0.2489,  ..., 0.4782, 0.4535, 0.3315],\n",
       "           [0.5803, 0.4316, 0.4277,  ..., 0.5115, 0.5278, 0.4520],\n",
       "           [0.5465, 0.4795, 0.4643,  ..., 0.4649, 0.5161, 0.5141]],\n",
       "\n",
       "          [[0.3356, 0.3377, 0.3307,  ..., 0.3260, 0.3571, 0.3859],\n",
       "           [0.3195, 0.3097, 0.3088,  ..., 0.3819, 0.4162, 0.4511],\n",
       "           [0.2996, 0.2890, 0.2885,  ..., 0.4512, 0.4845, 0.5176],\n",
       "           ...,\n",
       "           [0.3653, 0.3653, 0.3644,  ..., 0.4169, 0.4163, 0.4198],\n",
       "           [0.3610, 0.3593, 0.3622,  ..., 0.4121, 0.4093, 0.4150],\n",
       "           [0.3382, 0.3431, 0.3514,  ..., 0.4091, 0.4063, 0.4145]],\n",
       "\n",
       "          [[0.5593, 0.5503, 0.5579,  ..., 0.7039, 0.6793, 0.6357],\n",
       "           [0.5928, 0.5966, 0.6082,  ..., 0.6815, 0.6543, 0.6036],\n",
       "           [0.6455, 0.6501, 0.6579,  ..., 0.6537, 0.6185, 0.5594],\n",
       "           ...,\n",
       "           [0.6135, 0.6075, 0.6073,  ..., 0.2052, 0.1855, 0.1633],\n",
       "           [0.5760, 0.5758, 0.5778,  ..., 0.1755, 0.1628, 0.1383],\n",
       "           [0.5204, 0.5295, 0.5394,  ..., 0.1486, 0.1355, 0.1144]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.6915, 0.6909, 0.6899,  ..., 0.5875, 0.5981, 0.6315],\n",
       "           [0.6789, 0.6799, 0.6818,  ..., 0.6123, 0.6185, 0.6405],\n",
       "           [0.6641, 0.6625, 0.6656,  ..., 0.6905, 0.6886, 0.6834],\n",
       "           ...,\n",
       "           [0.2199, 0.1956, 0.2270,  ..., 0.3778, 0.3611, 0.3508],\n",
       "           [0.1422, 0.1083, 0.1247,  ..., 0.3896, 0.3750, 0.3630],\n",
       "           [0.0801, 0.0701, 0.0851,  ..., 0.4048, 0.3960, 0.3825]],\n",
       "\n",
       "          [[0.5907, 0.5933, 0.5953,  ..., 0.4637, 0.4902, 0.5488],\n",
       "           [0.5861, 0.5866, 0.5884,  ..., 0.4809, 0.5086, 0.5603],\n",
       "           [0.5833, 0.5835, 0.5852,  ..., 0.5387, 0.5632, 0.5898],\n",
       "           ...,\n",
       "           [0.6501, 0.6095, 0.5576,  ..., 0.4944, 0.4792, 0.4755],\n",
       "           [0.6239, 0.5921, 0.5740,  ..., 0.5197, 0.5059, 0.4994],\n",
       "           [0.6066, 0.5893, 0.5861,  ..., 0.5480, 0.5401, 0.5347]],\n",
       "\n",
       "          [[0.3895, 0.5685, 0.5806,  ..., 0.4220, 0.3835, 0.4056],\n",
       "           [0.5717, 0.5638, 0.4827,  ..., 0.4968, 0.3657, 0.3591],\n",
       "           [0.6595, 0.5594, 0.3168,  ..., 0.4799, 0.4284, 0.4374],\n",
       "           ...,\n",
       "           [0.4695, 0.5653, 0.4748,  ..., 0.4704, 0.4632, 0.3982],\n",
       "           [0.5854, 0.6157, 0.5626,  ..., 0.4496, 0.4813, 0.5170],\n",
       "           [0.5032, 0.3942, 0.4014,  ..., 0.4574, 0.4710, 0.4468]],\n",
       "\n",
       "          [[0.1336, 0.1337, 0.1391,  ..., 0.3156, 0.3013, 0.2926],\n",
       "           [0.1298, 0.1293, 0.1374,  ..., 0.3821, 0.3606, 0.3436],\n",
       "           [0.1387, 0.1360, 0.1381,  ..., 0.4297, 0.4131, 0.3968],\n",
       "           ...,\n",
       "           [0.3753, 0.3793, 0.3835,  ..., 0.4222, 0.4281, 0.4299],\n",
       "           [0.3909, 0.3998, 0.4085,  ..., 0.4147, 0.4184, 0.4210],\n",
       "           [0.4031, 0.4160, 0.4335,  ..., 0.4057, 0.4073, 0.4107]],\n",
       "\n",
       "          [[0.5893, 0.6022, 0.6198,  ..., 0.4107, 0.4048, 0.3958],\n",
       "           [0.6279, 0.6404, 0.6567,  ..., 0.4125, 0.4042, 0.3926],\n",
       "           [0.6583, 0.6745, 0.6935,  ..., 0.4225, 0.4113, 0.4031],\n",
       "           ...,\n",
       "           [0.5173, 0.5160, 0.5160,  ..., 0.1766, 0.1541, 0.1403],\n",
       "           [0.4867, 0.4889, 0.4929,  ..., 0.1421, 0.1270, 0.1117],\n",
       "           [0.4547, 0.4648, 0.4722,  ..., 0.1135, 0.1041, 0.0923]]],\n",
       "\n",
       "\n",
       "         [[[0.6440, 0.6511, 0.6498,  ..., 0.6046, 0.6053, 0.6198],\n",
       "           [0.6309, 0.6381, 0.6395,  ..., 0.5808, 0.5850, 0.6089],\n",
       "           [0.6142, 0.6180, 0.6201,  ..., 0.5635, 0.5669, 0.5882],\n",
       "           ...,\n",
       "           [0.2575, 0.2710, 0.3083,  ..., 0.3420, 0.3269, 0.3225],\n",
       "           [0.2339, 0.2393, 0.2546,  ..., 0.3811, 0.3753, 0.3718],\n",
       "           [0.2122, 0.2122, 0.2185,  ..., 0.4467, 0.4692, 0.4729]],\n",
       "\n",
       "          [[0.5909, 0.5910, 0.5903,  ..., 0.4405, 0.4711, 0.5455],\n",
       "           [0.5917, 0.5903, 0.5882,  ..., 0.4846, 0.5092, 0.5699],\n",
       "           [0.5945, 0.5929, 0.5906,  ..., 0.5213, 0.5352, 0.5713],\n",
       "           ...,\n",
       "           [0.6498, 0.6480, 0.6379,  ..., 0.4426, 0.4398, 0.4495],\n",
       "           [0.6483, 0.6462, 0.6425,  ..., 0.4793, 0.4802, 0.4888],\n",
       "           [0.6526, 0.6493, 0.6468,  ..., 0.5289, 0.5430, 0.5524]],\n",
       "\n",
       "          [[0.5921, 0.7468, 0.6415,  ..., 0.3632, 0.3990, 0.4373],\n",
       "           [0.5498, 0.4062, 0.5195,  ..., 0.3968, 0.3518, 0.4016],\n",
       "           [0.4915, 0.2719, 0.3564,  ..., 0.3916, 0.3637, 0.3962],\n",
       "           ...,\n",
       "           [0.4735, 0.2964, 0.2489,  ..., 0.4782, 0.4535, 0.3315],\n",
       "           [0.5803, 0.4316, 0.4277,  ..., 0.5115, 0.5278, 0.4520],\n",
       "           [0.5465, 0.4795, 0.4643,  ..., 0.4649, 0.5161, 0.5141]],\n",
       "\n",
       "          [[0.3356, 0.3377, 0.3307,  ..., 0.3260, 0.3571, 0.3859],\n",
       "           [0.3195, 0.3097, 0.3088,  ..., 0.3819, 0.4162, 0.4511],\n",
       "           [0.2996, 0.2890, 0.2885,  ..., 0.4512, 0.4845, 0.5176],\n",
       "           ...,\n",
       "           [0.3653, 0.3653, 0.3644,  ..., 0.4169, 0.4163, 0.4198],\n",
       "           [0.3610, 0.3593, 0.3622,  ..., 0.4121, 0.4093, 0.4150],\n",
       "           [0.3382, 0.3431, 0.3514,  ..., 0.4091, 0.4063, 0.4145]],\n",
       "\n",
       "          [[0.5593, 0.5503, 0.5579,  ..., 0.7039, 0.6793, 0.6357],\n",
       "           [0.5928, 0.5966, 0.6082,  ..., 0.6815, 0.6543, 0.6036],\n",
       "           [0.6455, 0.6501, 0.6579,  ..., 0.6537, 0.6185, 0.5594],\n",
       "           ...,\n",
       "           [0.6135, 0.6075, 0.6073,  ..., 0.2052, 0.1855, 0.1633],\n",
       "           [0.5760, 0.5758, 0.5778,  ..., 0.1755, 0.1628, 0.1383],\n",
       "           [0.5204, 0.5295, 0.5394,  ..., 0.1486, 0.1355, 0.1144]]],\n",
       "\n",
       "\n",
       "         [[[0.5161, 0.5540, 0.5683,  ..., 0.4175, 0.4267, 0.4573],\n",
       "           [0.4502, 0.4911, 0.5287,  ..., 0.4241, 0.4256, 0.4443],\n",
       "           [0.4157, 0.4557, 0.4999,  ..., 0.4346, 0.4310, 0.4361],\n",
       "           ...,\n",
       "           [0.4981, 0.4993, 0.5076,  ..., 0.4306, 0.4281, 0.4320],\n",
       "           [0.4494, 0.4476, 0.4562,  ..., 0.4675, 0.4751, 0.4796],\n",
       "           [0.3748, 0.3654, 0.3752,  ..., 0.4981, 0.5071, 0.5081]],\n",
       "\n",
       "          [[0.6231, 0.6146, 0.6050,  ..., 0.5684, 0.5979, 0.6389],\n",
       "           [0.6338, 0.6224, 0.6071,  ..., 0.6108, 0.6255, 0.6459],\n",
       "           [0.6373, 0.6236, 0.6072,  ..., 0.6332, 0.6371, 0.6425],\n",
       "           ...,\n",
       "           [0.6740, 0.6756, 0.6757,  ..., 0.4444, 0.4493, 0.4643],\n",
       "           [0.6789, 0.6809, 0.6832,  ..., 0.4745, 0.4835, 0.4941],\n",
       "           [0.6885, 0.6937, 0.6972,  ..., 0.5023, 0.5103, 0.5148]],\n",
       "\n",
       "          [[0.8166, 0.6956, 0.5131,  ..., 0.3597, 0.3823, 0.4034],\n",
       "           [0.4592, 0.5243, 0.5982,  ..., 0.3162, 0.3429, 0.4004],\n",
       "           [0.2388, 0.3990, 0.6207,  ..., 0.3328, 0.3572, 0.3859],\n",
       "           ...,\n",
       "           [0.2422, 0.3157, 0.3842,  ..., 0.4759, 0.3447, 0.3533],\n",
       "           [0.4471, 0.5133, 0.5163,  ..., 0.4870, 0.4113, 0.4296],\n",
       "           [0.5014, 0.4895, 0.4676,  ..., 0.4720, 0.5054, 0.5463]],\n",
       "\n",
       "          [[0.6117, 0.6198, 0.6160,  ..., 0.4591, 0.4743, 0.4876],\n",
       "           [0.5821, 0.5983, 0.6081,  ..., 0.4325, 0.4450, 0.4599],\n",
       "           [0.5618, 0.5770, 0.5947,  ..., 0.4052, 0.4193, 0.4338],\n",
       "           ...,\n",
       "           [0.2711, 0.2674, 0.2644,  ..., 0.3952, 0.3952, 0.3986],\n",
       "           [0.2785, 0.2786, 0.2767,  ..., 0.3936, 0.3957, 0.3995],\n",
       "           [0.2860, 0.2893, 0.2904,  ..., 0.3930, 0.3949, 0.4011]],\n",
       "\n",
       "          [[0.7015, 0.6841, 0.6649,  ..., 0.6422, 0.6490, 0.6551],\n",
       "           [0.7236, 0.7123, 0.6979,  ..., 0.6675, 0.6750, 0.6798],\n",
       "           [0.7450, 0.7330, 0.7214,  ..., 0.6928, 0.6996, 0.7061],\n",
       "           ...,\n",
       "           [0.4378, 0.4386, 0.4336,  ..., 0.1918, 0.1651, 0.1356],\n",
       "           [0.4173, 0.4226, 0.4253,  ..., 0.1574, 0.1326, 0.1060],\n",
       "           [0.3949, 0.3983, 0.4085,  ..., 0.1303, 0.1086, 0.0832]]]],\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.2115, 0.2709, 0.2968,  ..., 0.2267, 0.2284, 0.2747],\n",
       "           [0.2625, 0.3064, 0.3276,  ..., 0.2473, 0.2495, 0.2740],\n",
       "           [0.2695, 0.2863, 0.3112,  ..., 0.2808, 0.2632, 0.2638],\n",
       "           ...,\n",
       "           [0.7698, 0.8252, 0.8872,  ..., 0.6818, 0.6771, 0.6715],\n",
       "           [0.8703, 0.9249, 0.9630,  ..., 0.6439, 0.6440, 0.6486],\n",
       "           [0.9392, 0.9698, 0.9912,  ..., 0.6272, 0.6344, 0.6478]],\n",
       "\n",
       "          [[0.6826, 0.6744, 0.6685,  ..., 0.6192, 0.6236, 0.6625],\n",
       "           [0.6980, 0.6857, 0.6749,  ..., 0.6336, 0.6347, 0.6516],\n",
       "           [0.7118, 0.6983, 0.6836,  ..., 0.6524, 0.6341, 0.6312],\n",
       "           ...,\n",
       "           [0.6055, 0.5815, 0.5595,  ..., 0.7849, 0.7762, 0.7801],\n",
       "           [0.6066, 0.5789, 0.5578,  ..., 0.7678, 0.7634, 0.7641],\n",
       "           [0.6179, 0.5913, 0.5698,  ..., 0.7576, 0.7495, 0.7417]],\n",
       "\n",
       "          [[0.2910, 0.4079, 0.5352,  ..., 0.4582, 0.4403, 0.5281],\n",
       "           [0.3614, 0.2624, 0.3541,  ..., 0.4333, 0.4071, 0.5558],\n",
       "           [0.6350, 0.2834, 0.1771,  ..., 0.4888, 0.5579, 0.5720],\n",
       "           ...,\n",
       "           [0.6734, 0.7947, 0.8056,  ..., 0.4280, 0.4014, 0.4728],\n",
       "           [0.6026, 0.6232, 0.6385,  ..., 0.4707, 0.4037, 0.4317],\n",
       "           [0.5013, 0.3505, 0.3586,  ..., 0.4915, 0.4781, 0.4761]],\n",
       "\n",
       "          [[0.7914, 0.7988, 0.8038,  ..., 0.7134, 0.7166, 0.7222],\n",
       "           [0.7830, 0.7820, 0.7793,  ..., 0.7246, 0.7269, 0.7260],\n",
       "           [0.7642, 0.7628, 0.7562,  ..., 0.7180, 0.7187, 0.7177],\n",
       "           ...,\n",
       "           [0.3382, 0.3354, 0.3333,  ..., 0.4232, 0.4251, 0.4235],\n",
       "           [0.3655, 0.3666, 0.3658,  ..., 0.3876, 0.3913, 0.3978],\n",
       "           [0.3897, 0.3888, 0.3885,  ..., 0.3845, 0.3876, 0.3942]],\n",
       "\n",
       "          [[0.5661, 0.5731, 0.5836,  ..., 0.4171, 0.4115, 0.4042],\n",
       "           [0.5929, 0.5997, 0.6105,  ..., 0.4301, 0.4180, 0.4133],\n",
       "           [0.6281, 0.6349, 0.6428,  ..., 0.4359, 0.4310, 0.4317],\n",
       "           ...,\n",
       "           [0.5818, 0.5788, 0.5770,  ..., 0.1975, 0.1901, 0.1791],\n",
       "           [0.5621, 0.5560, 0.5536,  ..., 0.1438, 0.1402, 0.1318],\n",
       "           [0.5437, 0.5362, 0.5310,  ..., 0.0982, 0.0896, 0.0841]]],\n",
       "\n",
       "\n",
       "         [[[0.1860, 0.2354, 0.2698,  ..., 0.1903, 0.2050, 0.2211],\n",
       "           [0.1635, 0.2265, 0.2648,  ..., 0.2154, 0.2324, 0.2418],\n",
       "           [0.1413, 0.1959, 0.2300,  ..., 0.2616, 0.2651, 0.2650],\n",
       "           ...,\n",
       "           [0.8612, 0.8626, 0.8591,  ..., 0.6682, 0.6730, 0.6803],\n",
       "           [0.9085, 0.9145, 0.9100,  ..., 0.6631, 0.6710, 0.6912],\n",
       "           [0.9361, 0.9489, 0.9462,  ..., 0.6645, 0.6856, 0.7251]],\n",
       "\n",
       "          [[0.6833, 0.6886, 0.6883,  ..., 0.6021, 0.6162, 0.6334],\n",
       "           [0.6754, 0.6879, 0.6915,  ..., 0.6134, 0.6378, 0.6495],\n",
       "           [0.6797, 0.6911, 0.6934,  ..., 0.6413, 0.6574, 0.6646],\n",
       "           ...,\n",
       "           [0.6077, 0.5884, 0.5549,  ..., 0.7617, 0.7303, 0.6790],\n",
       "           [0.6093, 0.5986, 0.5799,  ..., 0.7547, 0.7398, 0.7167],\n",
       "           [0.5895, 0.5907, 0.5936,  ..., 0.7598, 0.7637, 0.7763]],\n",
       "\n",
       "          [[0.4142, 0.5884, 0.4636,  ..., 0.4177, 0.4485, 0.5705],\n",
       "           [0.0488, 0.2064, 0.2456,  ..., 0.4288, 0.4625, 0.5731],\n",
       "           [0.2723, 0.0611, 0.0679,  ..., 0.4710, 0.5497, 0.5720],\n",
       "           ...,\n",
       "           [0.8102, 0.8484, 0.7022,  ..., 0.4477, 0.4384, 0.4945],\n",
       "           [0.6114, 0.5717, 0.6575,  ..., 0.4056, 0.3822, 0.4699],\n",
       "           [0.3827, 0.4055, 0.5709,  ..., 0.5015, 0.4621, 0.4984]],\n",
       "\n",
       "          [[0.4618, 0.4646, 0.4759,  ..., 0.2700, 0.2871, 0.3086],\n",
       "           [0.4535, 0.4520, 0.4628,  ..., 0.2797, 0.2959, 0.3194],\n",
       "           [0.4611, 0.4621, 0.4669,  ..., 0.2839, 0.3003, 0.3229],\n",
       "           ...,\n",
       "           [0.3012, 0.3031, 0.3099,  ..., 0.4198, 0.4279, 0.4321],\n",
       "           [0.3284, 0.3260, 0.3246,  ..., 0.4209, 0.4310, 0.4359],\n",
       "           [0.3555, 0.3523, 0.3459,  ..., 0.4314, 0.4427, 0.4469]],\n",
       "\n",
       "          [[0.6030, 0.6160, 0.6349,  ..., 0.5809, 0.5815, 0.5791],\n",
       "           [0.6374, 0.6556, 0.6777,  ..., 0.5799, 0.5780, 0.5748],\n",
       "           [0.6758, 0.6958, 0.7161,  ..., 0.5740, 0.5728, 0.5698],\n",
       "           ...,\n",
       "           [0.4453, 0.4455, 0.4431,  ..., 0.1531, 0.1309, 0.1153],\n",
       "           [0.4204, 0.4164, 0.4149,  ..., 0.1111, 0.0933, 0.0777],\n",
       "           [0.3963, 0.3913, 0.3881,  ..., 0.0793, 0.0574, 0.0426]]],\n",
       "\n",
       "\n",
       "         [[[0.2904, 0.3080, 0.3112,  ..., 0.2491, 0.3097, 0.3123],\n",
       "           [0.2678, 0.2916, 0.3078,  ..., 0.2812, 0.3481, 0.3592],\n",
       "           [0.2376, 0.2559, 0.3027,  ..., 0.3291, 0.3695, 0.3752],\n",
       "           ...,\n",
       "           [0.7513, 0.7996, 0.8229,  ..., 0.6443, 0.6503, 0.6567],\n",
       "           [0.7953, 0.8372, 0.8474,  ..., 0.6479, 0.6529, 0.6613],\n",
       "           [0.8349, 0.8552, 0.8318,  ..., 0.6532, 0.6600, 0.6737]],\n",
       "\n",
       "          [[0.6919, 0.6842, 0.6745,  ..., 0.6300, 0.6932, 0.6946],\n",
       "           [0.6735, 0.6665, 0.6518,  ..., 0.6489, 0.7180, 0.7273],\n",
       "           [0.6539, 0.6497, 0.6083,  ..., 0.6751, 0.7324, 0.7449],\n",
       "           ...,\n",
       "           [0.6648, 0.6333, 0.5659,  ..., 0.8274, 0.7730, 0.7486],\n",
       "           [0.6595, 0.6478, 0.6166,  ..., 0.7934, 0.7604, 0.7499],\n",
       "           [0.6388, 0.6572, 0.6649,  ..., 0.7674, 0.7530, 0.7570]],\n",
       "\n",
       "          [[0.4979, 0.4055, 0.3451,  ..., 0.4621, 0.4679, 0.5412],\n",
       "           [0.1767, 0.2721, 0.4419,  ..., 0.4529, 0.5519, 0.5691],\n",
       "           [0.1440, 0.2155, 0.1855,  ..., 0.5257, 0.6104, 0.5614],\n",
       "           ...,\n",
       "           [0.7724, 0.6720, 0.6210,  ..., 0.4571, 0.4803, 0.5042],\n",
       "           [0.7161, 0.7305, 0.7016,  ..., 0.3844, 0.4378, 0.4122],\n",
       "           [0.5690, 0.6577, 0.6136,  ..., 0.4974, 0.4727, 0.4281]],\n",
       "\n",
       "          [[0.2928, 0.2956, 0.2919,  ..., 0.5346, 0.5229, 0.5102],\n",
       "           [0.3078, 0.3094, 0.3124,  ..., 0.5278, 0.5116, 0.4989],\n",
       "           [0.3586, 0.3533, 0.3415,  ..., 0.5122, 0.5041, 0.4971],\n",
       "           ...,\n",
       "           [0.3326, 0.3341, 0.3347,  ..., 0.4972, 0.5037, 0.5053],\n",
       "           [0.3642, 0.3586, 0.3533,  ..., 0.4835, 0.4883, 0.4906],\n",
       "           [0.3916, 0.3857, 0.3808,  ..., 0.4695, 0.4729, 0.4750]],\n",
       "\n",
       "          [[0.5728, 0.5835, 0.5871,  ..., 0.4492, 0.4420, 0.4372],\n",
       "           [0.6168, 0.6183, 0.6194,  ..., 0.4661, 0.4574, 0.4463],\n",
       "           [0.6410, 0.6396, 0.6425,  ..., 0.4825, 0.4678, 0.4526],\n",
       "           ...,\n",
       "           [0.5392, 0.5366, 0.5301,  ..., 0.1775, 0.1603, 0.1441],\n",
       "           [0.5135, 0.5113, 0.5099,  ..., 0.1375, 0.1203, 0.1057],\n",
       "           [0.4959, 0.4918, 0.4906,  ..., 0.1042, 0.0867, 0.0747]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.1860, 0.2354, 0.2698,  ..., 0.1903, 0.2050, 0.2211],\n",
       "           [0.1635, 0.2265, 0.2648,  ..., 0.2154, 0.2324, 0.2418],\n",
       "           [0.1413, 0.1959, 0.2300,  ..., 0.2616, 0.2651, 0.2650],\n",
       "           ...,\n",
       "           [0.8612, 0.8626, 0.8591,  ..., 0.6682, 0.6730, 0.6803],\n",
       "           [0.9085, 0.9145, 0.9100,  ..., 0.6631, 0.6710, 0.6912],\n",
       "           [0.9361, 0.9489, 0.9462,  ..., 0.6645, 0.6856, 0.7251]],\n",
       "\n",
       "          [[0.6833, 0.6886, 0.6883,  ..., 0.6021, 0.6162, 0.6334],\n",
       "           [0.6754, 0.6879, 0.6915,  ..., 0.6134, 0.6378, 0.6495],\n",
       "           [0.6797, 0.6911, 0.6934,  ..., 0.6413, 0.6574, 0.6646],\n",
       "           ...,\n",
       "           [0.6077, 0.5884, 0.5549,  ..., 0.7617, 0.7303, 0.6790],\n",
       "           [0.6093, 0.5986, 0.5799,  ..., 0.7547, 0.7398, 0.7167],\n",
       "           [0.5895, 0.5907, 0.5936,  ..., 0.7598, 0.7637, 0.7763]],\n",
       "\n",
       "          [[0.4142, 0.5884, 0.4636,  ..., 0.4177, 0.4485, 0.5705],\n",
       "           [0.0488, 0.2064, 0.2456,  ..., 0.4288, 0.4625, 0.5731],\n",
       "           [0.2723, 0.0611, 0.0679,  ..., 0.4710, 0.5497, 0.5720],\n",
       "           ...,\n",
       "           [0.8102, 0.8484, 0.7022,  ..., 0.4477, 0.4384, 0.4945],\n",
       "           [0.6114, 0.5717, 0.6575,  ..., 0.4056, 0.3822, 0.4699],\n",
       "           [0.3827, 0.4055, 0.5709,  ..., 0.5015, 0.4621, 0.4984]],\n",
       "\n",
       "          [[0.4618, 0.4646, 0.4759,  ..., 0.2700, 0.2871, 0.3086],\n",
       "           [0.4535, 0.4520, 0.4628,  ..., 0.2797, 0.2959, 0.3194],\n",
       "           [0.4611, 0.4621, 0.4669,  ..., 0.2839, 0.3003, 0.3229],\n",
       "           ...,\n",
       "           [0.3012, 0.3031, 0.3099,  ..., 0.4198, 0.4279, 0.4321],\n",
       "           [0.3284, 0.3260, 0.3246,  ..., 0.4209, 0.4310, 0.4359],\n",
       "           [0.3555, 0.3523, 0.3459,  ..., 0.4314, 0.4427, 0.4469]],\n",
       "\n",
       "          [[0.6030, 0.6160, 0.6349,  ..., 0.5809, 0.5815, 0.5791],\n",
       "           [0.6374, 0.6556, 0.6777,  ..., 0.5799, 0.5780, 0.5748],\n",
       "           [0.6758, 0.6958, 0.7161,  ..., 0.5740, 0.5728, 0.5698],\n",
       "           ...,\n",
       "           [0.4453, 0.4455, 0.4431,  ..., 0.1531, 0.1309, 0.1153],\n",
       "           [0.4204, 0.4164, 0.4149,  ..., 0.1111, 0.0933, 0.0777],\n",
       "           [0.3963, 0.3913, 0.3881,  ..., 0.0793, 0.0574, 0.0426]]],\n",
       "\n",
       "\n",
       "         [[[0.2904, 0.3080, 0.3112,  ..., 0.2491, 0.3097, 0.3123],\n",
       "           [0.2678, 0.2916, 0.3078,  ..., 0.2812, 0.3481, 0.3592],\n",
       "           [0.2376, 0.2559, 0.3027,  ..., 0.3291, 0.3695, 0.3752],\n",
       "           ...,\n",
       "           [0.7513, 0.7996, 0.8229,  ..., 0.6443, 0.6503, 0.6567],\n",
       "           [0.7953, 0.8372, 0.8474,  ..., 0.6479, 0.6529, 0.6613],\n",
       "           [0.8349, 0.8552, 0.8318,  ..., 0.6532, 0.6600, 0.6737]],\n",
       "\n",
       "          [[0.6919, 0.6842, 0.6745,  ..., 0.6300, 0.6932, 0.6946],\n",
       "           [0.6735, 0.6665, 0.6518,  ..., 0.6489, 0.7180, 0.7273],\n",
       "           [0.6539, 0.6497, 0.6083,  ..., 0.6751, 0.7324, 0.7449],\n",
       "           ...,\n",
       "           [0.6648, 0.6333, 0.5659,  ..., 0.8274, 0.7730, 0.7486],\n",
       "           [0.6595, 0.6478, 0.6166,  ..., 0.7934, 0.7604, 0.7499],\n",
       "           [0.6388, 0.6572, 0.6649,  ..., 0.7674, 0.7530, 0.7570]],\n",
       "\n",
       "          [[0.4979, 0.4055, 0.3451,  ..., 0.4621, 0.4679, 0.5412],\n",
       "           [0.1767, 0.2721, 0.4419,  ..., 0.4529, 0.5519, 0.5691],\n",
       "           [0.1440, 0.2155, 0.1855,  ..., 0.5257, 0.6104, 0.5614],\n",
       "           ...,\n",
       "           [0.7724, 0.6720, 0.6210,  ..., 0.4571, 0.4803, 0.5042],\n",
       "           [0.7161, 0.7305, 0.7016,  ..., 0.3844, 0.4378, 0.4122],\n",
       "           [0.5690, 0.6577, 0.6136,  ..., 0.4974, 0.4727, 0.4281]],\n",
       "\n",
       "          [[0.2928, 0.2956, 0.2919,  ..., 0.5346, 0.5229, 0.5102],\n",
       "           [0.3078, 0.3094, 0.3124,  ..., 0.5278, 0.5116, 0.4989],\n",
       "           [0.3586, 0.3533, 0.3415,  ..., 0.5122, 0.5041, 0.4971],\n",
       "           ...,\n",
       "           [0.3326, 0.3341, 0.3347,  ..., 0.4972, 0.5037, 0.5053],\n",
       "           [0.3642, 0.3586, 0.3533,  ..., 0.4835, 0.4883, 0.4906],\n",
       "           [0.3916, 0.3857, 0.3808,  ..., 0.4695, 0.4729, 0.4750]],\n",
       "\n",
       "          [[0.5728, 0.5835, 0.5871,  ..., 0.4492, 0.4420, 0.4372],\n",
       "           [0.6168, 0.6183, 0.6194,  ..., 0.4661, 0.4574, 0.4463],\n",
       "           [0.6410, 0.6396, 0.6425,  ..., 0.4825, 0.4678, 0.4526],\n",
       "           ...,\n",
       "           [0.5392, 0.5366, 0.5301,  ..., 0.1775, 0.1603, 0.1441],\n",
       "           [0.5135, 0.5113, 0.5099,  ..., 0.1375, 0.1203, 0.1057],\n",
       "           [0.4959, 0.4918, 0.4906,  ..., 0.1042, 0.0867, 0.0747]]],\n",
       "\n",
       "\n",
       "         [[[0.4622, 0.4963, 0.5049,  ..., 0.3937, 0.3739, 0.3536],\n",
       "           [0.3882, 0.4522, 0.4818,  ..., 0.4125, 0.4058, 0.3982],\n",
       "           [0.3853, 0.4097, 0.4355,  ..., 0.4391, 0.4795, 0.4889],\n",
       "           ...,\n",
       "           [0.5617, 0.5755, 0.6159,  ..., 0.5549, 0.5614, 0.5608],\n",
       "           [0.5826, 0.5873, 0.6174,  ..., 0.5718, 0.5772, 0.5794],\n",
       "           [0.6130, 0.6036, 0.6280,  ..., 0.5839, 0.5857, 0.5838]],\n",
       "\n",
       "          [[0.6620, 0.6401, 0.6305,  ..., 0.6484, 0.6775, 0.6586],\n",
       "           [0.6790, 0.6437, 0.6240,  ..., 0.6590, 0.6993, 0.6919],\n",
       "           [0.6674, 0.6432, 0.6152,  ..., 0.6412, 0.6705, 0.6690],\n",
       "           ...,\n",
       "           [0.5519, 0.5394, 0.5051,  ..., 0.8656, 0.8689, 0.8015],\n",
       "           [0.5515, 0.5496, 0.5406,  ..., 0.8430, 0.8491, 0.8179],\n",
       "           [0.5629, 0.5885, 0.6223,  ..., 0.8331, 0.8279, 0.8079]],\n",
       "\n",
       "          [[0.3550, 0.4240, 0.5723,  ..., 0.4810, 0.5438, 0.5695],\n",
       "           [0.1599, 0.4376, 0.5316,  ..., 0.5533, 0.6135, 0.6015],\n",
       "           [0.1757, 0.3955, 0.4768,  ..., 0.6952, 0.6112, 0.5221],\n",
       "           ...,\n",
       "           [0.6141, 0.5613, 0.5979,  ..., 0.4691, 0.4672, 0.5069],\n",
       "           [0.6888, 0.6282, 0.5958,  ..., 0.4373, 0.4514, 0.4330],\n",
       "           [0.6922, 0.5787, 0.4638,  ..., 0.4849, 0.4102, 0.2868]],\n",
       "\n",
       "          [[0.6468, 0.6503, 0.6549,  ..., 0.6331, 0.6252, 0.6234],\n",
       "           [0.6465, 0.6471, 0.6559,  ..., 0.5905, 0.5829, 0.5841],\n",
       "           [0.6533, 0.6519, 0.6525,  ..., 0.5471, 0.5412, 0.5365],\n",
       "           ...,\n",
       "           [0.3851, 0.3921, 0.3987,  ..., 0.4631, 0.4558, 0.4416],\n",
       "           [0.2961, 0.2969, 0.2962,  ..., 0.4410, 0.4374, 0.4261],\n",
       "           [0.2272, 0.2287, 0.2357,  ..., 0.4230, 0.4174, 0.4088]],\n",
       "\n",
       "          [[0.5053, 0.5085, 0.5052,  ..., 0.3796, 0.3817, 0.3825],\n",
       "           [0.5382, 0.5359, 0.5308,  ..., 0.4018, 0.4033, 0.4028],\n",
       "           [0.5631, 0.5572, 0.5544,  ..., 0.4240, 0.4290, 0.4300],\n",
       "           ...,\n",
       "           [0.4282, 0.4371, 0.4499,  ..., 0.2451, 0.2324, 0.2143],\n",
       "           [0.3795, 0.3861, 0.3954,  ..., 0.2039, 0.1893, 0.1742],\n",
       "           [0.3474, 0.3520, 0.3606,  ..., 0.1667, 0.1473, 0.1344]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.2904, 0.3080, 0.3112,  ..., 0.2491, 0.3097, 0.3123],\n",
       "           [0.2678, 0.2916, 0.3078,  ..., 0.2812, 0.3481, 0.3592],\n",
       "           [0.2376, 0.2559, 0.3027,  ..., 0.3291, 0.3695, 0.3752],\n",
       "           ...,\n",
       "           [0.7513, 0.7996, 0.8229,  ..., 0.6443, 0.6503, 0.6567],\n",
       "           [0.7953, 0.8372, 0.8474,  ..., 0.6479, 0.6529, 0.6613],\n",
       "           [0.8349, 0.8552, 0.8318,  ..., 0.6532, 0.6600, 0.6737]],\n",
       "\n",
       "          [[0.6919, 0.6842, 0.6745,  ..., 0.6300, 0.6932, 0.6946],\n",
       "           [0.6735, 0.6665, 0.6518,  ..., 0.6489, 0.7180, 0.7273],\n",
       "           [0.6539, 0.6497, 0.6083,  ..., 0.6751, 0.7324, 0.7449],\n",
       "           ...,\n",
       "           [0.6648, 0.6333, 0.5659,  ..., 0.8274, 0.7730, 0.7486],\n",
       "           [0.6595, 0.6478, 0.6166,  ..., 0.7934, 0.7604, 0.7499],\n",
       "           [0.6388, 0.6572, 0.6649,  ..., 0.7674, 0.7530, 0.7570]],\n",
       "\n",
       "          [[0.4979, 0.4055, 0.3451,  ..., 0.4621, 0.4679, 0.5412],\n",
       "           [0.1767, 0.2721, 0.4419,  ..., 0.4529, 0.5519, 0.5691],\n",
       "           [0.1440, 0.2155, 0.1855,  ..., 0.5257, 0.6104, 0.5614],\n",
       "           ...,\n",
       "           [0.7724, 0.6720, 0.6210,  ..., 0.4571, 0.4803, 0.5042],\n",
       "           [0.7161, 0.7305, 0.7016,  ..., 0.3844, 0.4378, 0.4122],\n",
       "           [0.5690, 0.6577, 0.6136,  ..., 0.4974, 0.4727, 0.4281]],\n",
       "\n",
       "          [[0.2928, 0.2956, 0.2919,  ..., 0.5346, 0.5229, 0.5102],\n",
       "           [0.3078, 0.3094, 0.3124,  ..., 0.5278, 0.5116, 0.4989],\n",
       "           [0.3586, 0.3533, 0.3415,  ..., 0.5122, 0.5041, 0.4971],\n",
       "           ...,\n",
       "           [0.3326, 0.3341, 0.3347,  ..., 0.4972, 0.5037, 0.5053],\n",
       "           [0.3642, 0.3586, 0.3533,  ..., 0.4835, 0.4883, 0.4906],\n",
       "           [0.3916, 0.3857, 0.3808,  ..., 0.4695, 0.4729, 0.4750]],\n",
       "\n",
       "          [[0.5728, 0.5835, 0.5871,  ..., 0.4492, 0.4420, 0.4372],\n",
       "           [0.6168, 0.6183, 0.6194,  ..., 0.4661, 0.4574, 0.4463],\n",
       "           [0.6410, 0.6396, 0.6425,  ..., 0.4825, 0.4678, 0.4526],\n",
       "           ...,\n",
       "           [0.5392, 0.5366, 0.5301,  ..., 0.1775, 0.1603, 0.1441],\n",
       "           [0.5135, 0.5113, 0.5099,  ..., 0.1375, 0.1203, 0.1057],\n",
       "           [0.4959, 0.4918, 0.4906,  ..., 0.1042, 0.0867, 0.0747]]],\n",
       "\n",
       "\n",
       "         [[[0.4622, 0.4963, 0.5049,  ..., 0.3937, 0.3739, 0.3536],\n",
       "           [0.3882, 0.4522, 0.4818,  ..., 0.4125, 0.4058, 0.3982],\n",
       "           [0.3853, 0.4097, 0.4355,  ..., 0.4391, 0.4795, 0.4889],\n",
       "           ...,\n",
       "           [0.5617, 0.5755, 0.6159,  ..., 0.5549, 0.5614, 0.5608],\n",
       "           [0.5826, 0.5873, 0.6174,  ..., 0.5718, 0.5772, 0.5794],\n",
       "           [0.6130, 0.6036, 0.6280,  ..., 0.5839, 0.5857, 0.5838]],\n",
       "\n",
       "          [[0.6620, 0.6401, 0.6305,  ..., 0.6484, 0.6775, 0.6586],\n",
       "           [0.6790, 0.6437, 0.6240,  ..., 0.6590, 0.6993, 0.6919],\n",
       "           [0.6674, 0.6432, 0.6152,  ..., 0.6412, 0.6705, 0.6690],\n",
       "           ...,\n",
       "           [0.5519, 0.5394, 0.5051,  ..., 0.8656, 0.8689, 0.8015],\n",
       "           [0.5515, 0.5496, 0.5406,  ..., 0.8430, 0.8491, 0.8179],\n",
       "           [0.5629, 0.5885, 0.6223,  ..., 0.8331, 0.8279, 0.8079]],\n",
       "\n",
       "          [[0.3550, 0.4240, 0.5723,  ..., 0.4810, 0.5438, 0.5695],\n",
       "           [0.1599, 0.4376, 0.5316,  ..., 0.5533, 0.6135, 0.6015],\n",
       "           [0.1757, 0.3955, 0.4768,  ..., 0.6952, 0.6112, 0.5221],\n",
       "           ...,\n",
       "           [0.6141, 0.5613, 0.5979,  ..., 0.4691, 0.4672, 0.5069],\n",
       "           [0.6888, 0.6282, 0.5958,  ..., 0.4373, 0.4514, 0.4330],\n",
       "           [0.6922, 0.5787, 0.4638,  ..., 0.4849, 0.4102, 0.2868]],\n",
       "\n",
       "          [[0.6468, 0.6503, 0.6549,  ..., 0.6331, 0.6252, 0.6234],\n",
       "           [0.6465, 0.6471, 0.6559,  ..., 0.5905, 0.5829, 0.5841],\n",
       "           [0.6533, 0.6519, 0.6525,  ..., 0.5471, 0.5412, 0.5365],\n",
       "           ...,\n",
       "           [0.3851, 0.3921, 0.3987,  ..., 0.4631, 0.4558, 0.4416],\n",
       "           [0.2961, 0.2969, 0.2962,  ..., 0.4410, 0.4374, 0.4261],\n",
       "           [0.2272, 0.2287, 0.2357,  ..., 0.4230, 0.4174, 0.4088]],\n",
       "\n",
       "          [[0.5053, 0.5085, 0.5052,  ..., 0.3796, 0.3817, 0.3825],\n",
       "           [0.5382, 0.5359, 0.5308,  ..., 0.4018, 0.4033, 0.4028],\n",
       "           [0.5631, 0.5572, 0.5544,  ..., 0.4240, 0.4290, 0.4300],\n",
       "           ...,\n",
       "           [0.4282, 0.4371, 0.4499,  ..., 0.2451, 0.2324, 0.2143],\n",
       "           [0.3795, 0.3861, 0.3954,  ..., 0.2039, 0.1893, 0.1742],\n",
       "           [0.3474, 0.3520, 0.3606,  ..., 0.1667, 0.1473, 0.1344]]],\n",
       "\n",
       "\n",
       "         [[[0.3792, 0.3712, 0.4026,  ..., 0.6900, 0.7333, 0.7004],\n",
       "           [0.5363, 0.5147, 0.4957,  ..., 0.6855, 0.7049, 0.6869],\n",
       "           [0.5930, 0.5704, 0.5565,  ..., 0.5858, 0.6042, 0.6025],\n",
       "           ...,\n",
       "           [0.4224, 0.4008, 0.3590,  ..., 0.4994, 0.4826, 0.4825],\n",
       "           [0.4170, 0.3975, 0.3563,  ..., 0.4987, 0.4872, 0.4811],\n",
       "           [0.4016, 0.3655, 0.3143,  ..., 0.4661, 0.4662, 0.4673]],\n",
       "\n",
       "          [[0.6568, 0.6555, 0.6593,  ..., 0.6021, 0.6361, 0.6165],\n",
       "           [0.6494, 0.6388, 0.6368,  ..., 0.6385, 0.6470, 0.6342],\n",
       "           [0.6408, 0.6193, 0.6071,  ..., 0.6394, 0.6349, 0.6237],\n",
       "           ...,\n",
       "           [0.6081, 0.5864, 0.5320,  ..., 0.8870, 0.8792, 0.8764],\n",
       "           [0.6060, 0.5942, 0.5584,  ..., 0.8873, 0.8848, 0.8619],\n",
       "           [0.6106, 0.6068, 0.5914,  ..., 0.8788, 0.8615, 0.8175]],\n",
       "\n",
       "          [[0.4434, 0.6422, 0.6437,  ..., 0.5397, 0.5117, 0.5932],\n",
       "           [0.5298, 0.6909, 0.6695,  ..., 0.6931, 0.5715, 0.6652],\n",
       "           [0.4069, 0.4839, 0.4906,  ..., 0.7120, 0.5647, 0.6298],\n",
       "           ...,\n",
       "           [0.5725, 0.6254, 0.7584,  ..., 0.4740, 0.4877, 0.5111],\n",
       "           [0.6073, 0.6158, 0.6568,  ..., 0.4769, 0.4954, 0.5229],\n",
       "           [0.6650, 0.5125, 0.4919,  ..., 0.4595, 0.4016, 0.4404]],\n",
       "\n",
       "          [[0.3513, 0.3640, 0.3779,  ..., 0.3139, 0.3126, 0.2986],\n",
       "           [0.3322, 0.3537, 0.3643,  ..., 0.3499, 0.3357, 0.3139],\n",
       "           [0.3174, 0.3399, 0.3490,  ..., 0.3722, 0.3520, 0.3278],\n",
       "           ...,\n",
       "           [0.4837, 0.4902, 0.5022,  ..., 0.4341, 0.4334, 0.4306],\n",
       "           [0.5128, 0.5110, 0.5261,  ..., 0.4427, 0.4426, 0.4428],\n",
       "           [0.5596, 0.5512, 0.5573,  ..., 0.4433, 0.4480, 0.4526]],\n",
       "\n",
       "          [[0.5168, 0.5327, 0.5465,  ..., 0.3891, 0.3897, 0.3863],\n",
       "           [0.5525, 0.5687, 0.5819,  ..., 0.3900, 0.3925, 0.3977],\n",
       "           [0.5777, 0.5963, 0.6129,  ..., 0.4032, 0.4102, 0.4208],\n",
       "           ...,\n",
       "           [0.4389, 0.4369, 0.4393,  ..., 0.1801, 0.1512, 0.1207],\n",
       "           [0.4236, 0.4176, 0.4130,  ..., 0.1458, 0.1129, 0.0827],\n",
       "           [0.3977, 0.3996, 0.3977,  ..., 0.1155, 0.0763, 0.0514]]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = torch.from_numpy(xx).float().to(configs.device)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth2_ano = trainer.network(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([178, 1, 60, 80])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth2_ano.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth2_ano = depth2_ano.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 1, 60, 80)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth2_ano.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred.cpu().numpy()\n",
    "test_true = test_true.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = unscaler(np.array(test_pred),test_min,test_scale)\n",
    "test_true = unscaler(np.array(test_true),test_min,test_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred.transpose(0,2,3,1)\n",
    "test_true = test_true.transpose(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 60, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst input': (166, 3, 5, 60, 80), 'sst target': (166, 1, 60, 80)}\n"
     ]
    }
   ],
   "source": [
    "dataset_test = cmip_dataset(X_train,true_train)\n",
    "print(dataset_test.GetDataShape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12) must match the size of tensor b (166) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29802/3713587589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_29802/1347341209.py\u001b[0m in \u001b[0;36minfer_test\u001b[0;34m(self, dataset, dataloader)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0msst_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_sst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# sc = self.score(nino_pred, nino_true)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mloss_sst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_sst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msst_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msst_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;31m# loss_nino = self.loss_nino(nino_pred, nino_true).item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_sst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msst_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msst_true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_29802/1347341209.py\u001b[0m in \u001b[0;36mloss_sst\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss_sst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# y_pred/y_true (N, 26, 24, 48)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (12) must match the size of tensor b (166) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "def add(vtype,depth,data):\n",
    "    \n",
    "    ds=xr.open_dataset('/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/BOA_Argo_annual.nc',decode_times=False)\n",
    "    annual = ds[vtype].data[0,depth,49:109,159:239]\n",
    "    for i in range(0,data.shape[0]):\n",
    "        data[i,:,:,0] = data[i,:,:,0]+ annual\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = add('temp', 1, test_pred)\n",
    "test_true = add('temp', 1, test_true)#改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/media/aita-ocean/data/XGX_NEW/argo/cf/depth1_tiao1/data/test_pred.npy\",test_pred)\n",
    "np.save(\"/media/aita-ocean/data/XGX_NEW/argo/cf/depth1_tiao1/data/test_true.npy\",test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "def loss(vtype,depth,test_pred,test_true):\n",
    "    test_preds = np.array(test_pred,copy=True)\n",
    "    test_trues = np.array(test_true,copy=True)\n",
    "    \n",
    "    test_preds = np.squeeze(test_preds)\n",
    "    test_trues = np.squeeze(test_trues)\n",
    "\n",
    "    test_preds[np.isnan(test_preds)] = 0\n",
    "    test_trues[np.isnan(test_trues)] = 0\n",
    "    mask = np.load('/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/'+vtype+'_'+str(depth)+'_ano.npy')\n",
    "#     mask = np.squeeze(mask)\n",
    "    mask = mask[0]\n",
    "\n",
    "    total = mask.shape[0]* mask.shape[1]\n",
    "    total_nan = len(mask[np.isnan(mask)])\n",
    "    total_real = total - total_nan\n",
    "#     print('Total NaN:',total_nan)#统计数据中的nan值\n",
    "#     print('Total Real:',total_real)#统计数据中的nan值\n",
    "#     #nan：0 values ：1\n",
    "    mask[~np.isnan(mask)] = 1\n",
    "    mask[np.isnan(mask)] = 0\n",
    "    rmse = []\n",
    "    rmse_temp = []\n",
    "    nrmse = []\n",
    "    nrmse_temp = []\n",
    "    mae = []\n",
    "    mae_temp = []\n",
    "    for i in range(0,test_preds.shape[0]):\n",
    "\n",
    "        final_temp = mask * test_preds[i]\n",
    "        test_temp = mask * test_trues[i]\n",
    "        # np.sum((y_actual - y_predicted) ** 2)\n",
    "        sse = np.sum((test_temp - final_temp) ** 2)\n",
    "        mse_temp = sse/total_real\n",
    "        rmse_temp = np.sqrt(mse_temp)\n",
    "        nrmse_temp = rmse_temp/np.mean(test_temp)\n",
    "        rmse.append(rmse_temp)\n",
    "        nrmse.append(nrmse_temp)\n",
    "        mae_temp = mean_absolute_error(test_temp,final_temp)*total/total_real\n",
    "\n",
    "        mae.append(mae_temp)\n",
    "#     print('NAN:',len(test_pred[np.isnan(test_pred)]))\n",
    "#     print('TEST NANMIN',np.nanmin(test_pred))\n",
    "#     print('TEST MIN',test_pred.min())\n",
    "    # print(str(depth)+'层')\n",
    "    RMSE = np.sum(rmse)/len(rmse)\n",
    "    MAE = np.sum(mae)/len(mae)\n",
    "    NRMSE= np.sum(nrmse)/len(nrmse)\n",
    "    # NRMSE = nrmse\n",
    "    print(str(depth)+'层:'+'NRMSE RESULT:\\n',NRMSE)\n",
    "    print(str(depth)+'层:'+'RMSE RESULT:\\n',RMSE)\n",
    "    print(str(depth)+'层:'+'MAE RESULT:\\n',MAE)\n",
    "\n",
    "#     print('MAE RESULT:\\n',MAE)\n",
    "\n",
    "    return NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1层:NRMSE RESULT:\n",
      " 0.008619476484036251\n",
      "1层:RMSE RESULT:\n",
      " 0.22833536713995176\n",
      "1层:MAE RESULT:\n",
      " 0.15476821829347653\n"
     ]
    }
   ],
   "source": [
    "nrmse = loss('temp', 1, test_pred, test_true) # yao gai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(vtype,depth,test_pred,test_true):\n",
    "    test_preds = np.array(test_pred,copy=True)\n",
    "    test_trues = np.array(test_true,copy=True)\n",
    "    \n",
    "    test_preds = np.squeeze(test_preds)\n",
    "    test_trues = np.squeeze(test_trues)\n",
    "\n",
    "    test_preds[np.isnan(test_preds)] = 0\n",
    "    test_trues[np.isnan(test_trues)] = 0\n",
    "    mask = np.load('/media/aita-ocean/data/WILL/1103Argo/mutichannel/data/'+vtype+'_'+str(depth)+'_ano.npy')\n",
    "#     mask = np.squeeze(mask)\n",
    "    mask = mask[0]\n",
    "\n",
    "    total = mask.shape[0]* mask.shape[1]\n",
    "    total_nan = len(mask[np.isnan(mask)])\n",
    "    total_real = total - total_nan\n",
    "#     print('Total NaN:',total_nan)#统计数据中的nan值\n",
    "#     print('Total Real:',total_real)#统计数据中的nan值\n",
    "#     #nan：0 values ：1\n",
    "    mask[~np.isnan(mask)] = 1\n",
    "    mask[np.isnan(mask)] = 0\n",
    "    CORR = []\n",
    "    corr = []\n",
    "    corr_temp = []\n",
    "    for i in range(0,test_preds.shape[0]):\n",
    "\n",
    "        final_temp = mask * test_preds[i]\n",
    "        final_temp_f = final_temp.flatten()\n",
    "        test_temp = mask * test_trues[i]\n",
    "        test_temp_f = test_temp.flatten()\n",
    "        corr_temp = np.corrcoef(final_temp_f,test_temp_f)[0,-1]\n",
    "        # print(corr_temp)\n",
    "        corr.append(corr_temp)\n",
    "#     print('NAN:',len(test_pred[np.isnan(test_pred)]))\n",
    "#     print('TEST NANMIN',np.nanmin(test_pred))\n",
    "#     print('TEST MIN',test_pred.min())\n",
    "    # print(str(depth)+'层')\n",
    "    CORR = np.sum(corr)/len(corr)\n",
    "    # CORR = corr\n",
    "    print(str(depth)+'层:'+'CORR RESULT:\\n',CORR)\n",
    "\n",
    "#     print('MAE RESULT:\\n',MAE)\n",
    "\n",
    "    return CORR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1层:CORR RESULT:\n",
      " 0.9971942156181228\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "cor = corr('temp', 1, test_pred, test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

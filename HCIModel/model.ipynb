{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-29T15:26:10.577107Z",
     "start_time": "2024-09-29T15:26:07.960761Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class BasicUnit(nn.Module):\n",
    "    \"\"\"\n",
    "        提取长短期特征\n",
    "        输入通道数：32\n",
    "        输出通道数：8\n",
    "        参数：\n",
    "        input_channels, output_channels,data\n",
    "        返回：\n",
    "        特征图 Tensor (N*C*L)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(BasicUnit, self).__init__()\n",
    "        # 使用3x1的卷积核\n",
    "        self.conv1 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)  # 第一组卷积\n",
    "        self.conv2 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)  # 第二组卷积\n",
    "        self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)  # 第三组卷积\n",
    "        self.conv4 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)  # 第四组卷积\n",
    "\n",
    "    def forward(self, x):\n",
    "        A = torch.tanh(self.conv1(x))  # A = tanh(x * w1 + b1)\n",
    "        B = torch.sigmoid(self.conv2(x))  # B = σ(x * w2 + b2)\n",
    "        C = torch.tanh(self.conv3(x))  # C = tanh(x * w3 + b3)\n",
    "        D = torch.sigmoid(self.conv4(x))  # D = σ(x * w4 + b4)\n",
    "        \n",
    "        E = A * B + C * D  # E = A ⊗ B + C ⊗ D\n",
    "        return E\n",
    "\n",
    "class BrainAnalysisModule(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(BrainAnalysisModule, self).__init__()\n",
    "        self.basic_unit1 = BasicUnit(input_channels, output_channels)  # 第一个基本单元\n",
    "        self.basic_unit2 = BasicUnit(input_channels, output_channels)  # 第二个基本单元\n",
    "\n",
    "    def forward(self, x):\n",
    "        output1 = self.basic_unit1(x)  # 经过第一个基本单元\n",
    "        output2 = self.basic_unit2(x)  # 经过第二个基本单元\n",
    "        combined_output = output1 + output2  # 将两个输出相加\n",
    "        return combined_output\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设输入数据形状为 (批大小, 通道数, 序列长度)\n",
    "    input_data = torch.randn(40, 32, 300)  # 批大小为 40，通道数为 32，序列长度为 3000\n",
    "    brain_module = BrainAnalysisModule(input_channels=32, output_channels=8)  # 创建模块实例\n",
    "    output = brain_module(input_data)  # 前向传播\n",
    "    print(output.shape)  # 打印输出形状\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 8, 300])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:34:14.573054Z",
     "start_time": "2024-09-29T15:34:14.560317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AnticipationModule:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, predicted_modes):\n",
    "        \"\"\"\n",
    "        计算重建预测值\n",
    "        输入通道数：8\n",
    "        输出通道数：1\n",
    "        参数：\n",
    "        predicted_modes: List[torch.Tensor] - 各个分解模式的预测值\n",
    "        返回：\n",
    "        torch.Tensor - 重建预测值\n",
    "        \"\"\"\n",
    "        # 将所有模式的预测值求和\n",
    "        reconstructed_prediction = sum(predicted_modes)\n",
    "        return reconstructed_prediction\n",
    "\n",
    "# 示例：使用 AnticipationModule\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 假设有8个模式的预测值\n",
    "#     predicted_modes = [torch.rand(1) for _ in range(8)]  # 生成随机预测值作为示例\n",
    "# \n",
    "#     anticipation_module = AnticipationModule()\n",
    "#     reconstructed_prediction = anticipation_module.forward(predicted_modes)\n",
    "# \n",
    "#     print(\"重建预测值：\", reconstructed_prediction.item())\n",
    "#     print(\"预测值的维度：\", reconstructed_prediction.size())  # 或者使用 reconstructed_prediction.shape"
   ],
   "id": "2f83ab61b503c16f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重建预测值： 3.883603096008301\n",
      "预测值的维度： torch.Size([1])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "class ModelTrainer:\n",
    "    def __init__(self, data_path, input_channels, output_channels, batch_size=40, num_epochs=200, initial_lr=0.01, min_lr=0.00001):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.data = self.load_data(data_path)\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.initial_lr = initial_lr\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "        # 初始化模型\n",
    "        self.brain_analysis_module = BrainAnalysisModule(input_channels, output_channels).to(self.device)\n",
    "        self.anticipation_module = AnticipationModule().to(self.device)\n",
    "\n",
    "        # 定义损失函数和优化器\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(list(self.brain_analysis_module.parameters()) + list(self.anticipation_module.parameters()), \n",
    "                                     lr=self.initial_lr, weight_decay=0.01)  # L2正则化\n",
    "\n",
    "    def load_data(self, data_path):\n",
    "        df = pd.read_csv(data_path)\n",
    "        data = torch.tensor(df.values, dtype=torch.float32).to(self.device)\n",
    "        return TensorDataset(data)\n",
    "\n",
    "    def train(self):\n",
    "        data_loader = DataLoader(self.data, batch_size=self.batch_size, shuffle=True)\n",
    "        learning_rate = self.initial_lr\n",
    "        decay_steps = self.num_epochs // 10\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for inputs in data_loader:\n",
    "                inputs = inputs[0].to(self.device)  # 获取输入数据并转移到 GPU\n",
    "\n",
    "                # 前向传播\n",
    "                y_hat = self.brain_analysis_module(inputs)\n",
    "                final_output = self.anticipation_module(y_hat)\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.criterion(final_output, inputs)  # 假设目标是输入数据\n",
    "\n",
    "                # 反向传播和优化\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # 学习率衰减\n",
    "            if epoch % decay_steps == 0 and learning_rate > self.min_lr:\n",
    "                learning_rate *= 0.5  # 每10轮将学习率减半\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = learning_rate\n",
    "\n",
    "            # 打印损失\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{self.num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 使用示例\n",
    "data_path = 'dataset1.csv'  # 数据路径\n",
    "input_channels = 32\n",
    "output_channels = 8\n",
    "trainer = ModelTrainer(data_path, input_channels, output_channels)\n",
    "trainer.train()\n"
   ],
   "id": "a94418b1286e8e80"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
